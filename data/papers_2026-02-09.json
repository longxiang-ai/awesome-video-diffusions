[
  {
    "title": "CineScene: Implicit 3D as Effective Scene Representation for Cinematic Video Generation",
    "authors": [
      "Kaiyi Huang",
      "Yukun Huang",
      "Yu Li",
      "Jianhong Bai",
      "Xintao Wang",
      "Zinan Lin",
      "Xuefei Ning",
      "Jiwen Yu",
      "Pengfei Wan",
      "Yu Wang",
      "Xihui Liu"
    ],
    "abstract": "Cinematic video production requires control over scene-subject composition and camera movement, but live-action shooting remains costly due to the need for constructing physical sets. To address this, we introduce the task of cinematic video generation with decoupled scene context: given multiple images of a static environment, the goal is to synthesize high-quality videos featuring dynamic subject while preserving the underlying scene consistency and following a user-specified camera trajectory. We present CineScene, a framework that leverages implicit 3D-aware scene representation for cinematic video generation. Our key innovation is a novel context conditioning mechanism that injects 3D-aware features in an implicit way: By encoding scene images into visual representations through VGGT, CineScene injects spatial priors into a pretrained text-to-video generation model by additional context concatenation, enabling camera-controlled video synthesis with consistent scenes and dynamic subjects. To further enhance the model's robustness, we introduce a simple yet effective random-shuffling strategy for the input scene images during training. To address the lack of training data, we construct a scene-decoupled dataset with Unreal Engine 5, containing paired videos of scenes with and without dynamic subjects, panoramic images representing the underlying static scene, along with their camera trajectories. Experiments show that CineScene achieves state-of-the-art performance in scene-consistent cinematic video generation, handling large camera movements and demonstrating generalization across diverse environments.",
    "arxiv_url": "https://arxiv.org/abs/2602.06959v1",
    "pdf_url": "https://arxiv.org/pdf/2602.06959v1",
    "published_date": "2026-02-06",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d-aware",
      "dit",
      "physical",
      "text-to-video",
      "video generation",
      "trajectory",
      "video synthesis"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2602.06959v1",
      "pdf": "https://arxiv.org/pdf/2602.06959v1"
    },
    "bibtex": ""
  },
  {
    "title": "RFDM: Residual Flow Diffusion Model for Efficient Causal Video Editing",
    "authors": [
      "Mohammadreza Salehi",
      "Mehdi Noroozi",
      "Luca Morreale",
      "Ruchika Chavhan",
      "Malcolm Chadwick",
      "Alberto Gil Ramos",
      "Abhinav Mehrotra"
    ],
    "abstract": "Instructional video editing applies edits to an input video using only text prompts, enabling intuitive natural-language control. Despite rapid progress, most methods still require fixed-length inputs and substantial compute. Meanwhile, autoregressive video generation enables efficient variable-length synthesis, yet remains under-explored for video editing. We introduce a causal, efficient video editing model that edits variable-length videos frame by frame. For efficiency, we start from a 2D image-to-image (I2I) diffusion model and adapt it to video-to-video (V2V) editing by conditioning the edit at time step t on the model's prediction at t-1. To leverage videos' temporal redundancy, we propose a new I2I diffusion forward process formulation that encourages the model to predict the residual between the target output and the previous prediction. We call this Residual Flow Diffusion Model (RFDM), which focuses the denoising process on changes between consecutive frames. Moreover, we propose a new benchmark that better ranks state-of-the-art methods for editing tasks. Trained on paired video data for global/local style transfer and object removal, RFDM surpasses I2I-based methods and competes with fully spatiotemporal (3D) V2V models, while matching the compute of image models and scaling independently of input video length. More content can be found in: https://smsd75.github.io/RFDM_page/",
    "arxiv_url": "https://arxiv.org/abs/2602.06871v1",
    "pdf_url": "https://arxiv.org/pdf/2602.06871v1",
    "published_date": "2026-02-06",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "dit",
      "video editing",
      "video generation",
      "efficient",
      "video-to-video",
      "style",
      "diffusion model",
      "denoising",
      "autoregressive",
      "benchmark"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2602.06871v1",
      "pdf": "https://arxiv.org/pdf/2602.06871v1",
      "project": "https://smsd75.github.io/RFDM_page"
    },
    "bibtex": ""
  },
  {
    "title": "DiTS: Multimodal Diffusion Transformers Are Time Series Forecasters",
    "authors": [
      "Haoran Zhang",
      "Haixuan Liu",
      "Yong Liu",
      "Yunzhong Qiu",
      "Yuxuan Wang",
      "Jianmin Wang",
      "Mingsheng Long"
    ],
    "abstract": "While generative modeling on time series facilitates more capable and flexible probabilistic forecasting, existing generative time series models do not address the multi-dimensional properties of time series data well. The prevalent architecture of Diffusion Transformers (DiT), which relies on simplistic conditioning controls and a single-stream Transformer backbone, tends to underutilize cross-variate dependencies in covariate-aware forecasting. Inspired by Multimodal Diffusion Transformers that integrate textual guidance into video generation, we propose Diffusion Transformers for Time Series (DiTS), a general-purpose architecture that frames endogenous and exogenous variates as distinct modalities. To better capture both inter-variate and intra-variate dependencies, we design a dual-stream Transformer block tailored for time-series data, comprising a Time Attention module for autoregressive modeling along the temporal dimension and a Variate Attention module for cross-variate modeling. Unlike the common approach for images, which flattens 2D token grids into 1D sequences, our design leverages the low-rank property inherent in multivariate dependencies, thereby reducing computational costs. Experiments show that DiTS achieves state-of-the-art performance across benchmarks, regardless of the presence of future exogenous variate observations, demonstrating unique generative forecasting strengths over traditional deterministic deep forecasting models.",
    "arxiv_url": "https://arxiv.org/abs/2602.06597v1",
    "pdf_url": "https://arxiv.org/pdf/2602.06597v1",
    "published_date": "2026-02-06",
    "categories": [
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "dit",
      "diffusion transformer",
      "video generation",
      "architecture",
      "autoregressive",
      "benchmark"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2602.06597v1",
      "pdf": "https://arxiv.org/pdf/2602.06597v1"
    },
    "bibtex": ""
  },
  {
    "title": "Bridging the Indoor-Outdoor Gap: Vision-Centric Instruction-Guided Embodied Navigation for the Last Meters",
    "authors": [
      "Yuxiang Zhao",
      "Yirong Yang",
      "Yanqing Zhu",
      "Yanfen Shen",
      "Chiyu Wang",
      "Zhining Gu",
      "Pei Shi",
      "Wei Guo",
      "Mu Xu"
    ],
    "abstract": "Embodied navigation holds significant promise for real-world applications such as last-mile delivery. However, most existing approaches are confined to either indoor or outdoor environments and rely heavily on strong assumptions, such as access to precise coordinate systems. While current outdoor methods can guide agents to the vicinity of a target using coarse-grained localization, they fail to enable fine-grained entry through specific building entrances, critically limiting their utility in practical deployment scenarios that require seamless outdoor-to-indoor transitions. To bridge this gap, we introduce a novel task: out-to-in prior-free instruction-driven embodied navigation. This formulation explicitly eliminates reliance on accurate external priors, requiring agents to navigate solely based on egocentric visual observations guided by instructions. To tackle this task, we propose a vision-centric embodied navigation framework that leverages image-based prompts to drive decision-making. Additionally, we present the first open-source dataset for this task, featuring a pipeline that integrates trajectory-conditioned video synthesis into the data generation process. Through extensive experiments, we demonstrate that our proposed method consistently outperforms state-of-the-art baselines across key metrics including success rate and path efficiency.",
    "arxiv_url": "https://arxiv.org/abs/2602.06427v1",
    "pdf_url": "https://arxiv.org/pdf/2602.06427v1",
    "published_date": "2026-02-06",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "video synthesis",
      "trajectory",
      "dit"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2602.06427v1",
      "pdf": "https://arxiv.org/pdf/2602.06427v1"
    },
    "bibtex": ""
  },
  {
    "title": "Driving with DINO: Vision Foundation Features as a Unified Bridge for Sim-to-Real Generation in Autonomous Driving",
    "authors": [
      "Xuyang Chen",
      "Conglang Zhang",
      "Chuanheng Fu",
      "Zihao Yang",
      "Kaixuan Zhou",
      "Yizhi Zhang",
      "Jianan He",
      "Yanfeng Zhang",
      "Mingwei Sun",
      "Zengmao Wang",
      "Zhen Dong",
      "Xiaoxiao Long",
      "Liqiu Meng"
    ],
    "abstract": "Driven by the emergence of Controllable Video Diffusion, existing Sim2Real methods for autonomous driving video generation typically rely on explicit intermediate representations to bridge the domain gap. However, these modalities face a fundamental Consistency-Realism Dilemma. Low-level signals (e.g., edges, blurred images) ensure precise control but compromise realism by \"baking in\" synthetic artifacts, whereas high-level priors (e.g., depth, semantics, HDMaps) facilitate photorealism but lack the structural detail required for consistent guidance. In this work, we present Driving with DINO (DwD), a novel framework that leverages Vision Foundation Module (VFM) features as a unified bridge between the simulation and real-world domains. We first identify that these features encode a spectrum of information, from high-level semantics to fine-grained structure. To effectively utilize this, we employ Principal Subspace Projection to discard the high-frequency elements responsible for \"texture baking,\" while concurrently introducing Random Channel Tail Drop to mitigate the structural loss inherent in rigid dimensionality reduction, thereby reconciling realism with control consistency. Furthermore, to fully leverage DINOv3's high-resolution capabilities for enhancing control precision, we introduce a learnable Spatial Alignment Module that adapts these high-resolution features to the diffusion backbone. Finally, we propose a Causal Temporal Aggregator employing causal convolutions to explicitly preserve historical motion context when integrating frame-wise DINO features, which effectively mitigates motion blur and guarantees temporal stability. Project page: https://albertchen98.github.io/DwD-project/",
    "arxiv_url": "https://arxiv.org/abs/2602.06159v1",
    "pdf_url": "https://arxiv.org/pdf/2602.06159v1",
    "published_date": "2026-02-05",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "controllable",
      "video diffusion",
      "video generation",
      "autonomous driving",
      "simulation"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2602.06159v1",
      "pdf": "https://arxiv.org/pdf/2602.06159v1",
      "project": "https://albertchen98.github.io/DwD-project"
    },
    "bibtex": ""
  },
  {
    "title": "From Blurry to Believable: Enhancing Low-quality Talking Heads with 3D Generative Priors",
    "authors": [
      "Ding-Jiun Huang",
      "Yuanhao Wang",
      "Shao-Ji Yuan",
      "Albert Mosella-Montoro",
      "Francisco Vicente Carrasco",
      "Cheng Zhang",
      "Fernando De la Torre"
    ],
    "abstract": "Creating high-fidelity, animatable 3D talking heads is crucial for immersive applications, yet often hindered by the prevalence of low-quality image or video sources, which yield poor 3D reconstructions. In this paper, we introduce SuperHead, a novel framework for enhancing low-resolution, animatable 3D head avatars. The core challenge lies in synthesizing high-quality geometry and textures, while ensuring both 3D and temporal consistency during animation and preserving subject identity. Despite recent progress in image, video and 3D-based super-resolution (SR), existing SR techniques are ill-equipped to handle dynamic 3D inputs. To address this, SuperHead leverages the rich priors from pre-trained 3D generative models via a novel dynamics-aware 3D inversion scheme. This process optimizes the latent representation of the generative model to produce a super-resolved 3D Gaussian Splatting (3DGS) head model, which is subsequently rigged to an underlying parametric head model (e.g., FLAME) for animation. The inversion is jointly supervised using a sparse collection of upscaled 2D face renderings and corresponding depth maps, captured from diverse facial expressions and camera viewpoints, to ensure realism under dynamic facial motions. Experiments demonstrate that SuperHead generates avatars with fine-grained facial details under dynamic motions, significantly outperforming baseline methods in visual quality.",
    "arxiv_url": "https://arxiv.org/abs/2602.06122v1",
    "pdf_url": "https://arxiv.org/pdf/2602.06122v1",
    "published_date": "2026-02-05",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "talking head",
      "temporal consistency",
      "super-resolution",
      "identity",
      "dynamics",
      "avatar"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2602.06122v1",
      "pdf": "https://arxiv.org/pdf/2602.06122v1"
    },
    "bibtex": ""
  },
  {
    "title": "Context Forcing: Consistent Autoregressive Video Generation with Long Context",
    "authors": [
      "Shuo Chen",
      "Cong Wei",
      "Sun Sun",
      "Ping Nie",
      "Kai Zhou",
      "Ge Zhang",
      "Ming-Hsuan Yang",
      "Wenhu Chen"
    ],
    "abstract": "Recent approaches to real-time long video generation typically employ streaming tuning strategies, attempting to train a long-context student using a short-context (memoryless) teacher. In these frameworks, the student performs long rollouts but receives supervision from a teacher limited to short 5-second windows. This structural discrepancy creates a critical \\textbf{student-teacher mismatch}: the teacher's inability to access long-term history prevents it from guiding the student on global temporal dependencies, effectively capping the student's context length. To resolve this, we propose \\textbf{Context Forcing}, a novel framework that trains a long-context student via a long-context teacher. By ensuring the teacher is aware of the full generation history, we eliminate the supervision mismatch, enabling the robust training of models capable of long-term consistency. To make this computationally feasible for extreme durations (e.g., 2 minutes), we introduce a context management system that transforms the linearly growing context into a \\textbf{Slow-Fast Memory} architecture, significantly reducing visual redundancy. Extensive results demonstrate that our method enables effective context lengths exceeding 20 seconds -- 2 to 10 times longer than state-of-the-art methods like LongLive and Infinite-RoPE. By leveraging this extended context, Context Forcing preserves superior consistency across long durations, surpassing state-of-the-art baselines on various long video evaluation metrics.",
    "arxiv_url": "https://arxiv.org/abs/2602.06028v1",
    "pdf_url": "https://arxiv.org/pdf/2602.06028v1",
    "published_date": "2026-02-05",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "streaming",
      "video generation",
      "evaluation",
      "architecture",
      "long video",
      "autoregressive"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2602.06028v1",
      "pdf": "https://arxiv.org/pdf/2602.06028v1"
    },
    "bibtex": ""
  },
  {
    "title": "RISE-Video: Can Video Generators Decode Implicit World Rules?",
    "authors": [
      "Mingxin Liu",
      "Shuran Ma",
      "Shibei Meng",
      "Xiangyu Zhao",
      "Zicheng Zhang",
      "Shaofeng Zhang",
      "Zhihang Zhong",
      "Peixian Chen",
      "Haoyu Cao",
      "Xing Sun",
      "Haodong Duan",
      "Xue Yang"
    ],
    "abstract": "While generative video models have achieved remarkable visual fidelity, their capacity to internalize and reason over implicit world rules remains a critical yet under-explored frontier. To bridge this gap, we present RISE-Video, a pioneering reasoning-oriented benchmark for Text-Image-to-Video (TI2V) synthesis that shifts the evaluative focus from surface-level aesthetics to deep cognitive reasoning. RISE-Video comprises 467 meticulously human-annotated samples spanning eight rigorous categories, providing a structured testbed for probing model intelligence across diverse dimensions, ranging from commonsense and spatial dynamics to specialized subject domains. Our framework introduces a multi-dimensional evaluation protocol consisting of four metrics: \\textit{Reasoning Alignment}, \\textit{Temporal Consistency}, \\textit{Physical Rationality}, and \\textit{Visual Quality}. To further support scalable evaluation, we propose an automated pipeline leveraging Large Multimodal Models (LMMs) to emulate human-centric assessment. Extensive experiments on 11 state-of-the-art TI2V models reveal pervasive deficiencies in simulating complex scenarios under implicit constraints, offering critical insights for the advancement of future world-simulating generative models.",
    "arxiv_url": "https://arxiv.org/abs/2602.05986v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05986v1",
    "published_date": "2026-02-05",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "temporal consistency",
      "physical",
      "image-to-video",
      "i2v",
      "evaluation",
      "dynamics",
      "benchmark"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2602.05986v1",
      "pdf": "https://arxiv.org/pdf/2602.05986v1"
    },
    "bibtex": ""
  },
  {
    "title": "LSA: Localized Semantic Alignment for Enhancing Temporal Consistency in Traffic Video Generation",
    "authors": [
      "Mirlan Karimov",
      "Teodora Spasojevic",
      "Markus Braun",
      "Julian Wiederer",
      "Vasileios Belagiannis",
      "Marc Pollefeys"
    ],
    "abstract": "Controllable video generation has emerged as a versatile tool for autonomous driving, enabling realistic synthesis of traffic scenarios. However, existing methods depend on control signals at inference time to guide the generative model towards temporally consistent generation of dynamic objects, limiting their utility as scalable and generalizable data engines. In this work, we propose Localized Semantic Alignment (LSA), a simple yet effective framework for fine-tuning pre-trained video generation models. LSA enhances temporal consistency by aligning semantic features between ground-truth and generated video clips. Specifically, we compare the output of an off-the-shelf feature extraction model between the ground-truth and generated video clips localized around dynamic objects inducing a semantic feature consistency loss. We fine-tune the base model by combining this loss with the standard diffusion loss. The model fine-tuned for a single epoch with our novel loss outperforms the baselines in common video generation evaluation metrics. To further test the temporal consistency in generated videos we adapt two additional metrics from object detection task, namely mAP and mIoU. Extensive experiments on nuScenes and KITTI datasets show the effectiveness of our approach in enhancing temporal consistency in video generation without the need for external control signals during inference and any computational overheads.",
    "arxiv_url": "https://arxiv.org/abs/2602.05966v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05966v1",
    "published_date": "2026-02-05",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "temporal consistency",
      "controllable",
      "dit",
      "video generation",
      "autonomous driving",
      "evaluation"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2602.05966v1",
      "pdf": "https://arxiv.org/pdf/2602.05966v1"
    },
    "bibtex": ""
  },
  {
    "title": "Pathwise Test-Time Correction for Autoregressive Long Video Generation",
    "authors": [
      "Xunzhi Xiang",
      "Zixuan Duan",
      "Guiyu Zhang",
      "Haiyu Zhang",
      "Zhe Gao",
      "Junta Wu",
      "Shaofeng Zhang",
      "Tengfei Wang",
      "Qi Fan",
      "Chunchao Guo"
    ],
    "abstract": "Distilled autoregressive diffusion models facilitate real-time short video synthesis but suffer from severe error accumulation during long-sequence generation. While existing Test-Time Optimization (TTO) methods prove effective for images or short clips, we identify that they fail to mitigate drift in extended sequences due to unstable reward landscapes and the hypersensitivity of distilled parameters. To overcome these limitations, we introduce Test-Time Correction (TTC), a training-free alternative. Specifically, TTC utilizes the initial frame as a stable reference anchor to calibrate intermediate stochastic states along the sampling trajectory. Extensive experiments demonstrate that our method seamlessly integrates with various distilled models, extending generation lengths with negligible overhead while matching the quality of resource-intensive training-based methods on 30-second benchmarks.",
    "arxiv_url": "https://arxiv.org/abs/2602.05871v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05871v1",
    "published_date": "2026-02-05",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "trajectory",
      "video generation",
      "video synthesis",
      "diffusion model",
      "long video",
      "autoregressive",
      "benchmark"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2602.05871v1",
      "pdf": "https://arxiv.org/pdf/2602.05871v1"
    },
    "bibtex": ""
  },
  {
    "title": "Sparse Video Generation Propels Real-World Beyond-the-View Vision-Language Navigation",
    "authors": [
      "Hai Zhang",
      "Siqi Liang",
      "Li Chen",
      "Yuxian Li",
      "Yukuan Xu",
      "Yichao Zhong",
      "Fu Zhang",
      "Hongyang Li"
    ],
    "abstract": "Why must vision-language navigation be bound to detailed and verbose language instructions? While such details ease decision-making, they fundamentally contradict the goal for navigation in the real-world. Ideally, agents should possess the autonomy to navigate in unknown environments guided solely by simple and high-level intents. Realizing this ambition introduces a formidable challenge: Beyond-the-View Navigation (BVN), where agents must locate distant, unseen targets without dense and step-by-step guidance. Existing large language model (LLM)-based methods, though adept at following dense instructions, often suffer from short-sighted behaviors due to their reliance on short-horimzon supervision. Simply extending the supervision horizon, however, destabilizes LLM training. In this work, we identify that video generation models inherently benefit from long-horizon supervision to align with language instructions, rendering them uniquely suitable for BVN tasks. Capitalizing on this insight, we propose introducing the video generation model into this field for the first time. Yet, the prohibitive latency for generating videos spanning tens of seconds makes real-world deployment impractical. To bridge this gap, we propose SparseVideoNav, achieving sub-second trajectory inference guided by a generated sparse future spanning a 20-second horizon. This yields a remarkable 27x speed-up compared to the unoptimized counterpart. Extensive real-world zero-shot experiments demonstrate that SparseVideoNav achieves 2.5x the success rate of state-of-the-art LLM baselines on BVN tasks and marks the first realization of such capability in challenging night scenes.",
    "arxiv_url": "https://arxiv.org/abs/2602.05827v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05827v1",
    "published_date": "2026-02-05",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "trajectory",
      "video generation"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2602.05827v1",
      "pdf": "https://arxiv.org/pdf/2602.05827v1"
    },
    "bibtex": ""
  },
  {
    "title": "ShapeGaussian: High-Fidelity 4D Human Reconstruction in Monocular Videos via Vision Priors",
    "authors": [
      "Zhenxiao Liang",
      "Ning Zhang",
      "Youbao Tang",
      "Ruei-Sung Lin",
      "Qixing Huang",
      "Peng Chang",
      "Jing Xiao"
    ],
    "abstract": "We introduce ShapeGaussian, a high-fidelity, template-free method for 4D human reconstruction from casual monocular videos. Generic reconstruction methods lacking robust vision priors, such as 4DGS, struggle to capture high-deformation human motion without multi-view cues. While template-based approaches, primarily relying on SMPL, such as HUGS, can produce photorealistic results, they are highly susceptible to errors in human pose estimation, often leading to unrealistic artifacts. In contrast, ShapeGaussian effectively integrates template-free vision priors to achieve both high-fidelity and robust scene reconstructions. Our method follows a two-step pipeline: first, we learn a coarse, deformable geometry using pretrained models that estimate data-driven priors, providing a foundation for reconstruction. Then, we refine this geometry using a neural deformation model to capture fine-grained dynamic details. By leveraging 2D vision priors, we mitigate artifacts from erroneous pose estimation in template-based methods and employ multiple reference frames to resolve the invisibility issue of 2D keypoints in a template-free manner. Extensive experiments demonstrate that ShapeGaussian surpasses template-based methods in reconstruction accuracy, achieving superior visual quality and robustness across diverse human motions in casual monocular videos.",
    "arxiv_url": "https://arxiv.org/abs/2602.05572v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05572v1",
    "published_date": "2026-02-05",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "human motion"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2602.05572v1",
      "pdf": "https://arxiv.org/pdf/2602.05572v1"
    },
    "bibtex": ""
  },
  {
    "title": "DisCa: Accelerating Video Diffusion Transformers with Distillation-Compatible Learnable Feature Caching",
    "authors": [
      "Chang Zou",
      "Changlin Li",
      "Yang Li",
      "Patrol Li",
      "Jianbing Wu",
      "Xiao He",
      "Songtao Liu",
      "Zhao Zhong",
      "Kailin Huang",
      "Linfeng Zhang"
    ],
    "abstract": "While diffusion models have achieved great success in the field of video generation, this progress is accompanied by a rapidly escalating computational burden. Among the existing acceleration methods, Feature Caching is popular due to its training-free property and considerable speedup performance, but it inevitably faces semantic and detail drop with further compression. Another widely adopted method, training-aware step-distillation, though successful in image generation, also faces drastic degradation in video generation with a few steps. Furthermore, the quality loss becomes more severe when simply applying training-free feature caching to the step-distilled models, due to the sparser sampling steps. This paper novelly introduces a distillation-compatible learnable feature caching mechanism for the first time. We employ a lightweight learnable neural predictor instead of traditional training-free heuristics for diffusion models, enabling a more accurate capture of the high-dimensional feature evolution process. Furthermore, we explore the challenges of highly compressed distillation on large-scale video models and propose a conservative Restricted MeanFlow approach to achieve more stable and lossless distillation. By undertaking these initiatives, we further push the acceleration boundaries to $11.8\\times$ while preserving generation quality. Extensive experiments demonstrate the effectiveness of our method. The code will be made publicly available soon.",
    "arxiv_url": "https://arxiv.org/abs/2602.05449v2",
    "pdf_url": "https://arxiv.org/pdf/2602.05449v2",
    "published_date": "2026-02-05",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "acceleration",
      "video diffusion",
      "distillation",
      "dit",
      "diffusion transformer",
      "video generation",
      "diffusion model"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2602.05449v2",
      "pdf": "https://arxiv.org/pdf/2602.05449v2"
    },
    "bibtex": ""
  },
  {
    "title": "Stable Velocity: A Variance Perspective on Flow Matching",
    "authors": [
      "Donglin Yang",
      "Yongxing Zhang",
      "Xin Yu",
      "Liang Hou",
      "Xin Tao",
      "Pengfei Wan",
      "Xiaojuan Qi",
      "Renjie Liao"
    ],
    "abstract": "While flow matching is elegant, its reliance on single-sample conditional velocities leads to high-variance training targets that destabilize optimization and slow convergence. By explicitly characterizing this variance, we identify 1) a high-variance regime near the prior, where optimization is challenging, and 2) a low-variance regime near the data distribution, where conditional and marginal velocities nearly coincide. Leveraging this insight, we propose Stable Velocity, a unified framework that improves both training and sampling. For training, we introduce Stable Velocity Matching (StableVM), an unbiased variance-reduction objective, along with Variance-Aware Representation Alignment (VA-REPA), which adaptively strengthen auxiliary supervision in the low-variance regime. For inference, we show that dynamics in the low-variance regime admit closed-form simplifications, enabling Stable Velocity Sampling (StableVS), a finetuning-free acceleration. Extensive experiments on ImageNet $256\\times256$ and large pretrained text-to-image and text-to-video models, including SD3.5, Flux, Qwen-Image, and Wan2.2, demonstrate consistent improvements in training efficiency and more than $2\\times$ faster sampling within the low-variance regime without degrading sample quality. Our code is available at https://github.com/linYDTHU/StableVelocity.",
    "arxiv_url": "https://arxiv.org/abs/2602.05435v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05435v1",
    "published_date": "2026-02-05",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/linYDTHU/StableVelocity",
    "keywords": [
      "acceleration",
      "flow matching",
      "dit",
      "text-to-video",
      "dynamics"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2602.05435v1",
      "pdf": "https://arxiv.org/pdf/2602.05435v1",
      "github": "https://github.com/linYDTHU/StableVelocity"
    },
    "bibtex": ""
  },
  {
    "title": "FlashBlock: Attention Caching for Efficient Long-Context Block Diffusion",
    "authors": [
      "Zhuokun Chen",
      "Jianfei Cai",
      "Bohan Zhuang"
    ],
    "abstract": "Generating long-form content, such as minute-long videos and extended texts, is increasingly important for modern generative models. Block diffusion improves inference efficiency via KV caching and block-wise causal inference and has been widely adopted in diffusion language models and video generation. However, in long-context settings, block diffusion still incurs substantial overhead from repeatedly computing attention over a growing KV cache. We identify an underexplored property of block diffusion: cross-step redundancy of attention within a block. Our analysis shows that attention outputs from tokens outside the current block remain largely stable across diffusion steps, while block-internal attention varies significantly. Based on this observation, we propose FlashBlock, a cached block-external attention mechanism that reuses stable attention output, reducing attention computation and KV cache access without modifying the diffusion process. Moreover, FlashBlock is orthogonal to sparse attention and can be combined as a complementary residual reuse strategy, substantially improving model accuracy under aggressive sparsification. Experiments on diffusion language models and video generation demonstrate up to 1.44$\\times$ higher token throughput and up to 1.6$\\times$ reduction in attention time, with negligible impact on generation quality. Project page: https://caesarhhh.github.io/FlashBlock/.",
    "arxiv_url": "https://arxiv.org/abs/2602.05305v2",
    "pdf_url": "https://arxiv.org/pdf/2602.05305v2",
    "published_date": "2026-02-05",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "long video",
      "video generation",
      "long-form"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2602.05305v2",
      "pdf": "https://arxiv.org/pdf/2602.05305v2",
      "project": "https://caesarhhh.github.io/FlashBlock"
    },
    "bibtex": ""
  },
  {
    "title": "GT-SVJ: Generative-Transformer-Based Self-Supervised Video Judge For Efficient Video Reward Modeling",
    "authors": [
      "Shivanshu Shekhar",
      "Uttaran Bhattacharya",
      "Raghavendra Addanki",
      "Mehrab Tanjim",
      "Somdeb Sarkhel",
      "Tong Zhang"
    ],
    "abstract": "Aligning video generative models with human preferences remains challenging: current approaches rely on Vision-Language Models (VLMs) for reward modeling, but these models struggle to capture subtle temporal dynamics. We propose a fundamentally different approach: repurposing video generative models, which are inherently designed to model temporal structure, as reward models. We present the Generative-Transformer-based Self-Supervised Video Judge (\\modelname), a novel evaluation model that transforms state-of-the-art video generation models into powerful temporally-aware reward models. Our key insight is that generative models can be reformulated as energy-based models (EBMs) that assign low energy to high-quality videos and high energy to degraded ones, enabling them to discriminate video quality with remarkable precision when trained via contrastive objectives. To prevent the model from exploiting superficial differences between real and generated videos, we design challenging synthetic negative videos through controlled latent-space perturbations: temporal slicing, feature swapping, and frame shuffling, which simulate realistic but subtle visual degradations. This forces the model to learn meaningful spatiotemporal features rather than trivial artifacts. \\modelname achieves state-of-the-art performance on GenAI-Bench and MonteBench using only 30K human-annotations: $6\\times$ to $65\\times$ fewer than existing VLM-based approaches.",
    "arxiv_url": "https://arxiv.org/abs/2602.05202v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05202v1",
    "published_date": "2026-02-05",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "evaluation",
      "dynamics",
      "video generation",
      "efficient"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2602.05202v1",
      "pdf": "https://arxiv.org/pdf/2602.05202v1"
    },
    "bibtex": ""
  },
  {
    "title": "Reinforced Attention Learning",
    "authors": [
      "Bangzheng Li",
      "Jianmo Ni",
      "Chen Qu",
      "Ian Miao",
      "Liu Yang",
      "Xingyu Fu",
      "Muhao Chen",
      "Derek Zhiyuan Cheng"
    ],
    "abstract": "Post-training with Reinforcement Learning (RL) has substantially improved reasoning in Large Language Models (LLMs) via test-time scaling. However, extending this paradigm to Multimodal LLMs (MLLMs) through verbose rationales yields limited gains for perception and can even degrade performance.   We propose Reinforced Attention Learning (RAL), a policy-gradient framework that directly optimizes internal attention distributions rather than output token sequences. By shifting optimization from what to generate to where to attend, RAL promotes effective information allocation and improved grounding in complex multimodal inputs. Experiments across diverse image and video benchmarks show consistent gains over GRPO and other baselines. We further introduce On-Policy Attention Distillation, demonstrating that transferring latent attention behaviors yields stronger cross-modal alignment than standard knowledge distillation. Our results position attention policies as a principled and general alternative for multimodal post-training.",
    "arxiv_url": "https://arxiv.org/abs/2602.04884v1",
    "pdf_url": "https://arxiv.org/pdf/2602.04884v1",
    "published_date": "2026-02-04",
    "categories": [
      "cs.CL",
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "benchmark",
      "distillation"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2602.04884v1",
      "pdf": "https://arxiv.org/pdf/2602.04884v1"
    },
    "bibtex": ""
  },
  {
    "title": "Light Forcing: Accelerating Autoregressive Video Diffusion via Sparse Attention",
    "authors": [
      "Chengtao Lv",
      "Yumeng Shi",
      "Yushi Huang",
      "Ruihao Gong",
      "Shen Ren",
      "Wenya Wang"
    ],
    "abstract": "Advanced autoregressive (AR) video generation models have improved visual fidelity and interactivity, but the quadratic complexity of attention remains a primary bottleneck for efficient deployment. While existing sparse attention solutions have shown promise on bidirectional models, we identify that applying these solutions to AR models leads to considerable performance degradation for two reasons: isolated consideration of chunk generation and insufficient utilization of past informative context. Motivated by these observations, we propose \\textsc{Light Forcing}, the \\textit{first} sparse attention solution tailored for AR video generation models. It incorporates a \\textit{Chunk-Aware Growth} mechanism to quantitatively estimate the contribution of each chunk, which determines their sparsity allocation. This progressive sparsity increase strategy enables the current chunk to inherit prior knowledge in earlier chunks during generation. Additionally, we introduce a \\textit{Hierarchical Sparse Attention} to capture informative historical and local context in a coarse-to-fine manner. Such two-level mask selection strategy (\\ie, frame and block level) can adaptively handle diverse attention patterns. Extensive experiments demonstrate that our method outperforms existing sparse attention in quality (\\eg, 84.5 on VBench) and efficiency (\\eg, $1.2{\\sim}1.3\\times$ end-to-end speedup). Combined with FP8 quantization and LightVAE, \\textsc{Light Forcing} further achieves a $2.3\\times$ speedup and 19.7\\,FPS on an RTX~5090 GPU. Code will be released at \\href{https://github.com/chengtao-lv/LightForcing}{https://github.com/chengtao-lv/LightForcing}.",
    "arxiv_url": "https://arxiv.org/abs/2602.04789v1",
    "pdf_url": "https://arxiv.org/pdf/2602.04789v1",
    "published_date": "2026-02-04",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/chengtao-lv/LightForcing",
    "keywords": [
      "video diffusion",
      "dit",
      "video generation",
      "autoregressive",
      "efficient"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2602.04789v1",
      "pdf": "https://arxiv.org/pdf/2602.04789v1",
      "github": "https://github.com/chengtao-lv/LightForcing"
    },
    "bibtex": ""
  },
  {
    "title": "Euphonium: Steering Video Flow Matching via Process Reward Gradient Guided Stochastic Dynamics",
    "authors": [
      "Ruizhe Zhong",
      "Jiesong Lian",
      "Xiaoyue Mi",
      "Zixiang Zhou",
      "Yuan Zhou",
      "Qinglin Lu",
      "Junchi Yan"
    ],
    "abstract": "While online Reinforcement Learning has emerged as a crucial technique for aligning flow matching models with human preferences, current approaches are hindered by inefficient exploration during training rollouts. Relying on undirected stochasticity and sparse outcome rewards, these methods struggle to discover high-reward samples, resulting in data-inefficient and slow optimization. To address these limitations, we propose Euphonium, a novel framework that steers generation via process reward gradient guided dynamics. Our key insight is to formulate the sampling process as a theoretically principled Stochastic Differential Equation that explicitly incorporates the gradient of a Process Reward Model into the flow drift. This design enables dense, step-by-step steering toward high-reward regions, advancing beyond the unguided exploration in prior works, and theoretically encompasses existing sampling methods (e.g., Flow-GRPO, DanceGRPO) as special cases. We further derive a distillation objective that internalizes the guidance signal into the flow network, eliminating inference-time dependency on the reward model. We instantiate this framework with a Dual-Reward Group Relative Policy Optimization algorithm, combining latent process rewards for efficient credit assignment with pixel-level outcome rewards for final visual fidelity. Experiments on text-to-video generation show that Euphonium achieves better alignment compared to existing methods while accelerating training convergence by 1.66x. Our code is available at https://github.com/zerzerzerz/Euphonium",
    "arxiv_url": "https://arxiv.org/abs/2602.04928v2",
    "pdf_url": "https://arxiv.org/pdf/2602.04928v2",
    "published_date": "2026-02-04",
    "categories": [
      "cs.LG"
    ],
    "github_url": "https://github.com/zerzerzerz/Euphonium",
    "keywords": [
      "distillation",
      "flow matching",
      "dit",
      "text-to-video",
      "video generation",
      "dynamics",
      "efficient"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2602.04928v2",
      "pdf": "https://arxiv.org/pdf/2602.04928v2",
      "github": "https://github.com/zerzerzerz/Euphonium"
    },
    "bibtex": ""
  },
  {
    "title": "SkeletonGaussian: Editable 4D Generation through Gaussian Skeletonization",
    "authors": [
      "Lifan Wu",
      "Ruijie Zhu",
      "Yubo Ai",
      "Tianzhu Zhang"
    ],
    "abstract": "4D generation has made remarkable progress in synthesizing dynamic 3D objects from input text, images, or videos. However, existing methods often represent motion as an implicit deformation field, which limits direct control and editability. To address this issue, we propose SkeletonGaussian, a novel framework for generating editable dynamic 3D Gaussians from monocular video input. Our approach introduces a hierarchical articulated representation that decomposes motion into sparse rigid motion explicitly driven by a skeleton and fine-grained non-rigid motion. Concretely, we extract a robust skeleton and drive rigid motion via linear blend skinning, followed by a hexplane-based refinement for non-rigid deformations, enhancing interpretability and editability. Experimental results demonstrate that SkeletonGaussian surpasses existing methods in generation quality while enabling intuitive motion editing, establishing a new paradigm for editable 4D generation. Project page: https://wusar.github.io/projects/skeletongaussian/",
    "arxiv_url": "https://arxiv.org/abs/2602.04271v1",
    "pdf_url": "https://arxiv.org/pdf/2602.04271v1",
    "published_date": "2026-02-04",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "4d generation",
      "dit"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2602.04271v1",
      "pdf": "https://arxiv.org/pdf/2602.04271v1",
      "project": "https://wusar.github.io/projects/skeletongaussian"
    },
    "bibtex": ""
  },
  {
    "title": "Adaptive 1D Video Diffusion Autoencoder",
    "authors": [
      "Yao Teng",
      "Minxuan Lin",
      "Xian Liu",
      "Shuai Wang",
      "Xiao Yang",
      "Xihui Liu"
    ],
    "abstract": "Recent video generation models largely rely on video autoencoders that compress pixel-space videos into latent representations. However, existing video autoencoders suffer from three major limitations: (1) fixed-rate compression that wastes tokens on simple videos, (2) inflexible CNN architectures that prevent variable-length latent modeling, and (3) deterministic decoders that struggle to recover appropriate details from compressed latents. To address these issues, we propose One-Dimensional Diffusion Video Autoencoder (One-DVA), a transformer-based framework for adaptive 1D encoding and diffusion-based decoding. The encoder employs query-based vision transformers to extract spatiotemporal features and produce latent representations, while a variable-length dropout mechanism dynamically adjusts the latent length. The decoder is a pixel-space diffusion transformer that reconstructs videos with the latents as input conditions. With a two-stage training strategy, One-DVA achieves performance comparable to 3D-CNN VAEs on reconstruction metrics at identical compression ratios. More importantly, it supports adaptive compression and thus can achieve higher compression ratios. To better support downstream latent generation, we further regularize the One-DVA latent distribution for generative modeling and fine-tune its decoder to mitigate artifacts caused by the generation process.",
    "arxiv_url": "https://arxiv.org/abs/2602.04220v1",
    "pdf_url": "https://arxiv.org/pdf/2602.04220v1",
    "published_date": "2026-02-04",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "video diffusion",
      "dit",
      "diffusion transformer",
      "video generation",
      "architecture"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2602.04220v1",
      "pdf": "https://arxiv.org/pdf/2602.04220v1"
    },
    "bibtex": ""
  },
  {
    "title": "VTok: A Unified Video Tokenizer with Decoupled Spatial-Temporal Latents",
    "authors": [
      "Feng Wang",
      "Yichun Shi",
      "Ceyuan Yang",
      "Qiushan Guo",
      "Jingxiang Sun",
      "Alan Yuille",
      "Peng Wang"
    ],
    "abstract": "This work presents VTok, a unified video tokenization framework that can be used for both generation and understanding tasks. Unlike the leading vision-language systems that tokenize videos through a naive frame-sampling strategy, we propose to decouple the spatial and temporal representations of videos by retaining the spatial features of a single key frame while encoding each subsequent frame into a single residual token, achieving compact yet expressive video tokenization. Our experiments suggest that VTok effectively reduces the complexity of video representation from the product of frame count and per-frame token count to their sum, while the residual tokens sufficiently capture viewpoint and motion changes relative to the key frame. Extensive evaluations demonstrate the efficacy and efficiency of VTok: it achieves notably higher performance on a range of video understanding and text-to-video generation benchmarks compared with baselines using naive tokenization, all with shorter token sequences per video (e.g., 3.4% higher accuracy on our TV-Align benchmark and 1.9% higher VBench score). Remarkably, VTok produces more coherent motion and stronger guidance following in text-to-video generation, owing to its more consistent temporal encoding. We hope VTok can serve as a standardized video tokenization paradigm for future research in video understanding and generation.",
    "arxiv_url": "https://arxiv.org/abs/2602.04202v1",
    "pdf_url": "https://arxiv.org/pdf/2602.04202v1",
    "published_date": "2026-02-04",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "benchmark",
      "evaluation",
      "text-to-video",
      "video generation"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2602.04202v1",
      "pdf": "https://arxiv.org/pdf/2602.04202v1"
    },
    "bibtex": ""
  },
  {
    "title": "WIND: Weather Inverse Diffusion for Zero-Shot Atmospheric Modeling",
    "authors": [
      "Michael Aich",
      "Andreas Frst",
      "Florian Sestak",
      "Carlos Ruiz-Gonzalez",
      "Niklas Boers",
      "Johannes Brandstetter"
    ],
    "abstract": "Deep learning has revolutionized weather and climate modeling, yet the current landscape remains fragmented: highly specialized models are typically trained individually for distinct tasks. To unify this landscape, we introduce WIND, a single pre-trained foundation model capable of replacing specialized baselines across a vast array of tasks. Crucially, in contrast to previous atmospheric foundation models, we achieve this without any task-specific fine-tuning. To learn a robust, task-agnostic prior of the atmosphere, we pre-train WIND with a self-supervised video reconstruction objective, utilizing an unconditional video diffusion model to iteratively reconstruct atmospheric dynamics from a noisy state. At inference, we frame diverse domain-specific problems strictly as inverse problems and solve them via posterior sampling. This unified approach allows us to tackle highly relevant weather and climate problems, including probabilistic forecasting, spatial and temporal downscaling, sparse reconstruction and enforcing conservation laws purely with our pre-trained model. We further demonstrate the model's capacity to generate physically consistent counterfactual storylines of extreme weather events under global warming scenarios. By combining generative video modeling with inverse problem solving, WIND offers a computationally efficient paradigm shift in AI-based atmospheric modeling.",
    "arxiv_url": "https://arxiv.org/abs/2602.03924v1",
    "pdf_url": "https://arxiv.org/pdf/2602.03924v1",
    "published_date": "2026-02-03",
    "categories": [
      "cs.LG",
      "cs.AI",
      "physics.ao-ph"
    ],
    "github_url": "",
    "keywords": [
      "video diffusion",
      "dit",
      "physical",
      "diffusion model",
      "dynamics",
      "efficient"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2602.03924v1",
      "pdf": "https://arxiv.org/pdf/2602.03924v1"
    },
    "bibtex": ""
  },
  {
    "title": "PrevizWhiz: Combining Rough 3D Scenes and 2D Video to Guide Generative Video Previsualization",
    "authors": [
      "Erzhen Hu",
      "Frederik Brudy",
      "David Ledo",
      "George Fitzmaurice",
      "Fraser Anderson"
    ],
    "abstract": "In pre-production, filmmakers and 3D animation experts must rapidly prototype ideas to explore a film's possibilities before fullscale production, yet conventional approaches involve trade-offs in efficiency and expressiveness. Hand-drawn storyboards often lack spatial precision needed for complex cinematography, while 3D previsualization demands expertise and high-quality rigged assets. To address this gap, we present PrevizWhiz, a system that leverages rough 3D scenes in combination with generative image and video models to create stylized video previews. The workflow integrates frame-level image restyling with adjustable resemblance, time-based editing through motion paths or external video inputs, and refinement into high-fidelity video clips. A study with filmmakers demonstrates that our system lowers technical barriers for film-makers, accelerates creative iteration, and effectively bridges the communication gap, while also surfacing challenges of continuity, authorship, and ethical consideration in AI-assisted filmmaking.",
    "arxiv_url": "https://arxiv.org/abs/2602.03838v1",
    "pdf_url": "https://arxiv.org/pdf/2602.03838v1",
    "published_date": "2026-02-03",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "creative",
      "film",
      "dit"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2602.03838v1",
      "pdf": "https://arxiv.org/pdf/2602.03838v1"
    },
    "bibtex": ""
  },
  {
    "title": "Continuous Control of Editing Models via Adaptive-Origin Guidance",
    "authors": [
      "Alon Wolf",
      "Chen Katzir",
      "Kfir Aberman",
      "Or Patashnik"
    ],
    "abstract": "Diffusion-based editing models have emerged as a powerful tool for semantic image and video manipulation. However, existing models lack a mechanism for smoothly controlling the intensity of text-guided edits. In standard text-conditioned generation, Classifier-Free Guidance (CFG) impacts prompt adherence, suggesting it as a potential control for edit intensity in editing models. However, we show that scaling CFG in these models does not produce a smooth transition between the input and the edited result. We attribute this behavior to the unconditional prediction, which serves as the guidance origin and dominates the generation at low guidance scales, while representing an arbitrary manipulation of the input content. To enable continuous control, we introduce Adaptive-Origin Guidance (AdaOr), a method that adjusts this standard guidance origin with an identity-conditioned adaptive origin, using an identity instruction corresponding to the identity manipulation. By interpolating this identity prediction with the standard unconditional prediction according to the edit strength, we ensure a continuous transition from the input to the edited result. We evaluate our method on image and video editing tasks, demonstrating that it provides smoother and more consistent control compared to current slider-based editing approaches. Our method incorporates an identity instruction into the standard training framework, enabling fine-grained control at inference time without per-edit procedure or reliance on specialized datasets.",
    "arxiv_url": "https://arxiv.org/abs/2602.03826v1",
    "pdf_url": "https://arxiv.org/pdf/2602.03826v1",
    "published_date": "2026-02-03",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "video editing",
      "identity",
      "video manipulation",
      "dit"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2602.03826v1",
      "pdf": "https://arxiv.org/pdf/2602.03826v1"
    },
    "bibtex": ""
  },
  {
    "title": "3D-Aware Implicit Motion Control for View-Adaptive Human Video Generation",
    "authors": [
      "Zhixue Fang",
      "Xu He",
      "Songlin Tang",
      "Haoxian Zhang",
      "Qingfeng Li",
      "Xiaoqiang Liu",
      "Pengfei Wan",
      "Kun Gai"
    ],
    "abstract": "Existing methods for human motion control in video generation typically rely on either 2D poses or explicit 3D parametric models (e.g., SMPL) as control signals. However, 2D poses rigidly bind motion to the driving viewpoint, precluding novel-view synthesis. Explicit 3D models, though structurally informative, suffer from inherent inaccuracies (e.g., depth ambiguity and inaccurate dynamics) which, when used as a strong constraint, override the powerful intrinsic 3D awareness of large-scale video generators. In this work, we revisit motion control from a 3D-aware perspective, advocating for an implicit, view-agnostic motion representation that naturally aligns with the generator's spatial priors rather than depending on externally reconstructed constraints. We introduce 3DiMo, which jointly trains a motion encoder with a pretrained video generator to distill driving frames into compact, view-agnostic motion tokens, injected semantically via cross-attention. To foster 3D awareness, we train with view-rich supervision (i.e., single-view, multi-view, and moving-camera videos), forcing motion consistency across diverse viewpoints. Additionally, we use auxiliary geometric supervision that leverages SMPL only for early initialization and is annealed to zero, enabling the model to transition from external 3D guidance to learning genuine 3D spatial motion understanding from the data and the generator's priors. Experiments confirm that 3DiMo faithfully reproduces driving motions with flexible, text-driven camera control, significantly surpassing existing methods in both motion fidelity and visual quality.",
    "arxiv_url": "https://arxiv.org/abs/2602.03796v1",
    "pdf_url": "https://arxiv.org/pdf/2602.03796v1",
    "published_date": "2026-02-03",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d-aware",
      "dit",
      "video generation",
      "dynamics",
      "human motion",
      "motion control",
      "camera control"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2602.03796v1",
      "pdf": "https://arxiv.org/pdf/2602.03796v1"
    },
    "bibtex": ""
  },
  {
    "title": "BridgeV2W: Bridging Video Generation Models to Embodied World Models via Embodiment Masks",
    "authors": [
      "Yixiang Chen",
      "Peiyan Li",
      "Jiabing Yang",
      "Keji He",
      "Xiangnan Wu",
      "Yuan Xu",
      "Kai Wang",
      "Jing Liu",
      "Nianfeng Liu",
      "Yan Huang",
      "Liang Wang"
    ],
    "abstract": "Embodied world models have emerged as a promising paradigm in robotics, most of which leverage large-scale Internet videos or pretrained video generation models to enrich visual and motion priors. However, they still face key challenges: a misalignment between coordinate-space actions and pixel-space videos, sensitivity to camera viewpoint, and non-unified architectures across embodiments. To this end, we present BridgeV2W, which converts coordinate-space actions into pixel-aligned embodiment masks rendered from the URDF and camera parameters. These masks are then injected into a pretrained video generation model via a ControlNet-style pathway, which aligns the action control signals with predicted videos, adds view-specific conditioning to accommodate camera viewpoints, and yields a unified world model architecture across embodiments. To mitigate overfitting to static backgrounds, BridgeV2W further introduces a flow-based motion loss that focuses on learning dynamic and task-relevant regions. Experiments on single-arm (DROID) and dual-arm (AgiBot-G1) datasets, covering diverse and challenging conditions with unseen viewpoints and scenes, show that BridgeV2W improves video generation quality compared to prior state-of-the-art methods. We further demonstrate the potential of BridgeV2W on downstream real-world tasks, including policy evaluation and goal-conditioned planning. More results can be found on our project website at https://BridgeV2W.github.io .",
    "arxiv_url": "https://arxiv.org/abs/2602.03793v1",
    "pdf_url": "https://arxiv.org/pdf/2602.03793v1",
    "published_date": "2026-02-03",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "dit",
      "world model",
      "video generation",
      "robotics",
      "evaluation",
      "architecture",
      "style"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2602.03793v1",
      "pdf": "https://arxiv.org/pdf/2602.03793v1",
      "project": "https://BridgeV2W.github.io"
    },
    "bibtex": ""
  },
  {
    "title": "Phaedra: Learning High-Fidelity Discrete Tokenization for the Physical Science",
    "authors": [
      "Levi Lingsch",
      "Georgios Kissas",
      "Johannes Jakubik",
      "Siddhartha Mishra"
    ],
    "abstract": "Tokens are discrete representations that allow modern deep learning to scale by transforming high-dimensional data into sequences that can be efficiently learned, generated, and generalized to new tasks. These have become foundational for image and video generation and, more recently, physical simulation. As existing tokenizers are designed for the explicit requirements of realistic visual perception of images, it is necessary to ask whether these approaches are optimal for scientific images, which exhibit a large dynamic range and require token embeddings to retain physical and spectral properties. In this work, we investigate the accuracy of a suite of image tokenizers across a range of metrics designed to measure the fidelity of PDE properties in both physical and spectral space. Based on the observation that these struggle to capture both fine details and precise magnitudes, we propose Phaedra, inspired by classical shape-gain quantization and proper orthogonal decomposition. We demonstrate that Phaedra consistently improves reconstruction across a range of PDE datasets. Additionally, our results show strong out-of-distribution generalization capabilities to three tasks of increasing complexity, namely known PDEs with different conditions, unknown PDEs, and real-world Earth observation and weather data.",
    "arxiv_url": "https://arxiv.org/abs/2602.03915v1",
    "pdf_url": "https://arxiv.org/pdf/2602.03915v1",
    "published_date": "2026-02-03",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CE",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "physical simulation",
      "dit",
      "physical",
      "video generation",
      "simulation",
      "efficient"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2602.03915v1",
      "pdf": "https://arxiv.org/pdf/2602.03915v1"
    },
    "bibtex": ""
  },
  {
    "title": "Tiled Prompts: Overcoming Prompt Underspecification in Image and Video Super-Resolution",
    "authors": [
      "Bryan Sangwoo Kim",
      "Jonghyun Park",
      "Jong Chul Ye"
    ],
    "abstract": "Text-conditioned diffusion models have advanced image and video super-resolution by using prompts as semantic priors, but modern super-resolution pipelines typically rely on latent tiling to scale to high resolutions, where a single global caption causes prompt underspecification. A coarse global prompt often misses localized details (prompt sparsity) and provides locally irrelevant guidance (prompt misguidance) that can be amplified by classifier-free guidance. We propose Tiled Prompts, a unified framework for image and video super-resolution that generates a tile-specific prompt for each latent tile and performs super-resolution under locally text-conditioned posteriors, providing high-information guidance that resolves prompt underspecification with minimal overhead. Experiments on high resolution real-world images and videos show consistent gains in perceptual quality and text alignment, while reducing hallucinations and tile-level artifacts relative to global-prompt baselines.",
    "arxiv_url": "https://arxiv.org/abs/2602.03342v1",
    "pdf_url": "https://arxiv.org/pdf/2602.03342v1",
    "published_date": "2026-02-03",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "super-resolution",
      "diffusion model",
      "dit"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2602.03342v1",
      "pdf": "https://arxiv.org/pdf/2602.03342v1"
    },
    "bibtex": ""
  },
  {
    "title": "InstaDrive: Instance-Aware Driving World Models for Realistic and Consistent Video Generation",
    "authors": [
      "Zhuoran Yang",
      "Xi Guo",
      "Chenjing Ding",
      "Chiyu Wang",
      "Wei Wu",
      "Yanyong Zhang"
    ],
    "abstract": "Autonomous driving relies on robust models trained on high-quality, large-scale multi-view driving videos. While world models offer a cost-effective solution for generating realistic driving videos, they struggle to maintain instance-level temporal consistency and spatial geometric fidelity. To address these challenges, we propose InstaDrive, a novel framework that enhances driving video realism through two key advancements: (1) Instance Flow Guider, which extracts and propagates instance features across frames to enforce temporal consistency, preserving instance identity over time. (2) Spatial Geometric Aligner, which improves spatial reasoning, ensures precise instance positioning, and explicitly models occlusion hierarchies. By incorporating these instance-aware mechanisms, InstaDrive achieves state-of-the-art video generation quality and enhances downstream autonomous driving tasks on the nuScenes dataset. Additionally, we utilize CARLA's autopilot to procedurally and stochastically simulate rare but safety-critical driving scenarios across diverse maps and regions, enabling rigorous safety evaluation for autonomous systems. Our project page is https://shanpoyang654.github.io/InstaDrive/page.html.",
    "arxiv_url": "https://arxiv.org/abs/2602.03242v1",
    "pdf_url": "https://arxiv.org/pdf/2602.03242v1",
    "published_date": "2026-02-03",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "temporal consistency",
      "dit",
      "world model",
      "identity",
      "video generation",
      "autonomous driving",
      "evaluation"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2602.03242v1",
      "pdf": "https://arxiv.org/pdf/2602.03242v1",
      "project": "https://shanpoyang654.github.io/InstaDrive/page.html"
    },
    "bibtex": ""
  },
  {
    "title": "ConsisDrive: Identity-Preserving Driving World Models for Video Generation by Instance Mask",
    "authors": [
      "Zhuoran Yang",
      "Yanyong Zhang"
    ],
    "abstract": "Autonomous driving relies on robust models trained on large-scale, high-quality multi-view driving videos. Although world models provide a cost-effective solution for generating realistic driving data, they often suffer from identity drift, where the same object changes its appearance or category across frames due to the absence of instance-level temporal constraints. We introduce ConsisDrive, an identity-preserving driving world model designed to enforce temporal consistency at the instance level. Our framework incorporates two key components: (1) Instance-Masked Attention, which applies instance identity masks and trajectory masks within attention blocks to ensure that visual tokens interact only with their corresponding instance features across spatial and temporal dimensions, thereby preserving object identity consistency; and (2) Instance-Masked Loss, which adaptively emphasizes foreground regions with probabilistic instance masking, reducing background noise while maintaining overall scene fidelity. By integrating these mechanisms, ConsisDrive achieves state-of-the-art driving video generation quality and demonstrates significant improvements in downstream autonomous driving tasks on the nuScenes dataset. Our project page is https://shanpoyang654.github.io/ConsisDrive/page.html.",
    "arxiv_url": "https://arxiv.org/abs/2602.03213v2",
    "pdf_url": "https://arxiv.org/pdf/2602.03213v2",
    "published_date": "2026-02-03",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "temporal consistency",
      "world model",
      "identity",
      "trajectory",
      "video generation",
      "autonomous driving"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2602.03213v2",
      "pdf": "https://arxiv.org/pdf/2602.03213v2",
      "project": "https://shanpoyang654.github.io/ConsisDrive/page.html"
    },
    "bibtex": ""
  },
  {
    "title": "Quant VideoGen: Auto-Regressive Long Video Generation via 2-Bit KV-Cache Quantization",
    "authors": [
      "Haocheng Xi",
      "Shuo Yang",
      "Yilong Zhao",
      "Muyang Li",
      "Han Cai",
      "Xingyang Li",
      "Yujun Lin",
      "Zhuoyang Zhang",
      "Jintao Zhang",
      "Xiuyu Li",
      "Zhiying Xu",
      "Jun Wu",
      "Chenfeng Xu",
      "Ion Stoica",
      "Song Han",
      "Kurt Keutzer"
    ],
    "abstract": "Despite rapid progress in autoregressive video diffusion, an emerging system algorithm bottleneck limits both deployability and generation capability: KV cache memory. In autoregressive video generation models, the KV cache grows with generation history and quickly dominates GPU memory, often exceeding 30 GB, preventing deployment on widely available hardware. More critically, constrained KV cache budgets restrict the effective working memory, directly degrading long horizon consistency in identity, layout, and motion. To address this challenge, we present Quant VideoGen (QVG), a training free KV cache quantization framework for autoregressive video diffusion models. QVG leverages video spatiotemporal redundancy through Semantic Aware Smoothing, producing low magnitude, quantization friendly residuals. It further introduces Progressive Residual Quantization, a coarse to fine multi stage scheme that reduces quantization error while enabling a smooth quality memory trade off. Across LongCat Video, HY WorldPlay, and Self Forcing benchmarks, QVG establishes a new Pareto frontier between quality and memory efficiency, reducing KV cache memory by up to 7.0 times with less than 4% end to end latency overhead while consistently outperforming existing baselines in generation quality.",
    "arxiv_url": "https://arxiv.org/abs/2602.02958v1",
    "pdf_url": "https://arxiv.org/pdf/2602.02958v1",
    "published_date": "2026-02-03",
    "categories": [
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "layout",
      "video diffusion",
      "identity",
      "video generation",
      "diffusion model",
      "long video",
      "autoregressive",
      "benchmark"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2602.02958v1",
      "pdf": "https://arxiv.org/pdf/2602.02958v1"
    },
    "bibtex": ""
  },
  {
    "title": "Thinking with Comics: Enhancing Multimodal Reasoning through Structured Visual Storytelling",
    "authors": [
      "Andong Chen",
      "Wenxin Zhu",
      "Qiuyu Ding",
      "Yuchen Song",
      "Muyun Yang",
      "Tiejun Zhao"
    ],
    "abstract": "Chain-of-Thought reasoning has driven large language models to extend from thinking with text to thinking with images and videos. However, different modalities still have clear limitations: static images struggle to represent temporal structure, while videos introduce substantial redundancy and computational cost. In this work, we propose Thinking with Comics, a visual reasoning paradigm that uses comics as a high information-density medium positioned between images and videos. Comics preserve temporal structure, embedded text, and narrative coherence while requiring significantly lower reasoning cost. We systematically study two reasoning paths based on comics and evaluate them on a range of reasoning tasks and long-context understanding tasks. Experimental results show that Thinking with Comics outperforms Thinking with Images on multi-step temporal and causal reasoning tasks, while remaining substantially more efficient than Thinking with Video. Further analysis indicates that different comic narrative structures and styles consistently affect performance across tasks, suggesting that comics serve as an effective intermediate visual representation for improving multimodal reasoning.",
    "arxiv_url": "https://arxiv.org/abs/2602.02453v2",
    "pdf_url": "https://arxiv.org/pdf/2602.02453v2",
    "published_date": "2026-02-02",
    "categories": [
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "style"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2602.02453v2",
      "pdf": "https://arxiv.org/pdf/2602.02453v2"
    },
    "bibtex": ""
  },
  {
    "title": "RANKVIDEO: Reasoning Reranking for Text-to-Video Retrieval",
    "authors": [
      "Tyler Skow",
      "Alexander Martin",
      "Benjamin Van Durme",
      "Rama Chellappa",
      "Reno Kriz"
    ],
    "abstract": "Reranking is a critical component of modern retrieval systems, which typically pair an efficient first-stage retriever with a more expressive model to refine results. While large reasoning models have driven rapid progress in text-centric reranking, reasoning-based reranking for video retrieval remains underexplored. To address this gap, we introduce RANKVIDEO, a reasoning-based reranker for video retrieval that explicitly reasons over query-video pairs using video content to assess relevance. RANKVIDEO is trained using a two-stage curriculum consisting of perception-grounded supervised fine-tuning followed by reranking training that combines pointwise, pairwise, and teacher confidence distillation objectives, and is supported by a data synthesis pipeline for constructing reasoning-intensive query-video pairs. Experiments on the large-scale MultiVENT 2.0 benchmark demonstrate that RANKVIDEO consistently improves retrieval performance within a two-stage framework, yielding an average improvement of 31% on nDCG@10 and outperforming text-only and vision-language reranking alternatives, while more efficient.",
    "arxiv_url": "https://arxiv.org/abs/2602.02444v2",
    "pdf_url": "https://arxiv.org/pdf/2602.02444v2",
    "published_date": "2026-02-02",
    "categories": [
      "cs.IR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "benchmark",
      "text-to-video",
      "efficient",
      "distillation"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2602.02444v2",
      "pdf": "https://arxiv.org/pdf/2602.02444v2"
    },
    "bibtex": ""
  },
  {
    "title": "Unified Personalized Reward Model for Vision Generation",
    "authors": [
      "Yibin Wang",
      "Yuhang Zang",
      "Feng Han",
      "Jiazi Bu",
      "Yujie Zhou",
      "Cheng Jin",
      "Jiaqi Wang"
    ],
    "abstract": "Recent advancements in multimodal reward models (RMs) have significantly propelled the development of visual generation. Existing frameworks typically adopt Bradley-Terry-style preference modeling or leverage generative VLMs as judges, and subsequently optimize visual generation models via reinforcement learning. However, current RMs suffer from inherent limitations: they often follow a one-size-fits-all paradigm that assumes a monolithic preference distribution or relies on fixed evaluation rubrics. As a result, they are insensitive to content-specific visual cues, leading to systematic misalignment with subjective and context-dependent human preferences. To this end, inspired by human assessment, we propose UnifiedReward-Flex, a unified personalized reward model for vision generation that couples reward modeling with flexible and context-adaptive reasoning. Specifically, given a prompt and the generated visual content, it first interprets the semantic intent and grounds on visual evidence, then dynamically constructs a hierarchical assessment by instantiating fine-grained criteria under both predefined and self-generated high-level dimensions. Our training pipeline follows a two-stage process: (1) we first distill structured, high-quality reasoning traces from advanced closed-source VLMs to bootstrap SFT, equipping the model with flexible and context-adaptive reasoning behaviors; (2) we then perform direct preference optimization (DPO) on carefully curated preference pairs to further strengthen reasoning fidelity and discriminative alignment. To validate the effectiveness, we integrate UnifiedReward-Flex into the GRPO framework for image and video synthesis, and extensive results demonstrate its superiority.",
    "arxiv_url": "https://arxiv.org/abs/2602.02380v1",
    "pdf_url": "https://arxiv.org/pdf/2602.02380v1",
    "published_date": "2026-02-02",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "evaluation",
      "video synthesis",
      "style"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2602.02380v1",
      "pdf": "https://arxiv.org/pdf/2602.02380v1"
    },
    "bibtex": ""
  },
  {
    "title": "Causal Forcing: Autoregressive Diffusion Distillation Done Right for High-Quality Real-Time Interactive Video Generation",
    "authors": [
      "Hongzhou Zhu",
      "Min Zhao",
      "Guande He",
      "Hang Su",
      "Chongxuan Li",
      "Jun Zhu"
    ],
    "abstract": "To achieve real-time interactive video generation, current methods distill pretrained bidirectional video diffusion models into few-step autoregressive (AR) models, facing an architectural gap when full attention is replaced by causal attention. However, existing approaches do not bridge this gap theoretically. They initialize the AR student via ODE distillation, which requires frame-level injectivity, where each noisy frame must map to a unique clean frame under the PF-ODE of an AR teacher. Distilling an AR student from a bidirectional teacher violates this condition, preventing recovery of the teacher's flow map and instead inducing a conditional-expectation solution, which degrades performance. To address this issue, we propose Causal Forcing that uses an AR teacher for ODE initialization, thereby bridging the architectural gap. Empirical results show that our method outperforms all baselines across all metrics, surpassing the SOTA Self Forcing by 19.3\\% in Dynamic Degree, 8.7\\% in VisionReward, and 16.7\\% in Instruction Following. Project page and the code: \\href{https://thu-ml.github.io/CausalForcing.github.io/}{https://thu-ml.github.io/CausalForcing.github.io/}",
    "arxiv_url": "https://arxiv.org/abs/2602.02214v2",
    "pdf_url": "https://arxiv.org/pdf/2602.02214v2",
    "published_date": "2026-02-02",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "interactive",
      "video diffusion",
      "distillation",
      "dit",
      "video generation",
      "diffusion model",
      "autoregressive"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2602.02214v2",
      "pdf": "https://arxiv.org/pdf/2602.02214v2",
      "project": "https://thu-ml.github.io/CausalForcing.github.io"
    },
    "bibtex": ""
  },
  {
    "title": "FSVideo: Fast Speed Video Diffusion Model in a Highly-Compressed Latent Space",
    "authors": [
      "FSVideo Team",
      "Qingyu Chen",
      "Zhiyuan Fang",
      "Haibin Huang",
      "Xinwei Huang",
      "Tong Jin",
      "Minxuan Lin",
      "Bo Liu",
      "Celong Liu",
      "Chongyang Ma",
      "Xing Mei",
      "Xiaohui Shen",
      "Yaojie Shen",
      "Fuwen Tan",
      "Angtian Wang",
      "Xiao Yang",
      "Yiding Yang",
      "Jiamin Yuan",
      "Lingxi Zhang",
      "Yuxin Zhang"
    ],
    "abstract": "We introduce FSVideo, a fast speed transformer-based image-to-video (I2V) diffusion framework. We build our framework on the following key components: 1.) a new video autoencoder with highly-compressed latent space ($64\\times64\\times4$ spatial-temporal downsampling ratio), achieving competitive reconstruction quality; 2.) a diffusion transformer (DIT) architecture with a new layer memory design to enhance inter-layer information flow and context reuse within DIT, and 3.) a multi-resolution generation strategy via a few-step DIT upsampler to increase video fidelity. Our final model, which contains a 14B DIT base model and a 14B DIT upsampler, achieves competitive performance against other popular open-source models, while being an order of magnitude faster. We discuss our model design as well as training strategies in this report.",
    "arxiv_url": "https://arxiv.org/abs/2602.02092v1",
    "pdf_url": "https://arxiv.org/pdf/2602.02092v1",
    "published_date": "2026-02-02",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "video diffusion",
      "dit",
      "diffusion transformer",
      "image-to-video",
      "i2v",
      "diffusion model",
      "architecture"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2602.02092v1",
      "pdf": "https://arxiv.org/pdf/2602.02092v1"
    },
    "bibtex": ""
  },
  {
    "title": "Grounding Generated Videos in Feasible Plans via World Models",
    "authors": [
      "Christos Ziakas",
      "Amir Bar",
      "Alessandra Russo"
    ],
    "abstract": "Large-scale video generative models have shown emerging capabilities as zero-shot visual planners, yet video-generated plans often violate temporal consistency and physical constraints, leading to failures when mapped to executable actions. To address this, we propose Grounding Video Plans with World Models (GVP-WM), a planning method that grounds video-generated plans into feasible action sequences using a learned action-conditioned world model. At test-time, GVP-WM first generates a video plan from initial and goal observations, then projects the video guidance onto the manifold of dynamically feasible latent trajectories via video-guided latent collocation. In particular, we formulate grounding as a goal-conditioned latent-space trajectory optimization problem that jointly optimizes latent states and actions under world-model dynamics, while preserving semantic alignment with the video-generated plan. Empirically, GVP-WM recovers feasible long-horizon plans from zero-shot image-to-video-generated and motion-blurred videos that violate physical constraints, across navigation and manipulation simulation tasks.",
    "arxiv_url": "https://arxiv.org/abs/2602.01960v1",
    "pdf_url": "https://arxiv.org/pdf/2602.01960v1",
    "published_date": "2026-02-02",
    "categories": [
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "temporal consistency",
      "dit",
      "world model",
      "physical",
      "trajectory",
      "image-to-video",
      "dynamics",
      "simulation",
      "action-conditioned"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2602.01960v1",
      "pdf": "https://arxiv.org/pdf/2602.01960v1"
    },
    "bibtex": ""
  },
  {
    "title": "ProxyImg: Towards Highly-Controllable Image Representation via Hierarchical Disentangled Proxy Embedding",
    "authors": [
      "Ye Chen",
      "Yupeng Zhu",
      "Xiongzhen Zhang",
      "Zhewen Wan",
      "Yingzhe Li",
      "Wenjun Zhang",
      "Bingbing Ni"
    ],
    "abstract": "Prevailing image representation methods, including explicit representations such as raster images and Gaussian primitives, as well as implicit representations such as latent images, either suffer from representation redundancy that leads to heavy manual editing effort, or lack a direct mapping from latent variables to semantic instances or parts, making fine-grained manipulation difficult. These limitations hinder efficient and controllable image and video editing. To address these issues, we propose a hierarchical proxy-based parametric image representation that disentangles semantic, geometric, and textural attributes into independent and manipulable parameter spaces. Based on a semantic-aware decomposition of the input image, our representation constructs hierarchical proxy geometries through adaptive Bezier fitting and iterative internal region subdivision and meshing. Multi-scale implicit texture parameters are embedded into the resulting geometry-aware distributed proxy nodes, enabling continuous high-fidelity reconstruction in the pixel domain and instance- or part-independent semantic editing. In addition, we introduce a locality-adaptive feature indexing mechanism to ensure spatial texture coherence, which further supports high-quality background completion without relying on generative models. Extensive experiments on image reconstruction and editing benchmarks, including ImageNet, OIR-Bench, and HumanEdit, demonstrate that our method achieves state-of-the-art rendering fidelity with significantly fewer parameters, while enabling intuitive, interactive, and physically plausible manipulation. Moreover, by integrating proxy nodes with Position-Based Dynamics, our framework supports real-time physics-driven animation using lightweight implicit rendering, achieving superior temporal consistency and visual realism compared with generative approaches.",
    "arxiv_url": "https://arxiv.org/abs/2602.01881v1",
    "pdf_url": "https://arxiv.org/pdf/2602.01881v1",
    "published_date": "2026-02-02",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "interactive",
      "temporal consistency",
      "controllable",
      "dit",
      "video editing",
      "physical",
      "efficient",
      "dynamics",
      "benchmark",
      "physics"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2602.01881v1",
      "pdf": "https://arxiv.org/pdf/2602.01881v1"
    },
    "bibtex": ""
  },
  {
    "title": "WS-IMUBench: Can Weakly Supervised Methods from Audio, Image, and Video Be Adapted for IMU-based Temporal Action Localization?",
    "authors": [
      "Pei Li",
      "Jiaxi Yin",
      "Lei Ouyang",
      "Shihan Pan",
      "Ge Wang",
      "Han Ding",
      "Fei Wang"
    ],
    "abstract": "IMU-based Human Activity Recognition (HAR) has enabled a wide range of ubiquitous computing applications, yet its dominant clip classification paradigm cannot capture the rich temporal structure of real-world behaviors. This motivates a shift toward IMU Temporal Action Localization (IMU-TAL), which predicts both action categories and their start/end times in continuous streams. However, current progress is strongly bottlenecked by the need for dense, frame-level boundary annotations, which are costly and difficult to scale. To address this bottleneck, we introduce WS-IMUBench, a systematic benchmark study of weakly supervised IMU-TAL (WS-IMU-TAL) under only sequence-level labels. Rather than proposing a new localization algorithm, we evaluate how well established weakly supervised localization paradigms from audio, image, and video transfer to IMU-TAL under only sequence-level labels. We benchmark seven representative weakly supervised methods on seven public IMU datasets, resulting in over 3,540 model training runs and 7,080 inference evaluations. Guided by three research questions on transferability, effectiveness, and insights, our findings show that (i) transfer is modality-dependent, with temporal-domain methods generally more stable than image-derived proposal-based approaches; (ii) weak supervision can be competitive on favorable datasets (e.g., with longer actions and higher-dimensional sensing); and (iii) dominant failure modes arise from short actions, temporal ambiguity, and proposal quality. Finally, we outline concrete directions for advancing WS-IMU-TAL (e.g., IMU-specific proposal generation, boundary-aware objectives, and stronger temporal reasoning). Beyond individual results, WS-IMUBench establishes a reproducible benchmarking template, datasets, protocols, and analyses, to accelerate community-wide progress toward scalable WS-IMU-TAL.",
    "arxiv_url": "https://arxiv.org/abs/2602.01850v1",
    "pdf_url": "https://arxiv.org/pdf/2602.01850v1",
    "published_date": "2026-02-02",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "benchmark",
      "evaluation"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2602.01850v1",
      "pdf": "https://arxiv.org/pdf/2602.01850v1"
    },
    "bibtex": ""
  },
  {
    "title": "GPD: Guided Progressive Distillation for Fast and High-Quality Video Generation",
    "authors": [
      "Xiao Liang",
      "Yunzhu Zhang",
      "Linchao Zhu"
    ],
    "abstract": "Diffusion models have achieved remarkable success in video generation; however, the high computational cost of the denoising process remains a major bottleneck. Existing approaches have shown promise in reducing the number of diffusion steps, but they often suffer from significant quality degradation when applied to video generation. We propose Guided Progressive Distillation (GPD), a framework that accelerates the diffusion process for fast and high-quality video generation. GPD introduces a novel training strategy in which a teacher model progressively guides a student model to operate with larger step sizes. The framework consists of two key components: (1) an online-generated training target that reduces optimization difficulty while improving computational efficiency, and (2) frequency-domain constraints in the latent space that promote the preservation of fine-grained details and temporal dynamics. Applied to the Wan2.1 model, GPD reduces the number of sampling steps from 48 to 6 while maintaining competitive visual quality on VBench. Compared with existing distillation methods, GPD demonstrates clear advantages in both pipeline simplicity and quality preservation.",
    "arxiv_url": "https://arxiv.org/abs/2602.01814v1",
    "pdf_url": "https://arxiv.org/pdf/2602.01814v1",
    "published_date": "2026-02-02",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "distillation",
      "video generation",
      "diffusion model",
      "denoising",
      "dynamics"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2602.01814v1",
      "pdf": "https://arxiv.org/pdf/2602.01814v1"
    },
    "bibtex": ""
  },
  {
    "title": "Fast Autoregressive Video Diffusion and World Models with Temporal Cache Compression and Sparse Attention",
    "authors": [
      "Dvir Samuel",
      "Issar Tzachor",
      "Matan Levy",
      "Micahel Green",
      "Gal Chechik",
      "Rami Ben-Ari"
    ],
    "abstract": "Autoregressive video diffusion models enable streaming generation, opening the door to long-form synthesis, video world models, and interactive neural game engines. However, their core attention layers become a major bottleneck at inference time: as generation progresses, the KV cache grows, causing both increasing latency and escalating GPU memory, which in turn restricts usable temporal context and harms long-range consistency. In this work, we study redundancy in autoregressive video diffusion and identify three persistent sources: near-duplicate cached keys across frames, slowly evolving (largely semantic) queries/keys that make many attention computations redundant, and cross-attention over long prompts where only a small subset of tokens matters per frame. Building on these observations, we propose a unified, training-free attention framework for autoregressive diffusion: TempCache compresses the KV cache via temporal correspondence to bound cache growth; AnnCA accelerates cross-attention by selecting frame-relevant prompt tokens using fast approximate nearest neighbor (ANN) matching; and AnnSA sparsifies self-attention by restricting each query to semantically matched keys, also using a lightweight ANN. Together, these modules reduce attention, compute, and memory and are compatible with existing autoregressive diffusion backbones and world models. Experiments demonstrate up to x5--x10 end-to-end speedups while preserving near-identical visual quality and, crucially, maintaining stable throughput and nearly constant peak GPU memory usage over long rollouts, where prior methods progressively slow down and suffer from increasing memory usage.",
    "arxiv_url": "https://arxiv.org/abs/2602.01801v1",
    "pdf_url": "https://arxiv.org/pdf/2602.01801v1",
    "published_date": "2026-02-02",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "interactive",
      "video diffusion",
      "streaming",
      "world model",
      "long-form",
      "diffusion model",
      "autoregressive"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2602.01801v1",
      "pdf": "https://arxiv.org/pdf/2602.01801v1"
    },
    "bibtex": ""
  },
  {
    "title": "FastPhysGS: Accelerating Physics-based Dynamic 3DGS Simulation via Interior Completion and Adaptive Optimization",
    "authors": [
      "Yikun Ma",
      "Yiqing Li",
      "Jingwen Ye",
      "Zhongkai Wu",
      "Weidong Zhang",
      "Lin Gao",
      "Zhi Jin"
    ],
    "abstract": "Extending 3D Gaussian Splatting (3DGS) to 4D physical simulation remains challenging. Based on the Material Point Method (MPM), existing methods either rely on manual parameter tuning or distill dynamics from video diffusion models, limiting the generalization and optimization efficiency. Recent attempts using LLMs/VLMs suffer from a text/image-to-3D perceptual gap, yielding unstable physics behavior. In addition, they often ignore the surface structure of 3DGS, leading to implausible motion. We propose FastPhysGS, a fast and robust framework for physics-based dynamic 3DGS simulation:(1) Instance-aware Particle Filling (IPF) with Monte Carlo Importance Sampling (MCIS) to efficiently populate interior particles while preserving geometric fidelity; (2) Bidirectional Graph Decoupling Optimization (BGDO), an adaptive strategy that rapidly optimizes material parameters predicted from a VLM. Experiments show FastPhysGS achieves high-fidelity physical simulation in 1 minute using only 7 GB runtime memory, outperforming prior works with broad potential applications.",
    "arxiv_url": "https://arxiv.org/abs/2602.01723v1",
    "pdf_url": "https://arxiv.org/pdf/2602.01723v1",
    "published_date": "2026-02-02",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "physical simulation",
      "video diffusion",
      "dit",
      "physical",
      "diffusion model",
      "simulation",
      "dynamics",
      "efficient",
      "physics"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2602.01723v1",
      "pdf": "https://arxiv.org/pdf/2602.01723v1"
    },
    "bibtex": ""
  },
  {
    "title": "Meta Engine: A Unified Semantic Query Engine on Heterogeneous LLM-Based Query Systems",
    "authors": [
      "Ruyu Li",
      "Tinghui Zhang",
      "Haodi Ma",
      "Daisy Zhe Wang",
      "Yifan Wang"
    ],
    "abstract": "With the increasingly use of multi-modal data, semantic query has become more and more demanded in data management systems, which is an important way to access and analyze multi-modal data. As unstructured data, most information of multi-modal data (text, image, video, etc) hides in the semantics, which cannot be accessed by the traditional database queries like SQL.   Given the power of Large Language Model (LLM) in understanding semantics and processing natural language, in recent years several LLM-based semantic query systems have been proposed, to support semantic querying over unstructured data. However, this rapid growth has produced a fragmented ecosystem. Applications face significant integration challenges due to (1) disparate APIs of different semantic query systems and (2) a fundamental trade-off between specialization and generality. Many semantic query systems are highly specialized, offering state-of-the-art performance within a single modality but struggling with multi-modal data. Conversely, some \"all-in-one\" systems handle multiple modalities but often exhibit suboptimal performance compared to their specialized counterparts in specific modalities.   This paper introduces Meta Engine, a novel \"query system on query systems\", designed to resolve those aforementioned challenges. Meta Engine is a unified semantic query engine that integrates heterogeneous, specialized LLM-based query systems. Its architecture comprises five key components: (1) a Natural Language (NL) Query Parser, (2) an Operator Generator, (3) a Query Router, (4) a set of Adapters, and (5) a Result Aggregator. In the evaluation, Meta Engine consistently outperforms all baselines, yielding 3-6x higher F1 in most cases and up to 24x on specific datasets.",
    "arxiv_url": "https://arxiv.org/abs/2602.01701v1",
    "pdf_url": "https://arxiv.org/pdf/2602.01701v1",
    "published_date": "2026-02-02",
    "categories": [
      "cs.DB",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "evaluation",
      "architecture",
      "multi-modal",
      "dit"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2602.01701v1",
      "pdf": "https://arxiv.org/pdf/2602.01701v1"
    },
    "bibtex": ""
  },
  {
    "title": "VRGaussianAvatar: Integrating 3D Gaussian Avatars into VR",
    "authors": [
      "Hail Song",
      "Boram Yoon",
      "Seokhwan Yang",
      "Seoyoung Kang",
      "Hyunjeong Kim",
      "Henning Metzmacher",
      "Woontack Woo"
    ],
    "abstract": "We present VRGaussianAvatar, an integrated system that enables real-time full-body 3D Gaussian Splatting (3DGS) avatars in virtual reality using only head-mounted display (HMD) tracking signals. The system adopts a parallel pipeline with a VR Frontend and a GA Backend. The VR Frontend uses inverse kinematics to estimate full-body pose and streams the resulting pose along with stereo camera parameters to the backend. The GA Backend stereoscopically renders a 3DGS avatar reconstructed from a single image. To improve stereo rendering efficiency, we introduce Binocular Batching, which jointly processes left and right eye views in a single batched pass to reduce redundant computation and support high-resolution VR displays. We evaluate VRGaussianAvatar with quantitative performance tests and a within-subject user study against image- and video-based mesh avatar baselines. Results show that VRGaussianAvatar sustains interactive VR performance and yields higher perceived appearance similarity, embodiment, and plausibility. Project page and source code are available at https://vrgaussianavatar.github.io.",
    "arxiv_url": "https://arxiv.org/abs/2602.01674v1",
    "pdf_url": "https://arxiv.org/pdf/2602.01674v1",
    "published_date": "2026-02-02",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "avatar",
      "interactive"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2602.01674v1",
      "pdf": "https://arxiv.org/pdf/2602.01674v1",
      "project": "https://vrgaussianavatar.github.io"
    },
    "bibtex": ""
  },
  {
    "title": "PISCES: Annotation-free Text-to-Video Post-Training via Optimal Transport-Aligned Rewards",
    "authors": [
      "Minh-Quan Le",
      "Gaurav Mittal",
      "Cheng Zhao",
      "David Gu",
      "Dimitris Samaras",
      "Mei Chen"
    ],
    "abstract": "Text-to-video (T2V) generation aims to synthesize videos with high visual quality and temporal consistency that are semantically aligned with input text. Reward-based post-training has emerged as a promising direction to improve the quality and semantic alignment of generated videos. However, recent methods either rely on large-scale human preference annotations or operate on misaligned embeddings from pre-trained vision-language models, leading to limited scalability or suboptimal supervision. We present $\\texttt{PISCES}$, an annotation-free post-training algorithm that addresses these limitations via a novel Dual Optimal Transport (OT)-aligned Rewards module. To align reward signals with human judgment, $\\texttt{PISCES}$ uses OT to bridge text and video embeddings at both distributional and discrete token levels, enabling reward supervision to fulfill two objectives: (i) a Distributional OT-aligned Quality Reward that captures overall visual quality and temporal coherence; and (ii) a Discrete Token-level OT-aligned Semantic Reward that enforces semantic, spatio-temporal correspondence between text and video tokens. To our knowledge, $\\texttt{PISCES}$ is the first to improve annotation-free reward supervision in generative post-training through the lens of OT. Experiments on both short- and long-video generation show that $\\texttt{PISCES}$ outperforms both annotation-based and annotation-free methods on VBench across Quality and Semantic scores, with human preference studies further validating its effectiveness. We show that the Dual OT-aligned Rewards module is compatible with multiple optimization paradigms, including direct backpropagation and reinforcement learning fine-tuning.",
    "arxiv_url": "https://arxiv.org/abs/2602.01624v1",
    "pdf_url": "https://arxiv.org/pdf/2602.01624v1",
    "published_date": "2026-02-02",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "temporal consistency",
      "text-to-video",
      "video generation",
      "t2v"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2602.01624v1",
      "pdf": "https://arxiv.org/pdf/2602.01624v1"
    },
    "bibtex": ""
  },
  {
    "title": "Omni-Judge: Can Omni-LLMs Serve as Human-Aligned Judges for Text-Conditioned Audio-Video Generation?",
    "authors": [
      "Susan Liang",
      "Chao Huang",
      "Filippos Bellos",
      "Yolo Yunlong Tang",
      "Qianxiang Shen",
      "Jing Bi",
      "Luchuan Song",
      "Zeliang Zhang",
      "Jason Corso",
      "Chenliang Xu"
    ],
    "abstract": "State-of-the-art text-to-video generation models such as Sora 2 and Veo 3 can now produce high-fidelity videos with synchronized audio directly from a textual prompt, marking a new milestone in multi-modal generation. However, evaluating such tri-modal outputs remains an unsolved challenge. Human evaluation is reliable but costly and difficult to scale, while traditional automatic metrics, such as FVD, CLAP, and ViCLIP, focus on isolated modality pairs, struggle with complex prompts, and provide limited interpretability. Omni-modal large language models (omni-LLMs) present a promising alternative: they naturally process audio, video, and text, support rich reasoning, and offer interpretable chain-of-thought feedback. Driven by this, we introduce Omni-Judge, a study assessing whether omni-LLMs can serve as human-aligned judges for text-conditioned audio-video generation. Across nine perceptual and alignment metrics, Omni-Judge achieves correlation comparable to traditional metrics and excels on semantically demanding tasks such as audio-text alignment, video-text alignment, and audio-video-text coherence. It underperforms on high-FPS perceptual metrics, including video quality and audio-video synchronization, due to limited temporal resolution. Omni-Judge provides interpretable explanations that expose semantic or physical inconsistencies, enabling practical downstream uses such as feedback-based refinement. Our findings highlight both the potential and current limitations of omni-LLMs as unified evaluators for multi-modal generation.",
    "arxiv_url": "https://arxiv.org/abs/2602.01623v1",
    "pdf_url": "https://arxiv.org/pdf/2602.01623v1",
    "published_date": "2026-02-02",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "multi-modal",
      "dit",
      "physical",
      "text-to-video",
      "video generation",
      "evaluation"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2602.01623v1",
      "pdf": "https://arxiv.org/pdf/2602.01623v1"
    },
    "bibtex": ""
  },
  {
    "title": "Making Avatars Interact: Towards Text-Driven Human-Object Interaction for Controllable Talking Avatars",
    "authors": [
      "Youliang Zhang",
      "Zhengguang Zhou",
      "Zhentao Yu",
      "Ziyao Huang",
      "Teng Hu",
      "Sen Liang",
      "Guozhen Zhang",
      "Ziqiao Peng",
      "Shunkai Li",
      "Yi Chen",
      "Zixiang Zhou",
      "Yuan Zhou",
      "Qinglin Lu",
      "Xiu Li"
    ],
    "abstract": "Generating talking avatars is a fundamental task in video generation. Although existing methods can generate full-body talking avatars with simple human motion, extending this task to grounded human-object interaction (GHOI) remains an open challenge, requiring the avatar to perform text-aligned interactions with surrounding objects. This challenge stems from the need for environmental perception and the control-quality dilemma in GHOI generation. To address this, we propose a novel dual-stream framework, InteractAvatar, which decouples perception and planning from video synthesis for grounded human-object interaction. Leveraging detection to enhance environmental perception, we introduce a Perception and Interaction Module (PIM) to generate text-aligned interaction motions. Additionally, an Audio-Interaction Aware Generation Module (AIM) is proposed to synthesize vivid talking avatars performing object interactions. With a specially designed motion-to-video aligner, PIM and AIM share a similar network structure and enable parallel co-generation of motions and plausible videos, effectively mitigating the control-quality dilemma. Finally, we establish a benchmark, GroundedInter, for evaluating GHOI video generation. Extensive experiments and comparisons demonstrate the effectiveness of our method in generating grounded human-object interactions for talking avatars. Project page: https://interactavatar.github.io",
    "arxiv_url": "https://arxiv.org/abs/2602.01538v1",
    "pdf_url": "https://arxiv.org/pdf/2602.01538v1",
    "published_date": "2026-02-02",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "github_url": "",
    "keywords": [
      "controllable",
      "dit",
      "video generation",
      "video synthesis",
      "human motion",
      "benchmark",
      "avatar"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2602.01538v1",
      "pdf": "https://arxiv.org/pdf/2602.01538v1",
      "project": "https://interactavatar.github.io"
    },
    "bibtex": ""
  },
  {
    "title": "MTC-VAE: Multi-Level Temporal Compression with Content Awareness",
    "authors": [
      "Yubo Dong",
      "Linchao Zhu"
    ],
    "abstract": "Latent Video Diffusion Models (LVDMs) rely on Variational Autoencoders (VAEs) to compress videos into compact latent representations. For continuous Variational Autoencoders (VAEs), achieving higher compression rates is desirable; yet, the efficiency notably declines when extra sampling layers are added without expanding the dimensions of hidden channels. In this paper, we present a technique to convert fixed compression rate VAEs into models that support multi-level temporal compression, providing a straightforward and minimal fine-tuning approach to counteract performance decline at elevated compression rates.Moreover, we examine how varying compression levels impact model performance over video segments with diverse characteristics, offering empirical evidence on the effectiveness of our proposed approach. We also investigate the integration of our multi-level temporal compression VAE with diffusion-based generative models, DiT, highlighting successful concurrent training and compatibility within these frameworks. This investigation illustrates the potential uses of multi-level temporal compression.",
    "arxiv_url": "https://arxiv.org/abs/2602.01340v1",
    "pdf_url": "https://arxiv.org/pdf/2602.01340v1",
    "published_date": "2026-02-01",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "diffusion model",
      "latent video",
      "video diffusion",
      "dit"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2602.01340v1",
      "pdf": "https://arxiv.org/pdf/2602.01340v1"
    },
    "bibtex": ""
  },
  {
    "title": "FlowCast: Trajectory Forecasting for Scalable Zero-Cost Speculative Flow Matching",
    "authors": [
      "Divya Jyoti Bajpai",
      "Shubham Agarwal",
      "Apoorv Saxena",
      "Kuldeep Kulkarni",
      "Subrata Mitra",
      "Manjesh Kumar Hanawal"
    ],
    "abstract": "Flow Matching (FM) has recently emerged as a powerful approach for high-quality visual generation. However, their prohibitively slow inference due to a large number of denoising steps limits their potential use in real-time or interactive applications. Existing acceleration methods, like distillation, truncation, or consistency training, either degrade quality, incur costly retraining, or lack generalization. We propose FlowCast, a training-free speculative generation framework that accelerates inference by exploiting the fact that FM models are trained to preserve constant velocity. FlowCast speculates future velocity by extrapolating current velocity without incurring additional time cost, and accepts it if it is within a mean-squared error threshold. This constant-velocity forecasting allows redundant steps in stable regions to be aggressively skipped while retaining precision in complex ones. FlowCast is a plug-and-play framework that integrates seamlessly with any FM model and requires no auxiliary networks. We also present a theoretical analysis and bound the worst-case deviation between speculative and full FM trajectories. Empirical evaluations demonstrate that FlowCast achieves $>2.5\\times$ speedup in image generation, video generation, and editing tasks, outperforming existing baselines with no quality loss as compared to standard full generation.",
    "arxiv_url": "https://arxiv.org/abs/2602.01329v1",
    "pdf_url": "https://arxiv.org/pdf/2602.01329v1",
    "published_date": "2026-02-01",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "interactive",
      "acceleration",
      "distillation",
      "flow matching",
      "dit",
      "trajectory",
      "video generation",
      "evaluation",
      "denoising"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2602.01329v1",
      "pdf": "https://arxiv.org/pdf/2602.01329v1"
    },
    "bibtex": ""
  },
  {
    "title": "MTAVG-Bench: A Comprehensive Benchmark for Evaluating Multi-Talker Dialogue-Centric Audio-Video Generation",
    "authors": [
      "Yang-Hao Zhou",
      "Haitian Li",
      "Rexar Lin",
      "Heyan Huang",
      "Jinxing Zhou",
      "Changsen Yuan",
      "Tian Lan",
      "Ziqin Zhou",
      "Yudong Li",
      "Jiajun Xu",
      "Jingyun Liao",
      "Yi-Ming Cheng",
      "Xuefeng Chen",
      "Xian-Ling Mao",
      "Yousheng Feng"
    ],
    "abstract": "Recent advances in text-to-audio-video (T2AV) generation have enabled models to synthesize audio-visual videos with multi-participant dialogues. However, existing evaluation benchmarks remain largely designed for human-recorded videos or single-speaker settings. As a result, potential errors that occur in generated multi-talker dialogue videos, such as identity drift, unnatural turn transitions, and audio-visual misalignment, cannot be effectively captured and analyzed. To address this issue, we introduce MTAVG-Bench, a benchmark for evaluating audio-visual multi-speaker dialogue generation. MTAVG-Bench is built via a semi-automatic pipeline, where 1.8k videos are generated using multiple popular models with carefully designed prompts, yielding 2.4k manually annotated QA pairs. The benchmark evaluates multi-speaker dialogue generation at four levels: audio-visual signal fidelity, temporal attribute consistency, social interaction, and cinematic expression. We benchmark 12 proprietary and open-source omni-models on MTAVG-Bench, with Gemini 3 Pro achieving the strongest overall performance, while leading open-source models remain competitive in signal fidelity and consistency. Overall, MTAVG-Bench enables fine-grained failure analysis for rigorous model comparison and targeted video generation refinement.",
    "arxiv_url": "https://arxiv.org/abs/2602.00607v1",
    "pdf_url": "https://arxiv.org/pdf/2602.00607v1",
    "published_date": "2026-01-31",
    "categories": [
      "cs.MM",
      "cs.SD"
    ],
    "github_url": "",
    "keywords": [
      "benchmark",
      "evaluation",
      "video generation",
      "identity"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2602.00607v1",
      "pdf": "https://arxiv.org/pdf/2602.00607v1"
    },
    "bibtex": ""
  },
  {
    "title": "DuoGen: Towards General Purpose Interleaved Multimodal Generation",
    "authors": [
      "Min Shi",
      "Xiaohui Zeng",
      "Jiannan Huang",
      "Yin Cui",
      "Francesco Ferroni",
      "Jialuo Li",
      "Shubham Pachori",
      "Zhaoshuo Li",
      "Yogesh Balaji",
      "Haoxiang Wang",
      "Tsung-Yi Lin",
      "Xiao Fu",
      "Yue Zhao",
      "Chieh-Yun Chen",
      "Ming-Yu Liu",
      "Humphrey Shi"
    ],
    "abstract": "Interleaved multimodal generation enables capabilities beyond unimodal generation models, such as step-by-step instructional guides, visual planning, and generating visual drafts for reasoning. However, the quality of existing interleaved generation models under general instructions remains limited by insufficient training data and base model capacity. We present DuoGen, a general-purpose interleaved generation framework that systematically addresses data curation, architecture design, and evaluation. On the data side, we build a large-scale, high-quality instruction-tuning dataset by combining multimodal conversations rewritten from curated raw websites, and diverse synthetic examples covering everyday scenarios. Architecturally, DuoGen leverages the strong visual understanding of a pretrained multimodal LLM and the visual generation capabilities of a diffusion transformer (DiT) pretrained on video generation, avoiding costly unimodal pretraining and enabling flexible base model selection. A two-stage decoupled strategy first instruction-tunes the MLLM, then aligns DiT with it using curated interleaved image-text sequences. Across public and newly proposed benchmarks, DuoGen outperforms prior open-source models in text quality, image fidelity, and image-context alignment, and also achieves state-of-the-art performance on text-to-image and image editing among unified generation models. Data and code will be released at https://research.nvidia.com/labs/dir/duogen/.",
    "arxiv_url": "https://arxiv.org/abs/2602.00508v2",
    "pdf_url": "https://arxiv.org/pdf/2602.00508v2",
    "published_date": "2026-01-31",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "dit",
      "diffusion transformer",
      "video generation",
      "evaluation",
      "architecture",
      "benchmark"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2602.00508v2",
      "pdf": "https://arxiv.org/pdf/2602.00508v2",
      "project": "https://research.nvidia.com/labs/dir/duogen"
    },
    "bibtex": ""
  },
  {
    "title": "DISK: Dynamic Inference SKipping for World Models",
    "authors": [
      "Anugunj Naman",
      "Gaibo Zhang",
      "Ayushman Singh",
      "Yaguang Zhang"
    ],
    "abstract": "We present DISK, a training-free adaptive inference method for autoregressive world models. DISK coordinates two coupled diffusion transformers for video and ego-trajectory via dual-branch controllers with cross-modal skip decisions, preserving motion-appearance consistency without retraining. We extend higher-order latent-difference skip testing to the autoregressive chain-of-forward regime and propagate controller statistics through rollout loops for long-horizon stability. When integrated into closed-loop driving rollouts on 1500 NuPlan and NuScenes samples using an NVIDIA L40S GPU, DISK achieves 2x speedup on trajectory diffusion and 1.6x speedup on video diffusion while maintaining L2 planning error, visual quality (FID/FVD), and NAVSIM PDMS scores, demonstrating practical long-horizon video-and-trajectory prediction at substantially reduced cost.",
    "arxiv_url": "https://arxiv.org/abs/2602.00440v1",
    "pdf_url": "https://arxiv.org/pdf/2602.00440v1",
    "published_date": "2026-01-31",
    "categories": [
      "cs.CV",
      "cs.LG",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "video diffusion",
      "world model",
      "diffusion transformer",
      "trajectory",
      "autoregressive"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2602.00440v1",
      "pdf": "https://arxiv.org/pdf/2602.00440v1"
    },
    "bibtex": ""
  },
  {
    "title": "TokenTrim: Inference-Time Token Pruning for Autoregressive Long Video Generation",
    "authors": [
      "Ariel Shaulov",
      "Eitan Shaar",
      "Amit Edenzon",
      "Lior Wolf"
    ],
    "abstract": "Auto-regressive video generation enables long video synthesis by iteratively conditioning each new batch of frames on previously generated content. However, recent work has shown that such pipelines suffer from severe temporal drift, where errors accumulate and amplify over long horizons. We hypothesize that this drift does not primarily stem from insufficient model capacity, but rather from inference-time error propagation. Specifically, we contend that drift arises from the uncontrolled reuse of corrupted latent conditioning tokens during auto-regressive inference. To correct this accumulation of errors, we propose a simple, inference-time method that mitigates temporal drift by identifying and removing unstable latent tokens before they are reused for conditioning. For this purpose, we define unstable tokens as latent tokens whose representations deviate significantly from those of the previously generated batch, indicating potential corruption or semantic drift. By explicitly removing corrupted latent tokens from the auto-regressive context, rather than modifying entire spatial regions or model parameters, our method prevents unreliable latent information from influencing future generation steps. As a result, it significantly improves long-horizon temporal consistency without modifying the model architecture, training procedure, or leaving latent space.",
    "arxiv_url": "https://arxiv.org/abs/2602.00268v1",
    "pdf_url": "https://arxiv.org/pdf/2602.00268v1",
    "published_date": "2026-01-30",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "temporal consistency",
      "dit",
      "video generation",
      "video synthesis",
      "architecture",
      "long video",
      "autoregressive"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2602.00268v1",
      "pdf": "https://arxiv.org/pdf/2602.00268v1"
    },
    "bibtex": ""
  },
  {
    "title": "PLACID: Identity-Preserving Multi-Object Compositing via Video Diffusion with Synthetic Trajectories",
    "authors": [
      "Gemma Canet Tarrs",
      "Manel Baradad",
      "Francesc Moreno-Noguer",
      "Yumeng Li"
    ],
    "abstract": "Recent advances in generative AI have dramatically improved photorealistic image synthesis, yet they fall short for studio-level multi-object compositing. This task demands simultaneous (i) near-perfect preservation of each item's identity, (ii) precise background and color fidelity, (iii) layout and design elements control, and (iv) complete, appealing displays showcasing all objects. However, current state-of-the-art models often alter object details, omit or duplicate objects, and produce layouts with incorrect relative sizing or inconsistent item presentations. To bridge this gap, we introduce PLACID, a framework that transforms a collection of object images into an appealing multi-object composite. Our approach makes two main contributions. First, we leverage a pretrained image-to-video (I2V) diffusion model with text control to preserve objects consistency, identities, and background details by exploiting temporal priors from videos. Second, we propose a novel data curation strategy that generates synthetic sequences where randomly placed objects smoothly move to their target positions. This synthetic data aligns with the video model's temporal priors during training. At inference, objects initialized at random positions consistently converge into coherent layouts guided by text, with the final frame serving as the composite image. Extensive quantitative evaluations and user studies demonstrate that PLACID surpasses state-of-the-art methods in multi-object compositing, achieving superior identity, background, and color preservation, with less omitted objects and visually appealing results.",
    "arxiv_url": "https://arxiv.org/abs/2602.00267v1",
    "pdf_url": "https://arxiv.org/pdf/2602.00267v1",
    "published_date": "2026-01-30",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "layout",
      "video diffusion",
      "identity",
      "image-to-video",
      "i2v",
      "evaluation",
      "diffusion model"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2602.00267v1",
      "pdf": "https://arxiv.org/pdf/2602.00267v1"
    },
    "bibtex": ""
  },
  {
    "title": "VideoGPA: Distilling Geometry Priors for 3D-Consistent Video Generation",
    "authors": [
      "Hongyang Du",
      "Junjie Ye",
      "Xiaoyan Cong",
      "Runhao Li",
      "Jingcheng Ni",
      "Aman Agarwal",
      "Zeqi Zhou",
      "Zekun Li",
      "Randall Balestriero",
      "Yue Wang"
    ],
    "abstract": "While recent video diffusion models (VDMs) produce visually impressive results, they fundamentally struggle to maintain 3D structural consistency, often resulting in object deformation or spatial drift. We hypothesize that these failures arise because standard denoising objectives lack explicit incentives for geometric coherence. To address this, we introduce VideoGPA (Video Geometric Preference Alignment), a data-efficient self-supervised framework that leverages a geometry foundation model to automatically derive dense preference signals that guide VDMs via Direct Preference Optimization (DPO). This approach effectively steers the generative distribution toward inherent 3D consistency without requiring human annotations. VideoGPA significantly enhances temporal stability, physical plausibility, and motion coherence using minimal preference pairs, consistently outperforming state-of-the-art baselines in extensive experiments.",
    "arxiv_url": "https://arxiv.org/abs/2601.23286v1",
    "pdf_url": "https://arxiv.org/pdf/2601.23286v1",
    "published_date": "2026-01-30",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "video diffusion",
      "physical",
      "video generation",
      "diffusion model",
      "denoising",
      "efficient"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.23286v1",
      "pdf": "https://arxiv.org/pdf/2601.23286v1"
    },
    "bibtex": ""
  },
  {
    "title": "VisionTrim: Unified Vision Token Compression for Training-Free MLLM Acceleration",
    "authors": [
      "Hanxun Yu",
      "Wentong Li",
      "Xuan Qu",
      "Song Wang",
      "Junbo Chen",
      "Jianke Zhu"
    ],
    "abstract": "Multimodal large language models (MLLMs) suffer from high computational costs due to excessive visual tokens, particularly in high-resolution and video-based scenarios. Existing token reduction methods typically focus on isolated pipeline components and often neglect textual alignment, leading to performance degradation. In this paper, we propose VisionTrim, a unified framework for training-free MLLM acceleration, integrating two effective plug-and-play modules: 1) the Dominant Vision Token Selection (DVTS) module, which preserves essential visual tokens via a global-local view, and 2) the Text-Guided Vision Complement (TGVC) module, which facilitates context-aware token merging guided by textual cues. Extensive experiments across diverse image and video multimodal benchmarks demonstrate the performance superiority of our VisionTrim, advancing practical MLLM deployment in real-world applications. The code is available at: https://github.com/hanxunyu/VisionTrim.",
    "arxiv_url": "https://arxiv.org/abs/2601.22674v2",
    "pdf_url": "https://arxiv.org/pdf/2601.22674v2",
    "published_date": "2026-01-30",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/hanxunyu/VisionTrim",
    "keywords": [
      "benchmark",
      "acceleration"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.22674v2",
      "pdf": "https://arxiv.org/pdf/2601.22674v2",
      "github": "https://github.com/hanxunyu/VisionTrim"
    },
    "bibtex": ""
  },
  {
    "title": "VMonarch: Efficient Video Diffusion Transformers with Structured Attention",
    "authors": [
      "Cheng Liang",
      "Haoxian Chen",
      "Liang Hou",
      "Qi Fan",
      "Gangshan Wu",
      "Xin Tao",
      "Limin Wang"
    ],
    "abstract": "The quadratic complexity of the attention mechanism severely limits the context scalability of Video Diffusion Transformers (DiTs). We find that the highly sparse spatio-temporal attention patterns exhibited in Video DiTs can be naturally represented by the Monarch matrix. It is a class of structured matrices with flexible sparsity, enabling sub-quadratic attention via an alternating minimization algorithm. Accordingly, we propose VMonarch, a novel attention mechanism for Video DiTs that enables efficient computation over the dynamic sparse patterns with structured Monarch matrices. First, we adapt spatio-temporal Monarch factorization to explicitly capture the intra-frame and inter-frame correlations of the video data. Second, we introduce a recomputation strategy to mitigate artifacts arising from instabilities during alternating minimization of Monarch matrices. Third, we propose a novel online entropy algorithm fused into FlashAttention, enabling fast Monarch matrix updates for long sequences. Extensive experiments demonstrate that VMonarch achieves comparable or superior generation quality to full attention on VBench after minimal tuning. It overcomes the attention bottleneck in Video DiTs, reduces attention FLOPs by a factor of 17.5, and achieves a speedup of over 5x in attention computation for long videos, surpassing state-of-the-art sparse attention methods at 90% sparsity.",
    "arxiv_url": "https://arxiv.org/abs/2601.22275v1",
    "pdf_url": "https://arxiv.org/pdf/2601.22275v1",
    "published_date": "2026-01-29",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "video diffusion",
      "dit",
      "diffusion transformer",
      "long video",
      "efficient"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.22275v1",
      "pdf": "https://arxiv.org/pdf/2601.22275v1"
    },
    "bibtex": ""
  },
  {
    "title": "JUST-DUB-IT: Video Dubbing via Joint Audio-Visual Diffusion",
    "authors": [
      "Anthony Chen",
      "Naomi Ken Korem",
      "Tavi Halperin",
      "Matan Ben Yosef",
      "Urska Jelercic",
      "Ofir Bibi",
      "Or Patashnik",
      "Daniel Cohen-Or"
    ],
    "abstract": "Audio-Visual Foundation Models, which are pretrained to jointly generate sound and visual content, have recently shown an unprecedented ability to model multi-modal generation and editing, opening new opportunities for downstream tasks. Among these tasks, video dubbing could greatly benefit from such priors, yet most existing solutions still rely on complex, task-specific pipelines that struggle in real-world settings. In this work, we introduce a single-model approach that adapts a foundational audio-video diffusion model for video-to-video dubbing via a lightweight LoRA. The LoRA enables the model to condition on an input audio-video while jointly generating translated audio and synchronized facial motion. To train this LoRA, we leverage the generative model itself to synthesize paired multilingual videos of the same speaker. Specifically, we generate multilingual videos with language switches within a single clip, and then inpaint the face and audio in each half to match the language of the other half. By leveraging the rich generative prior of the audio-visual model, our approach preserves speaker identity and lip synchronization while remaining robust to complex motion and real-world dynamics. We demonstrate that our approach produces high-quality dubbed videos with improved visual fidelity, lip synchronization, and robustness compared to existing dubbing pipelines.",
    "arxiv_url": "https://arxiv.org/abs/2601.22143v1",
    "pdf_url": "https://arxiv.org/pdf/2601.22143v1",
    "published_date": "2026-01-29",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "multi-modal",
      "video diffusion",
      "dit",
      "identity",
      "video-to-video",
      "diffusion model",
      "dynamics",
      "sound"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.22143v1",
      "pdf": "https://arxiv.org/pdf/2601.22143v1"
    },
    "bibtex": ""
  },
  {
    "title": "EditYourself: Audio-Driven Generation and Manipulation of Talking Head Videos with Diffusion Transformers",
    "authors": [
      "John Flynn",
      "Wolfgang Paier",
      "Dimitar Dinev",
      "Sam Nhut Nguyen",
      "Hayk Poghosyan",
      "Manuel Toribio",
      "Sandipan Banerjee",
      "Guy Gafni"
    ],
    "abstract": "Current generative video models excel at producing novel content from text and image prompts, but leave a critical gap in editing existing pre-recorded videos, where minor alterations to the spoken script require preserving motion, temporal coherence, speaker identity, and accurate lip synchronization. We introduce EditYourself, a DiT-based framework for audio-driven video-to-video (V2V) editing that enables transcript-based modification of talking head videos, including the seamless addition, removal, and retiming of visually spoken content. Building on a general-purpose video diffusion model, EditYourself augments its V2V capabilities with audio conditioning and region-aware, edit-focused training extensions. This enables precise lip synchronization and temporally coherent restructuring of existing performances via spatiotemporal inpainting, including the synthesis of realistic human motion in newly added segments, while maintaining visual fidelity and identity consistency over long durations. This work represents a foundational step toward generative video models as practical tools for professional video post-production.",
    "arxiv_url": "https://arxiv.org/abs/2601.22127v1",
    "pdf_url": "https://arxiv.org/pdf/2601.22127v1",
    "published_date": "2026-01-29",
    "categories": [
      "cs.CV",
      "cs.GR",
      "cs.LG",
      "cs.MM"
    ],
    "github_url": "",
    "keywords": [
      "talking head",
      "video diffusion",
      "dit",
      "identity",
      "diffusion transformer",
      "video-to-video",
      "diffusion model",
      "human motion",
      "audio-driven"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.22127v1",
      "pdf": "https://arxiv.org/pdf/2601.22127v1"
    },
    "bibtex": ""
  },
  {
    "title": "Learning Transient Convective Heat Transfer with Geometry Aware World Models",
    "authors": [
      "Onur T. Doganay",
      "Alexander Klawonn",
      "Martin Eigel",
      "Hanno Gottschalk"
    ],
    "abstract": "Partial differential equation (PDE) simulations are fundamental to engineering and physics but are often computationally prohibitive for real-time applications. While generative AI offers a promising avenue for surrogate modeling, standard video generation architectures lack the specific control and data compatibility required for physical simulations. This paper introduces a geometry aware world model architecture, derived from a video generation architecture (LongVideoGAN), designed to learn transient physics. We introduce two key architecture elements: (1) a twofold conditioning mechanism incorporating global physical parameters and local geometric masks, and (2) an architectural adaptation to support arbitrary channel dimensions, moving beyond standard RGB constraints. We evaluate this approach on a 2D transient computational fluid dynamics (CFD) problem involving convective heat transfer from buoyancy-driven flow coupled to a heat flow in a solid structure. We demonstrate that the conditioned model successfully reproduces complex temporal dynamics and spatial correlations of the training data. Furthermore, we assess the model's generalization capabilities on unseen geometric configurations, highlighting both its potential for controlled simulation synthesis and current limitations in spatial precision for out-of-distribution samples.",
    "arxiv_url": "https://arxiv.org/abs/2601.22086v1",
    "pdf_url": "https://arxiv.org/pdf/2601.22086v1",
    "published_date": "2026-01-29",
    "categories": [
      "physics.flu-dyn",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "physical simulation",
      "dit",
      "world model",
      "physical",
      "video generation",
      "architecture",
      "simulation",
      "dynamics",
      "physics"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.22086v1",
      "pdf": "https://arxiv.org/pdf/2601.22086v1"
    },
    "bibtex": ""
  },
  {
    "title": "Where Do the Joules Go? Diagnosing Inference Energy Consumption",
    "authors": [
      "Jae-Won Chung",
      "Ruofan Wu",
      "Jeff J. Ma",
      "Mosharaf Chowdhury"
    ],
    "abstract": "Energy is now a critical ML computing resource. While measuring energy consumption and observing trends is a valuable first step, accurately understanding and diagnosing why those differences occur is crucial for optimization. To that end, we begin by presenting a large-scale measurement study of inference time and energy across the generative AI landscape with 46 models, 7 tasks, and 1,858 different configurations on NVIDIA H100 and B200 GPUs. Our empirical findings span order-of-magnitude variations: LLM task type can lead to 25$\\times$ energy differences, video generation sometimes consumes more than 100$\\times$ the energy of images, and GPU utilization differences can result in 3--5$\\times$ energy differences. Based on our observations, we present a framework for reasoning about the underlying mechanisms that govern time and energy consumption. The essence is that time and energy are determined by latent metrics like memory and utilization, which are in turn affected by various factors across the algorithm, software, and hardware layers. Our framework also extends directly to throughput per watt, a critical metric for power-constrained datacenters.",
    "arxiv_url": "https://arxiv.org/abs/2601.22076v2",
    "pdf_url": "https://arxiv.org/pdf/2601.22076v2",
    "published_date": "2026-01-29",
    "categories": [
      "cs.LG",
      "cs.DC"
    ],
    "github_url": "",
    "keywords": [
      "video generation"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.22076v2",
      "pdf": "https://arxiv.org/pdf/2601.22076v2"
    },
    "bibtex": ""
  },
  {
    "title": "Zero-Shot Video Restoration and Enhancement with Assistance of Video Diffusion Models",
    "authors": [
      "Cong Cao",
      "Huanjing Yue",
      "Shangbin Xie",
      "Xin Liu",
      "Jingyu Yang"
    ],
    "abstract": "Although diffusion-based zero-shot image restoration and enhancement methods have achieved great success, applying them to video restoration or enhancement will lead to severe temporal flickering. In this paper, we propose the first framework that utilizes the rapidly-developed video diffusion model to assist the image-based method in maintaining more temporal consistency for zero-shot video restoration and enhancement. We propose homologous latents fusion, heterogenous latents fusion, and a COT-based fusion ratio strategy to utilize both homologous and heterogenous text-to-video diffusion models to complement the image method. Moreover, we propose temporal-strengthening post-processing to utilize the image-to-video diffusion model to further improve temporal consistency. Our method is training-free and can be applied to any diffusion-based image restoration and enhancement methods. Experimental results demonstrate the superiority of the proposed method.",
    "arxiv_url": "https://arxiv.org/abs/2601.21922v1",
    "pdf_url": "https://arxiv.org/pdf/2601.21922v1",
    "published_date": "2026-01-29",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "video restoration",
      "temporal consistency",
      "video diffusion",
      "text-to-video",
      "image-to-video",
      "diffusion model"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.21922v1",
      "pdf": "https://arxiv.org/pdf/2601.21922v1"
    },
    "bibtex": ""
  },
  {
    "title": "Past- and Future-Informed KV Cache Policy with Salience Estimation in Autoregressive Video Diffusion",
    "authors": [
      "Hanmo Chen",
      "Chenghao Xu",
      "Xu Yang",
      "Xuan Chen",
      "Cheng Deng"
    ],
    "abstract": "Video generation is pivotal to digital media creation, and recent advances in autoregressive video generation have markedly enhanced the efficiency of real-time video synthesis. However, existing approaches generally rely on heuristic KV Cache policies, which ignore differences in token importance in long-term video generation. This leads to the loss of critical spatiotemporal information and the accumulation of redundant, invalid cache, thereby degrading video generation quality and efficiency. To address this limitation, we first observe that token contributions to video generation are highly time-heterogeneous and accordingly propose a novel Past- and Future-Informed KV Cache Policy (PaFu-KV). Specifically, PaFu-KV introduces a lightweight Salience Estimation Head distilled from a bidirectional teacher to estimate salience scores, allowing the KV cache to retain informative tokens while discarding less relevant ones. This policy yields a better quality-efficiency trade-off by shrinking KV cache capacity and reducing memory footprint at inference time. Extensive experiments on benchmarks demonstrate that our method preserves high-fidelity video generation quality while enables accelerated inference, thereby enabling more efficient long-horizon video generation. Our code will be released upon paper acceptance.",
    "arxiv_url": "https://arxiv.org/abs/2601.21896v3",
    "pdf_url": "https://arxiv.org/pdf/2601.21896v3",
    "published_date": "2026-01-29",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "video diffusion",
      "video generation",
      "efficient",
      "video synthesis",
      "autoregressive",
      "benchmark"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.21896v3",
      "pdf": "https://arxiv.org/pdf/2601.21896v3"
    },
    "bibtex": ""
  },
  {
    "title": "Moral Outrage Shapes Commitments Beyond Attention: Multimodal Moral Emotions on YouTube in Korea and the US",
    "authors": [
      "Seongchan Park",
      "Jaehong Kim",
      "Hyeonseung Kim",
      "Heejin Bin",
      "Sue Moon",
      "Wonjae Lee"
    ],
    "abstract": "Understanding how media rhetoric shapes audience engagement is crucial in the attention economy. This study examines how moral emotional framing by mainstream news channels on YouTube influences user behavior across Korea and the United States. To capture the platform's multimodal nature, combining thumbnail images and video titles, we develop a multimodal moral emotion classifier by fine tuning a vision language model. The model is trained on human annotated multimodal datasets in both languages and applied to approximately 400,000 videos from major news outlets. We analyze engagement levels including views, likes, and comments, representing increasing degrees of commitment. The results show that other condemning rhetoric expressions of moral outrage that criticize others morally consistently increase all forms of engagement across cultures, with effects ranging from passive viewing to active commenting. These findings suggest that moral outrage is a particularly effective emotional strategy, attracting not only attention but also active participation. We discuss concerns about the potential misuse of other condemning rhetoric, as such practices may deepen polarization by reinforcing in group and out group divisions. To facilitate future research and ensure reproducibility, we publicly release our Korean and English multimodal moral emotion classifiers.",
    "arxiv_url": "https://arxiv.org/abs/2601.21815v2",
    "pdf_url": "https://arxiv.org/pdf/2601.21815v2",
    "published_date": "2026-01-29",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL",
      "cs.SI"
    ],
    "github_url": "",
    "keywords": [],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.21815v2",
      "pdf": "https://arxiv.org/pdf/2601.21815v2"
    },
    "bibtex": ""
  },
  {
    "title": "Learning Physics-Grounded 4D Dynamics with Neural Gaussian Force Fields",
    "authors": [
      "Shiqian Li",
      "Ruihong Shen",
      "Junfeng Ni",
      "Chang Pan",
      "Chi Zhang",
      "Yixin Zhu"
    ],
    "abstract": "Predicting physical dynamics from raw visual data remains a major challenge in AI. While recent video generation models have achieved impressive visual quality, they still cannot consistently generate physically plausible videos due to a lack of modeling of physical laws. Recent approaches combining 3D Gaussian splatting and physics engines can produce physically plausible videos, but are hindered by high computational costs in both reconstruction and simulation, and often lack robustness in complex real-world scenarios. To address these issues, we introduce Neural Gaussian Force Field (NGFF), an end-to-end neural framework that integrates 3D Gaussian perception with physics-based dynamic modeling to generate interactive, physically realistic 4D videos from multi-view RGB inputs, achieving two orders of magnitude faster than prior Gaussian simulators. To support training, we also present GSCollision, a 4D Gaussian dataset featuring diverse materials, multi-object interactions, and complex scenes, totaling over 640k rendered physical videos (~4 TB). Evaluations on synthetic and real 3D scenarios show NGFF's strong generalization and robustness in physical reasoning, advancing video prediction towards physics-grounded world models.",
    "arxiv_url": "https://arxiv.org/abs/2602.00148v1",
    "pdf_url": "https://arxiv.org/pdf/2602.00148v1",
    "published_date": "2026-01-29",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "interactive",
      "video prediction",
      "world model",
      "physical",
      "video generation",
      "evaluation",
      "dynamics",
      "simulation",
      "physics"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2602.00148v1",
      "pdf": "https://arxiv.org/pdf/2602.00148v1"
    },
    "bibtex": ""
  },
  {
    "title": "MPF-Net: Exposing High-Fidelity AI-Generated Video Forgeries via Hierarchical Manifold Deviation and Micro-Temporal Fluctuations",
    "authors": [
      "Xinan He",
      "Kaiqing Lin",
      "Yue Zhou",
      "Jiaming Zhong",
      "Wei Ye",
      "Wenhui Yi",
      "Bing Fan",
      "Feng Ding",
      "Haodong Li",
      "Bo Cao",
      "Bin Li"
    ],
    "abstract": "With the rapid advancement of video generation models such as Veo and Wan, the visual quality of synthetic content has reached a level where macro-level semantic errors and temporal inconsistencies are no longer prominent. However, this does not imply that the distinction between real and cutting-edge high-fidelity fake is untraceable. We argue that AI-generated videos are essentially products of a manifold-fitting process rather than a physical recording. Consequently, the pixel composition logic of consecutive adjacent frames residual in AI videos exhibits a structured and homogenous characteristic. We term this phenomenon `Manifold Projection Fluctuations' (MPF). Driven by this insight, we propose a hierarchical dual-path framework that operates as a sequential filtering process. The first, the Static Manifold Deviation Branch, leverages the refined perceptual boundaries of Large-Scale Vision Foundation Models (VFMs) to capture residual spatial anomalies or physical violations that deviate from the natural real-world manifold (off-manifold). For the remaining high-fidelity videos that successfully reside on-manifold and evade spatial detection, we introduce the Micro-Temporal Fluctuation Branch as a secondary, fine-grained filter. By analyzing the structured MPF that persists even in visually perfect sequences, our framework ensures that forgeries are exposed regardless of whether they manifest as global real-world manifold deviations or subtle computational fingerprints.",
    "arxiv_url": "https://arxiv.org/abs/2601.21408v2",
    "pdf_url": "https://arxiv.org/pdf/2601.21408v2",
    "published_date": "2026-01-29",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "physical",
      "video generation"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.21408v2",
      "pdf": "https://arxiv.org/pdf/2601.21408v2"
    },
    "bibtex": ""
  },
  {
    "title": "Ostrakon-VL: Towards Domain-Expert MLLM for Food-Service and Retail Stores",
    "authors": [
      "Zhiyong Shen",
      "Gongpeng Zhao",
      "Jun Zhou",
      "Li Yu",
      "Guandong Kou",
      "Jichen Li",
      "Chuanlei Dong",
      "Zuncheng Li",
      "Kaimao Li",
      "Bingkun Wei",
      "Shicheng Hu",
      "Wei Xia",
      "Wenguo Duan"
    ],
    "abstract": "Multimodal Large Language Models (MLLMs) have recently achieved substantial progress in general-purpose perception and reasoning. Nevertheless, their deployment in Food-Service and Retail Stores (FSRS) scenarios encounters two major obstacles: (i) real-world FSRS data, collected from heterogeneous acquisition devices, are highly noisy and lack auditable, closed-loop data curation, which impedes the construction of high-quality, controllable, and reproducible training corpora; and (ii) existing evaluation protocols do not offer a unified, fine-grained and standardized benchmark spanning single-image, multi-image, and video inputs, making it challenging to objectively gauge model robustness. To address these challenges, we first develop Ostrakon-VL, an FSRS-oriented MLLM based on Qwen3-VL-8B. Second, we introduce ShopBench, the first public benchmark for FSRS. Third, we propose QUAD (Quality-aware Unbiased Automated Data-curation), a multi-stage multimodal instruction data curation pipeline. Leveraging a multi-stage training strategy, Ostrakon-VL achieves an average score of 60.1 on ShopBench, establishing a new state of the art among open-source MLLMs with comparable parameter scales and diverse architectures. Notably, it surpasses the substantially larger Qwen3-VL-235B-A22B (59.4) by +0.7, and exceeds the same-scale Qwen3-VL-8B (55.3) by +4.8, demonstrating significantly improved parameter efficiency. These results indicate that Ostrakon-VL delivers more robust and reliable FSRS-centric perception and decision-making capabilities. To facilitate reproducible research, we will publicly release Ostrakon-VL and the ShopBench benchmark.",
    "arxiv_url": "https://arxiv.org/abs/2601.21342v1",
    "pdf_url": "https://arxiv.org/pdf/2601.21342v1",
    "published_date": "2026-01-29",
    "categories": [
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "controllable",
      "dit",
      "evaluation",
      "architecture",
      "benchmark"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.21342v1",
      "pdf": "https://arxiv.org/pdf/2601.21342v1"
    },
    "bibtex": ""
  },
  {
    "title": "WorldBench: Disambiguating Physics for Diagnostic Evaluation of World Models",
    "authors": [
      "Rishi Upadhyay",
      "Howard Zhang",
      "Jim Solomon",
      "Ayush Agrawal",
      "Pranay Boreddy",
      "Shruti Satya Narayana",
      "Yunhao Ba",
      "Alex Wong",
      "Celso M de Melo",
      "Achuta Kadambi"
    ],
    "abstract": "Recent advances in generative foundational models, often termed \"world models,\" have propelled interest in applying them to critical tasks like robotic planning and autonomous system training. For reliable deployment, these models must exhibit high physical fidelity, accurately simulating real-world dynamics. Existing physics-based video benchmarks, however, suffer from entanglement, where a single test simultaneously evaluates multiple physical laws and concepts, fundamentally limiting their diagnostic capability. We introduce WorldBench, a novel video-based benchmark specifically designed for concept-specific, disentangled evaluation, allowing us to rigorously isolate and assess understanding of a single physical concept or law at a time. To make WorldBench comprehensive, we design benchmarks at two different levels: 1) an evaluation of intuitive physical understanding with concepts such as object permanence or scale/perspective, and 2) an evaluation of low-level physical constants and material properties such as friction coefficients or fluid viscosity. When SOTA video-based world models are evaluated on WorldBench, we find specific patterns of failure in particular physics concepts, with all tested models lacking the physical consistency required to generate reliable real-world interactions. Through its concept-specific evaluation, WorldBench offers a more nuanced and scalable framework for rigorously evaluating the physical reasoning capabilities of video generation and world models, paving the way for more robust and generalizable world-model-driven learning.",
    "arxiv_url": "https://arxiv.org/abs/2601.21282v1",
    "pdf_url": "https://arxiv.org/pdf/2601.21282v1",
    "published_date": "2026-01-29",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "world model",
      "physical",
      "video generation",
      "efficient",
      "evaluation",
      "concept",
      "dynamics",
      "benchmark",
      "physics"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.21282v1",
      "pdf": "https://arxiv.org/pdf/2601.21282v1"
    },
    "bibtex": ""
  },
  {
    "title": "Generative Recall, Dense Reranking: Learning Multi-View Semantic IDs for Efficient Text-to-Video Retrieval",
    "authors": [
      "Zecheng Zhao",
      "Zhi Chen",
      "Zi Huang",
      "Shazia Sadiq",
      "Tong Chen"
    ],
    "abstract": "Text-to-Video Retrieval (TVR) is essential in video platforms. Dense retrieval with dual-modality encoders leads in accuracy, but its computation and storage scale poorly with corpus size. Thus, real-time large-scale applications adopt two-stage retrieval, where a fast recall model gathers a small candidate pool, which is reranked by an advanced dense retriever. Due to hugely reduced candidates, the reranking model can use any off-the-shelf dense retriever without hurting efficiency, meaning the recall model bounds two-stage TVR performance. Recently, generative retrieval (GR) replaces dense video embeddings with discrete semantic IDs and retrieves by decoding text queries into ID tokens. GR offers near-constant inference and storage complexity, and its semantic IDs capture high-level video features via quantization, making it ideal for quickly eliminating irrelevant candidates during recall. However, as a recall model in two-stage TVR, GR suffers from (i) semantic ambiguity, where each video satisfies diverse queries but is forced into one semantic ID; and (ii) cross-modal misalignment, as semantic IDs are solely derived from visual features without text supervision. We propose Generative Recall and Dense Reranking (GRDR), designing a novel GR method to uplift recalled candidate quality. GRDR assigns multiple semantic IDs to each video using a query-guided multi-view tokenizer exposing diverse semantic access paths, and jointly trains the tokenizer and generative retriever via a shared codebook to cast semantic IDs as the semantic bridge between texts and videos. At inference, trie-constrained decoding generates a compact candidate set reranked by a dense model for fine-grained matching. Experiments on TVR benchmarks show GRDR matches strong dense retrievers in accuracy while reducing index storage by an order of magnitude and accelerating up to 300$\\times$ in full-corpus retrieval.",
    "arxiv_url": "https://arxiv.org/abs/2601.21193v1",
    "pdf_url": "https://arxiv.org/pdf/2601.21193v1",
    "published_date": "2026-01-29",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "benchmark",
      "text-to-video",
      "efficient"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.21193v1",
      "pdf": "https://arxiv.org/pdf/2601.21193v1"
    },
    "bibtex": ""
  },
  {
    "title": "Thinking in Frames: How Visual Context and Test-Time Scaling Empower Video Reasoning",
    "authors": [
      "Chengzu Li",
      "Zanyi Wang",
      "Jiaang Li",
      "Yi Xu",
      "Han Zhou",
      "Huanyu Zhang",
      "Ruichuan An",
      "Dengyang Jiang",
      "Zhaochong An",
      "Ivan Vuli",
      "Serge Belongie",
      "Anna Korhonen"
    ],
    "abstract": "Vision-Language Models have excelled at textual reasoning, but they often struggle with fine-grained spatial understanding and continuous action planning, failing to simulate the dynamics required for complex visual reasoning. In this work, we formulate visual reasoning by means of video generation models, positing that generated frames can act as intermediate reasoning steps between initial states and solutions. We evaluate their capacity in two distinct regimes: Maze Navigation for sequential discrete planning with low visual change and Tangram Puzzle for continuous manipulation with high visual change. Our experiments reveal three critical insights: (1) Robust Zero-Shot Generalization: In both tasks, the model demonstrates strong performance on unseen data distributions without specific finetuning. (2) Visual Context: The model effectively uses visual context as explicit control, such as agent icons and tangram shapes, enabling it to maintain high visual consistency and adapt its planning capability robustly to unseen patterns. (3) Visual Test-Time Scaling: We observe a test-time scaling law in sequential planning; increasing the generated video length (visual inference budget) empowers better zero-shot generalization to spatially and temporally complex paths. These findings suggest that video generation is not merely a media tool, but a scalable, generalizable paradigm for visual reasoning.",
    "arxiv_url": "https://arxiv.org/abs/2601.21037v1",
    "pdf_url": "https://arxiv.org/pdf/2601.21037v1",
    "published_date": "2026-01-28",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "dynamics",
      "video generation"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.21037v1",
      "pdf": "https://arxiv.org/pdf/2601.21037v1"
    },
    "bibtex": ""
  },
  {
    "title": "FreeFix: Boosting 3D Gaussian Splatting via Fine-Tuning-Free Diffusion Models",
    "authors": [
      "Hongyu Zhou",
      "Zisen Shao",
      "Sheng Miao",
      "Pan Wang",
      "Dongfeng Bai",
      "Bingbing Liu",
      "Yiyi Liao"
    ],
    "abstract": "Neural Radiance Fields and 3D Gaussian Splatting have advanced novel view synthesis, yet still rely on dense inputs and often degrade at extrapolated views. Recent approaches leverage generative models, such as diffusion models, to provide additional supervision, but face a trade-off between generalization and fidelity: fine-tuning diffusion models for artifact removal improves fidelity but risks overfitting, while fine-tuning-free methods preserve generalization but often yield lower fidelity. We introduce FreeFix, a fine-tuning-free approach that pushes the boundary of this trade-off by enhancing extrapolated rendering with pretrained image diffusion models. We present an interleaved 2D-3D refinement strategy, showing that image diffusion models can be leveraged for consistent refinement without relying on costly video diffusion models. Furthermore, we take a closer look at the guidance signal for 2D refinement and propose a per-pixel confidence mask to identify uncertain regions for targeted improvement. Experiments across multiple datasets show that FreeFix improves multi-frame consistency and achieves performance comparable to or surpassing fine-tuning-based methods, while retaining strong generalization ability.",
    "arxiv_url": "https://arxiv.org/abs/2601.20857v1",
    "pdf_url": "https://arxiv.org/pdf/2601.20857v1",
    "published_date": "2026-01-28",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "diffusion model",
      "novel view",
      "video diffusion",
      "dit"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.20857v1",
      "pdf": "https://arxiv.org/pdf/2601.20857v1"
    },
    "bibtex": ""
  },
  {
    "title": "FAIRT2V: Training-Free Debiasing for Text-to-Video Diffusion Models",
    "authors": [
      "Haonan Zhong",
      "Wei Song",
      "Tingxu Han",
      "Maurice Pagnucco",
      "Jingling Xue",
      "Yang Song"
    ],
    "abstract": "Text-to-video (T2V) diffusion models have achieved rapid progress, yet their demographic biases, particularly gender bias, remain largely unexplored. We present FairT2V, a training-free debiasing framework for text-to-video generation that mitigates encoder-induced bias without finetuning. We first analyze demographic bias in T2V models and show that it primarily originates from pretrained text encoders, which encode implicit gender associations even for neutral prompts. We quantify this effect with a gender-leaning score that correlates with bias in generated videos.   Based on this insight, FairT2V mitigates demographic bias by neutralizing prompt embeddings via anchor-based spherical geodesic transformations while preserving semantics. To maintain temporal coherence, we apply debiasing only during early identity-forming steps through a dynamic denoising schedule. We further propose a video-level fairness evaluation protocol combining VideoLLM-based reasoning with human verification. Experiments on the modern T2V model Open-Sora show that FairT2V substantially reduces demographic bias across occupations with minimal impact on video quality.",
    "arxiv_url": "https://arxiv.org/abs/2601.20791v1",
    "pdf_url": "https://arxiv.org/pdf/2601.20791v1",
    "published_date": "2026-01-28",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "video diffusion",
      "identity",
      "text-to-video",
      "video generation",
      "evaluation",
      "diffusion model",
      "denoising",
      "t2v"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.20791v1",
      "pdf": "https://arxiv.org/pdf/2601.20791v1"
    },
    "bibtex": ""
  },
  {
    "title": "Block Erasure-Aware Semantic Multimedia Compression via JSCC Autoencoder",
    "authors": [
      "Homa Esfahanizadeh",
      "Nargis Fayaz",
      "Jinfeng Du",
      "Harish Viswanathan"
    ],
    "abstract": "We present an AI-based framework for semantic transmission of multimedia data over band-limited, time-varying channels. The method targets scenarios where large content is split into multiple packets, with an unknown number potentially dropped due to channel impairments. Using joint source-channel coding (JSCC), our approach achieves reliable semantic reconstruction with graceful quality degradation as channel conditions worsen, eliminating the need for retransmissions that cause unacceptable delays in latency-sensitive applications such as video conferencing and robotic control. The framework is compatible with existing network protocols and further enables intelligent congestion control and unequal error protection. A tunable design parameter allows balancing robustness at low channel quality against fidelity at high channel quality. Experiments demonstrate significant robustness improvement over state-of-the-art baselines in both image and video domains.",
    "arxiv_url": "https://arxiv.org/abs/2601.20707v1",
    "pdf_url": "https://arxiv.org/pdf/2601.20707v1",
    "published_date": "2026-01-28",
    "categories": [
      "cs.MM"
    ],
    "github_url": "",
    "keywords": [
      "dit"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.20707v1",
      "pdf": "https://arxiv.org/pdf/2601.20707v1"
    },
    "bibtex": ""
  },
  {
    "title": "StructAlign: Structured Cross-Modal Alignment for Continual Text-to-Video Retrieval",
    "authors": [
      "Shaokun Wang",
      "Weili Guan",
      "Jizhou Han",
      "Jianlong Wu",
      "Yupeng Hu",
      "Liqiang Nie"
    ],
    "abstract": "Continual Text-to-Video Retrieval (CTVR) is a challenging multimodal continual learning setting, where models must incrementally learn new semantic categories while maintaining accurate text-video alignment for previously learned ones, thus making it particularly prone to catastrophic forgetting. A key challenge in CTVR is feature drift, which manifests in two forms: intra-modal feature drift caused by continual learning within each modality, and non-cooperative feature drift across modalities that leads to modality misalignment. To mitigate these issues, we propose StructAlign, a structured cross-modal alignment method for CTVR. First, StructAlign introduces a simplex Equiangular Tight Frame (ETF) geometry as a unified geometric prior to mitigate modality misalignment. Building upon this geometric prior, we design a cross-modal ETF alignment loss that aligns text and video features with category-level ETF prototypes, encouraging the learned representations to form an approximate simplex ETF geometry. In addition, to suppress intra-modal feature drift, we design a Cross-modal Relation Preserving loss, which leverages complementary modalities to preserve cross-modal similarity relations, providing stable relational supervision for feature updates. By jointly addressing non-cooperative feature drift across modalities and intra-modal feature drift, StructAlign effectively alleviates catastrophic forgetting in CTVR. Extensive experiments on benchmark datasets demonstrate that our method consistently outperforms state-of-the-art continual retrieval approaches.",
    "arxiv_url": "https://arxiv.org/abs/2601.20597v1",
    "pdf_url": "https://arxiv.org/pdf/2601.20597v1",
    "published_date": "2026-01-28",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "benchmark",
      "text-to-video",
      "dit"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.20597v1",
      "pdf": "https://arxiv.org/pdf/2601.20597v1"
    },
    "bibtex": ""
  },
  {
    "title": "Advancing Open-source World Models",
    "authors": [
      "Robbyant Team",
      "Zelin Gao",
      "Qiuyu Wang",
      "Yanhong Zeng",
      "Jiapeng Zhu",
      "Ka Leong Cheng",
      "Yixuan Li",
      "Hanlin Wang",
      "Yinghao Xu",
      "Shuailei Ma",
      "Yihang Chen",
      "Jie Liu",
      "Yansong Cheng",
      "Yao Yao",
      "Jiayi Zhu",
      "Yihao Meng",
      "Kecheng Zheng",
      "Qingyan Bai",
      "Jingye Chen",
      "Zehong Shen",
      "Yue Yu",
      "Xing Zhu",
      "Yujun Shen",
      "Hao Ouyang"
    ],
    "abstract": "We present LingBot-World, an open-sourced world simulator stemming from video generation. Positioned as a top-tier world model, LingBot-World offers the following features. (1) It maintains high fidelity and robust dynamics in a broad spectrum of environments, including realism, scientific contexts, cartoon styles, and beyond. (2) It enables a minute-level horizon while preserving contextual consistency over time, which is also known as \"long-term memory\". (3) It supports real-time interactivity, achieving a latency of under 1 second when producing 16 frames per second. We provide public access to the code and model in an effort to narrow the divide between open-source and closed-source technologies. We believe our release will empower the community with practical applications across areas like content creation, gaming, and robot learning.",
    "arxiv_url": "https://arxiv.org/abs/2601.20540v1",
    "pdf_url": "https://arxiv.org/pdf/2601.20540v1",
    "published_date": "2026-01-28",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "world model",
      "video generation",
      "world simulator",
      "dynamics",
      "style"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.20540v1",
      "pdf": "https://arxiv.org/pdf/2601.20540v1"
    },
    "bibtex": ""
  },
  {
    "title": "Latent Temporal Discrepancy as Motion Prior: A Loss-Weighting Strategy for Dynamic Fidelity in T2V",
    "authors": [
      "Meiqi Wu",
      "Bingze Song",
      "Ruimin Lin",
      "Chen Zhu",
      "Xiaokun Feng",
      "Jiahong Wu",
      "Xiangxiang Chu",
      "Kaiqi Huang"
    ],
    "abstract": "Video generation models have achieved notable progress in static scenarios, yet their performance in motion video generation remains limited, with quality degrading under drastic dynamic changes. This is due to noise disrupting temporal coherence and increasing the difficulty of learning dynamic regions. {Unfortunately, existing diffusion models rely on static loss for all scenarios, constraining their ability to capture complex dynamics.} To address this issue, we introduce Latent Temporal Discrepancy (LTD) as a motion prior to guide loss weighting. LTD measures frame-to-frame variation in the latent space, assigning larger penalties to regions with higher discrepancy while maintaining regular optimization for stable regions. This motion-aware strategy stabilizes training and enables the model to better reconstruct high-frequency dynamics. Extensive experiments on the general benchmark VBench and the motion-focused VMBench show consistent gains, with our method outperforming strong baselines by 3.31% on VBench and 3.58% on VMBench, achieving significant improvements in motion quality.",
    "arxiv_url": "https://arxiv.org/abs/2601.20504v1",
    "pdf_url": "https://arxiv.org/pdf/2601.20504v1",
    "published_date": "2026-01-28",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "video generation",
      "diffusion model",
      "dynamics",
      "t2v",
      "benchmark"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.20504v1",
      "pdf": "https://arxiv.org/pdf/2601.20504v1"
    },
    "bibtex": ""
  },
  {
    "title": "Efficient Autoregressive Video Diffusion with Dummy Head",
    "authors": [
      "Hang Guo",
      "Zhaoyang Jia",
      "Jiahao Li",
      "Bin Li",
      "Yuanhao Cai",
      "Jiangshan Wang",
      "Yawei Li",
      "Yan Lu"
    ],
    "abstract": "The autoregressive video diffusion model has recently gained considerable research interest due to its causal modeling and iterative denoising. In this work, we identify that the multi-head self-attention in these models under-utilizes historical frames: approximately 25% heads attend almost exclusively to the current frame, and discarding their KV caches incurs only minor performance degradation. Building upon this, we propose Dummy Forcing, a simple yet effective method to control context accessibility across different heads. Specifically, the proposed heterogeneous memory allocation reduces head-wise context redundancy, accompanied by dynamic head programming to adaptively classify head types. Moreover, we develop a context packing technique to achieve more aggressive cache compression. Without additional training, our Dummy Forcing delivers up to 2.0x speedup over the baseline, supporting video generation at 24.3 FPS with less than 0.5% quality drop. Project page is available at https://csguoh.github.io/project/DummyForcing/.",
    "arxiv_url": "https://arxiv.org/abs/2601.20499v1",
    "pdf_url": "https://arxiv.org/pdf/2601.20499v1",
    "published_date": "2026-01-28",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "video diffusion",
      "dit",
      "video generation",
      "diffusion model",
      "denoising",
      "autoregressive",
      "efficient"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.20499v1",
      "pdf": "https://arxiv.org/pdf/2601.20499v1",
      "project": "https://csguoh.github.io/project/DummyForcing"
    },
    "bibtex": ""
  },
  {
    "title": "Artifact-Aware Evaluation for High-Quality Video Generation",
    "authors": [
      "Chen Zhu",
      "Jiashu Zhu",
      "Yanxun Li",
      "Meiqi Wu",
      "Bingze Song",
      "Chubin Chen",
      "Jiahong Wu",
      "Xiangxiang Chu",
      "Yangang Wang"
    ],
    "abstract": "With the rapid advancement of video generation techniques, evaluating and auditing generated videos has become increasingly crucial. Existing approaches typically offer coarse video quality scores, lacking detailed localization and categorization of specific artifacts. In this work, we introduce a comprehensive evaluation protocol focusing on three key aspects affecting human perception: Appearance, Motion, and Camera. We define these axes through a taxonomy of 10 prevalent artifact categories reflecting common generative failures observed in video generation. To enable robust artifact detection and categorization, we introduce GenVID, a large-scale dataset of 80k videos generated by various state-of-the-art video generation models, each carefully annotated for the defined artifact categories. Leveraging GenVID, we develop DVAR, a Dense Video Artifact Recognition framework for fine-grained identification and classification of generative artifacts. Extensive experiments show that our approach significantly improves artifact detection accuracy and enables effective filtering of low-quality content.",
    "arxiv_url": "https://arxiv.org/abs/2601.20297v1",
    "pdf_url": "https://arxiv.org/pdf/2601.20297v1",
    "published_date": "2026-01-28",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "evaluation",
      "video generation",
      "dit"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.20297v1",
      "pdf": "https://arxiv.org/pdf/2601.20297v1"
    },
    "bibtex": ""
  },
  {
    "title": "StreamFusion: Scalable Sequence Parallelism for Distributed Inference of Diffusion Transformers on GPUs",
    "authors": [
      "Jiacheng Yang",
      "Jun Wu",
      "Yaoyao Ding",
      "Zhiying Xu",
      "Yida Wang",
      "Gennady Pekhimenko"
    ],
    "abstract": "Diffusion Transformers (DiTs) have gained increasing adoption in high-quality image and video generation. As demand for higher-resolution images and longer videos increases, single-GPU inference becomes inefficient due to increased latency and large activation sizes. Current frameworks employ sequence parallelism (SP) techniques such as Ulysses Attention and Ring Attention to scale inference. However, these implementations have three primary limitations: (1) suboptimal communication patterns for network topologies on modern GPU machines, (2) latency bottlenecks from all-to-all operations in inter-machine communication, and (3) GPU sender-receiver synchronization and computation overheads from using two-sided communication libraries. To address these issues, we present StreamFusion, a topology-aware efficient DiT serving engine. StreamFusion incorporates three key innovations: (1) a topology-aware sequence parallelism technique that accounts for inter- and intra-machine bandwidth differences, (2) Torus Attention, a novel SP technique enabling overlapping of inter-machine all-to-all operations with computation, and (3) a one-sided communication implementation that minimizes GPU sender-receiver synchronization and computation overheads. Our experiments demonstrate that StreamFusion outperforms the state-of-the-art approach by an average of $1.35\\times$ (up to $1.77\\times$).",
    "arxiv_url": "https://arxiv.org/abs/2601.20273v1",
    "pdf_url": "https://arxiv.org/pdf/2601.20273v1",
    "published_date": "2026-01-28",
    "categories": [
      "cs.DC",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "diffusion transformer",
      "video generation",
      "dit"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.20273v1",
      "pdf": "https://arxiv.org/pdf/2601.20273v1"
    },
    "bibtex": ""
  },
  {
    "title": "TeleStyle: Content-Preserving Style Transfer in Images and Videos",
    "authors": [
      "Shiwen Zhang",
      "Xiaoyan Yang",
      "Bojia Zi",
      "Haibin Huang",
      "Chi Zhang",
      "Xuelong Li"
    ],
    "abstract": "Content-preserving style transfer, generating stylized outputs based on content and style references, remains a significant challenge for Diffusion Transformers (DiTs) due to the inherent entanglement of content and style features in their internal representations. In this technical report, we present TeleStyle, a lightweight yet effective model for both image and video stylization. Built upon Qwen-Image-Edit, TeleStyle leverages the base model's robust capabilities in content preservation and style customization. To facilitate effective training, we curated a high-quality dataset of distinct specific styles and further synthesized triplets using thousands of diverse, in-the-wild style categories. We introduce a Curriculum Continual Learning framework to train TeleStyle on this hybrid dataset of clean (curated) and noisy (synthetic) triplets. This approach enables the model to generalize to unseen styles without compromising precise content fidelity. Additionally, we introduce a video-to-video stylization module to enhance temporal consistency and visual quality. TeleStyle achieves state-of-the-art performance across three core evaluation metrics: style similarity, content consistency, and aesthetic quality. Code and pre-trained models are available at https://github.com/Tele-AI/TeleStyle",
    "arxiv_url": "https://arxiv.org/abs/2601.20175v1",
    "pdf_url": "https://arxiv.org/pdf/2601.20175v1",
    "published_date": "2026-01-28",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/Tele-AI/TeleStyle",
    "keywords": [
      "temporal consistency",
      "dit",
      "diffusion transformer",
      "video-to-video",
      "style",
      "evaluation",
      "customization"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.20175v1",
      "pdf": "https://arxiv.org/pdf/2601.20175v1",
      "github": "https://github.com/Tele-AI/TeleStyle"
    },
    "bibtex": ""
  },
  {
    "title": "Entropy-Guided k-Guard Sampling for Long-Horizon Autoregressive Video Generation",
    "authors": [
      "Yizhao Han",
      "Tianxing Shi",
      "Zhao Wang",
      "Zifan Xu",
      "Zhiyuan Pu",
      "Mingxiao Li",
      "Qian Zhang",
      "Wei Yin",
      "Xiao-Xiao Long"
    ],
    "abstract": "Autoregressive (AR) architectures have achieved significant successes in LLMs, inspiring explorations for video generation. In LLMs, top-p/top-k sampling strategies work exceptionally well: language tokens have high semantic density and low redundancy, so a fixed size of token candidates already strikes a balance between semantic accuracy and generation diversity. In contrast, video tokens have low semantic density and high spatio-temporal redundancy. This mismatch makes static top-k/top-p strategies ineffective for video decoders: they either introduce unnecessary randomness for low-uncertainty regions (static backgrounds) or get stuck in early errors for high-uncertainty regions (foreground objects). Prediction errors will accumulate as more frames are generated and eventually severely degrade long-horizon quality. To address this, we propose Entropy-Guided k-Guard (ENkG) sampling, a simple yet effective strategy that adapts sampling to token-wise dispersion, quantified by the entropy of each token's predicted distribution. ENkG uses adaptive token candidate sizes: for low-entropy regions, it employs fewer candidates to suppress redundant noise and preserve structural integrity; for high-entropy regions, it uses more candidates to mitigate error compounding. ENkG is model-agnostic, training-free, and adds negligible overhead. Experiments demonstrate consistent improvements in perceptual quality and structural stability compared to static top-k/top-p strategies.",
    "arxiv_url": "https://arxiv.org/abs/2601.19488v2",
    "pdf_url": "https://arxiv.org/pdf/2601.19488v2",
    "published_date": "2026-01-27",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "architecture",
      "video generation",
      "autoregressive"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.19488v2",
      "pdf": "https://arxiv.org/pdf/2601.19488v2"
    },
    "bibtex": ""
  },
  {
    "title": "VC-Bench: Pioneering the Video Connecting Benchmark with a Dataset and Evaluation Metrics",
    "authors": [
      "Zhiyu Yin",
      "Zhipeng Liu",
      "Kehai Chen",
      "Lemao Liu",
      "Jin Liu",
      "Hong-Dong Li",
      "Yang Xiang",
      "Min Zhang"
    ],
    "abstract": "While current video generation focuses on text or image conditions, practical applications like video editing and vlogging often need to seamlessly connect separate clips. In our work, we introduce Video Connecting, an innovative task that aims to generate smooth intermediate video content between given start and end clips. However, the absence of standardized evaluation benchmarks has hindered the development of this task. To bridge this gap, we proposed VC-Bench, a novel benchmark specifically designed for video connecting. It includes 1,579 high-quality videos collected from public platforms, covering 15 main categories and 72 subcategories to ensure diversity and structure. VC-Bench focuses on three core aspects: Video Quality Score VQS, Start-End Consistency Score SECS, and Transition Smoothness Score TSS. Together, they form a comprehensive framework that moves beyond conventional quality-only metrics. We evaluated multiple state-of-the-art video generation models on VC-Bench. Experimental results reveal significant limitations in maintaining start-end consistency and transition smoothness, leading to lower overall coherence and fluidity. We expect that VC-Bench will serve as a pioneering benchmark to inspire and guide future research in video connecting. The evaluation metrics and dataset are publicly available at: https://anonymous.4open.science/r/VC-Bench-1B67/.",
    "arxiv_url": "https://arxiv.org/abs/2601.19236v1",
    "pdf_url": "https://arxiv.org/pdf/2601.19236v1",
    "published_date": "2026-01-27",
    "categories": [
      "cs.CV",
      "cs.MM"
    ],
    "github_url": "",
    "keywords": [
      "dit",
      "video editing",
      "video generation",
      "evaluation",
      "benchmark"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.19236v1",
      "pdf": "https://arxiv.org/pdf/2601.19236v1",
      "project": "https://anonymous.4open.science/r/VC-Bench-1B67"
    },
    "bibtex": ""
  },
  {
    "title": "FreeOrbit4D: Training-Free Arbitrary Camera Redirection for Monocular Videos via Geometry-Complete 4D Reconstruction",
    "authors": [
      "Wei Cao",
      "Hao Zhang",
      "Fengrui Tian",
      "Yulun Wu",
      "Yingying Li",
      "Shenlong Wang",
      "Ning Yu",
      "Yaoyao Liu"
    ],
    "abstract": "Camera redirection aims to replay a dynamic scene from a single monocular video under a user-specified camera trajectory. However, large-angle redirection is inherently ill-posed: a monocular video captures only a narrow spatio-temporal view of a dynamic 3D scene, providing highly partial observations of the underlying 4D world. The key challenge is therefore to recover a complete and coherent representation from this limited input, with consistent geometry and motion. While recent diffusion-based methods achieve impressive results, they often break down under large-angle viewpoint changes far from the original trajectory, where missing visual grounding leads to severe geometric ambiguity and temporal inconsistency. To address this, we present FreeOrbit4D, an effective training-free framework that tackles this geometric ambiguity by recovering a geometry-complete 4D proxy as structural grounding for video generation. We obtain this proxy by decoupling foreground and background reconstructions: we unproject the monocular video into a static background and geometry-incomplete foreground point clouds in a unified global space, then leverage an object-centric multi-view diffusion model to synthesize multi-view images and reconstruct geometry-complete foreground point clouds in canonical object space. By aligning the canonical foreground point cloud to the global scene space via dense pixel-synchronized 3D--3D correspondences and projecting the geometry-complete 4D proxy onto target camera viewpoints, we provide geometric scaffolds that guide a conditional video diffusion model. Extensive experiments show that FreeOrbit4D produces more faithful redirected videos under challenging large-angle trajectories, and our geometry-complete 4D proxy further opens a potential avenue for practical applications such as edit propagation and 4D data generation. Project page and code will be released soon.",
    "arxiv_url": "https://arxiv.org/abs/2601.18993v1",
    "pdf_url": "https://arxiv.org/pdf/2601.18993v1",
    "published_date": "2026-01-26",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "video diffusion",
      "dit",
      "trajectory",
      "video generation",
      "diffusion model"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.18993v1",
      "pdf": "https://arxiv.org/pdf/2601.18993v1"
    },
    "bibtex": ""
  },
  {
    "title": "Are Video Generation Models Geographically Fair? An Attraction-Centric Evaluation of Global Visual Knowledge",
    "authors": [
      "Xiao Liu",
      "Jiawei Zhang"
    ],
    "abstract": "Recent advances in text-to-video generation have produced visually compelling results, yet it remains unclear whether these models encode geographically equitable visual knowledge. In this work, we investigate the geo-equity and geographically grounded visual knowledge of text-to-video models through an attraction-centric evaluation. We introduce Geo-Attraction Landmark Probing (GAP), a systematic framework for assessing how faithfully models synthesize tourist attractions from diverse regions, and construct GEOATTRACTION-500, a benchmark of 500 globally distributed attractions spanning varied regions and popularity levels. GAP integrates complementary metrics that disentangle overall video quality from attraction-specific knowledge, including global structural alignment, fine-grained keypoint-based alignment, and vision-language model judgments, all validated against human evaluation. Applying GAP to the state-of-the-art text-to-video model Sora 2, we find that, contrary to common assumptions of strong geographic bias, the model exhibits a relatively uniform level of geographically grounded visual knowledge across regions, development levels, and cultural groupings, with only weak dependence on attraction popularity. These results suggest that current text-to-video models express global visual knowledge more evenly than expected, highlighting both their promise for globally deployed applications and the need for continued evaluation as such systems evolve.",
    "arxiv_url": "https://arxiv.org/abs/2601.18698v1",
    "pdf_url": "https://arxiv.org/pdf/2601.18698v1",
    "published_date": "2026-01-26",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "benchmark",
      "evaluation",
      "text-to-video",
      "video generation"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.18698v1",
      "pdf": "https://arxiv.org/pdf/2601.18698v1"
    },
    "bibtex": ""
  },
  {
    "title": "Self-Refining Video Sampling",
    "authors": [
      "Sangwon Jang",
      "Taekyung Ki",
      "Jaehyeong Jo",
      "Saining Xie",
      "Jaehong Yoon",
      "Sung Ju Hwang"
    ],
    "abstract": "Modern video generators still struggle with complex physical dynamics, often falling short of physical realism. Existing approaches address this using external verifiers or additional training on augmented data, which is computationally expensive and still limited in capturing fine-grained motion. In this work, we present self-refining video sampling, a simple method that uses a pre-trained video generator trained on large-scale datasets as its own self-refiner. By interpreting the generator as a denoising autoencoder, we enable iterative inner-loop refinement at inference time without any external verifier or additional training. We further introduce an uncertainty-aware refinement strategy that selectively refines regions based on self-consistency, which prevents artifacts caused by over-refinement. Experiments on state-of-the-art video generators demonstrate significant improvements in motion coherence and physics alignment, achieving over 70\\% human preference compared to the default sampler and guidance-based sampler.",
    "arxiv_url": "https://arxiv.org/abs/2601.18577v1",
    "pdf_url": "https://arxiv.org/pdf/2601.18577v1",
    "published_date": "2026-01-26",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "dit",
      "physical",
      "dynamics",
      "denoising",
      "physics"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.18577v1",
      "pdf": "https://arxiv.org/pdf/2601.18577v1"
    },
    "bibtex": ""
  },
  {
    "title": "Beyond Rigid: Benchmarking Non-Rigid Video Editing",
    "authors": [
      "Bingzheng Qu",
      "Kehai Chen",
      "Xuefeng Bai",
      "Jun Yu",
      "Min Zhang"
    ],
    "abstract": "Despite the remarkable progress in text-driven video editing, generating coherent non-rigid deformations remains a critical challenge, often plagued by physical distortion and temporal flicker. To bridge this gap, we propose NRVBench, the first dedicated and comprehensive benchmark designed to evaluate non-rigid video editing. First, we curate a high-quality dataset consisting of 180 non-rigid motion videos from six physics-based categories, equipped with 2,340 fine-grained task instructions and 360 multiple-choice questions. Second, we propose NRVE-Acc, a novel evaluation metric based on Vision-Language Models that can rigorously assess physical compliance, temporal consistency, and instruction alignment, overcoming the limitations of general metrics in capturing complex dynamics. Third, we introduce a training-free baseline, VM-Edit, which utilizes a dual-region denoising mechanism to achieve structure-aware control, balancing structural preservation and dynamic deformation. Extensive experiments demonstrate that while current methods have shortcomings in maintaining physical plausibility, our method achieves excellent performance across both standard and proposed metrics. We believe the benchmark could serve as a standard testing platform for advancing physics-aware video editing.",
    "arxiv_url": "https://arxiv.org/abs/2601.18340v1",
    "pdf_url": "https://arxiv.org/pdf/2601.18340v1",
    "published_date": "2026-01-26",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "temporal consistency",
      "dit",
      "video editing",
      "physical",
      "evaluation",
      "dynamics",
      "denoising",
      "physics-aware",
      "benchmark",
      "text-driven video",
      "physics"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.18340v1",
      "pdf": "https://arxiv.org/pdf/2601.18340v1"
    },
    "bibtex": ""
  },
  {
    "title": "ViTCoP: Accelerating Large Vision-Language Models via Visual and Textual Semantic Collaborative Pruning",
    "authors": [
      "Wen Luo",
      "Peng Chen",
      "Xiaotao Huang",
      "LiQun Huang"
    ],
    "abstract": "Large Vision-Language Models (LVLMs) incur high computational costs due to significant redundancy in their visual tokens. To effectively reduce this cost, researchers have proposed various visual token pruning methods. However, existing methods are generally limited, either losing critical visual information prematurely due to pruning in the vision encoder, or leading to information redundancy among the selected tokens due to pruning in the Large Language Models (LLMs). To address these challenges, we propose a Visual and Textual Semantic Collaborative Pruning framework (ViTCoP) that combines redundancy filtering in the vision encoder with step-wise co-pruning within the LLM based on its hierarchical characteristics, to efficiently preserve critical and informationally diverse visual tokens. Meanwhile, to ensure compatibility with acceleration techniques like FlashAttention, we introduce the L2 norm of K-vectors as the token saliency metric in the LLM. Extensive experiments on various Large Vision-Language Models demonstrate that ViTCoP not only achieves state-of-the-art performance surpassing existing methods on both image and video understanding tasks, but also significantly reduces model inference latency and GPU memory consumption. Notably, its performance advantage over other methods becomes even more pronounced under extreme pruning rates.",
    "arxiv_url": "https://arxiv.org/abs/2601.17818v1",
    "pdf_url": "https://arxiv.org/pdf/2601.17818v1",
    "published_date": "2026-01-25",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "acceleration"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.17818v1",
      "pdf": "https://arxiv.org/pdf/2601.17818v1"
    },
    "bibtex": ""
  },
  {
    "title": "MV-S2V: Multi-View Subject-Consistent Video Generation",
    "authors": [
      "Ziyang Song",
      "Xinyu Gong",
      "Bangya Liu",
      "Zelin Zhao"
    ],
    "abstract": "Existing Subject-to-Video Generation (S2V) methods have achieved high-fidelity and subject-consistent video generation, yet remain constrained to single-view subject references. This limitation renders the S2V task reducible to an S2I + I2V pipeline, failing to exploit the full potential of video subject control. In this work, we propose and address the challenging Multi-View S2V (MV-S2V) task, which synthesizes videos from multiple reference views to enforce 3D-level subject consistency. Regarding the scarcity of training data, we first develop a synthetic data curation pipeline to generate highly customized synthetic data, complemented by a small-scale real-world captured dataset to boost the training of MV-S2V. Another key issue lies in the potential confusion between cross-subject and cross-view references in conditional generation. To overcome this, we further introduce Temporally Shifted RoPE (TS-RoPE) to distinguish between different subjects and distinct views of the same subject in reference conditioning. Our framework achieves superior 3D subject consistency w.r.t. multi-view reference images and high-quality visual outputs, establishing a new meaningful direction for subject-driven video generation. Our project page is available at: https://szy-young.github.io/mv-s2v",
    "arxiv_url": "https://arxiv.org/abs/2601.17756v2",
    "pdf_url": "https://arxiv.org/pdf/2601.17756v2",
    "published_date": "2026-01-25",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "subject-driven",
      "dit",
      "video generation",
      "i2v"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.17756v2",
      "pdf": "https://arxiv.org/pdf/2601.17756v2",
      "project": "https://szy-young.github.io/mv-s2v"
    },
    "bibtex": ""
  },
  {
    "title": "The Script is All You Need: An Agentic Framework for Long-Horizon Dialogue-to-Cinematic Video Generation",
    "authors": [
      "Chenyu Mu",
      "Xin He",
      "Qu Yang",
      "Wanshun Chen",
      "Jiadi Yao",
      "Huang Liu",
      "Zihao Yi",
      "Bo Zhao",
      "Xingyu Chen",
      "Ruotian Ma",
      "Fanghua Ye",
      "Erkun Yang",
      "Cheng Deng",
      "Zhaopeng Tu",
      "Xiaolong Li",
      "Linus"
    ],
    "abstract": "Recent advances in video generation have produced models capable of synthesizing stunning visual content from simple text prompts. However, these models struggle to generate long-form, coherent narratives from high-level concepts like dialogue, revealing a ``semantic gap'' between a creative idea and its cinematic execution. To bridge this gap, we introduce a novel, end-to-end agentic framework for dialogue-to-cinematic-video generation. Central to our framework is ScripterAgent, a model trained to translate coarse dialogue into a fine-grained, executable cinematic script. To enable this, we construct ScriptBench, a new large-scale benchmark with rich multimodal context, annotated via an expert-guided pipeline. The generated script then guides DirectorAgent, which orchestrates state-of-the-art video models using a cross-scene continuous generation strategy to ensure long-horizon coherence. Our comprehensive evaluation, featuring an AI-powered CriticAgent and a new Visual-Script Alignment (VSA) metric, shows our framework significantly improves script faithfulness and temporal fidelity across all tested video models. Furthermore, our analysis uncovers a crucial trade-off in current SOTA models between visual spectacle and strict script adherence, providing valuable insights for the future of automated filmmaking.",
    "arxiv_url": "https://arxiv.org/abs/2601.17737v2",
    "pdf_url": "https://arxiv.org/pdf/2601.17737v2",
    "published_date": "2026-01-25",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "creative",
      "long-form",
      "video generation",
      "film",
      "evaluation",
      "concept",
      "benchmark"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.17737v2",
      "pdf": "https://arxiv.org/pdf/2601.17737v2"
    },
    "bibtex": ""
  },
  {
    "title": "SkyReels-V3 Technique Report",
    "authors": [
      "Debang Li",
      "Zhengcong Fei",
      "Tuanhui Li",
      "Yikun Dou",
      "Zheng Chen",
      "Jiangping Yang",
      "Mingyuan Fan",
      "Jingtao Xu",
      "Jiahua Wang",
      "Baoxuan Gu",
      "Mingshan Chang",
      "Wenjing Cai",
      "Yuqiang Xie",
      "Binjie Mao",
      "Youqiang Zhang",
      "Nuo Pang",
      "Hao Zhang",
      "Yuzhe Jin",
      "Zhiheng Xu",
      "Dixuan Lin",
      "Guibin Chen",
      "Yahui Zhou"
    ],
    "abstract": "Video generation serves as a cornerstone for building world models, where multimodal contextual inference stands as the defining test of capability. In this end, we present SkyReels-V3, a conditional video generation model, built upon a unified multimodal in-context learning framework with diffusion Transformers. SkyReels-V3 model supports three core generative paradigms within a single architecture: reference images-to-video synthesis, video-to-video extension and audio-guided video generation. (i) reference images-to-video model is designed to produce high-fidelity videos with strong subject identity preservation, temporal coherence, and narrative consistency. To enhance reference adherence and compositional stability, we design a comprehensive data processing pipeline that leverages cross frame pairing, image editing, and semantic rewriting, effectively mitigating copy paste artifacts. During training, an image video hybrid strategy combined with multi-resolution joint optimization is employed to improve generalization and robustness across diverse scenarios. (ii) video extension model integrates spatio-temporal consistency modeling with large-scale video understanding, enabling both seamless single-shot continuation and intelligent multi-shot switching with professional cinematographic patterns. (iii) Talking avatar model supports minute-level audio-conditioned video generation by training first-and-last frame insertion patterns and reconstructing key-frame inference paradigms. On the basis of ensuring visual quality, synchronization of audio and videos has been optimized.   Extensive evaluations demonstrate that SkyReels-V3 achieves state-of-the-art or near state-of-the-art performance on key metrics including visual quality, instruction following, and specific aspect metrics, approaching leading closed-source systems. Github: https://github.com/SkyworkAI/SkyReels-V3.",
    "arxiv_url": "https://arxiv.org/abs/2601.17323v2",
    "pdf_url": "https://arxiv.org/pdf/2601.17323v2",
    "published_date": "2026-01-24",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/SkyworkAI/SkyReels-V3",
    "keywords": [
      "temporal consistency",
      "dit",
      "world model",
      "diffusion transformer",
      "identity",
      "video generation",
      "video-to-video",
      "evaluation",
      "video synthesis",
      "architecture",
      "avatar"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.17323v2",
      "pdf": "https://arxiv.org/pdf/2601.17323v2",
      "github": "https://github.com/SkyworkAI/SkyReels-V3"
    },
    "bibtex": ""
  },
  {
    "title": "AnyView: Synthesizing Any Novel View in Dynamic Scenes",
    "authors": [
      "Basile Van Hoorick",
      "Dian Chen",
      "Shun Iwase",
      "Pavel Tokmakov",
      "Muhammad Zubair Irshad",
      "Igor Vasiljevic",
      "Swati Gupta",
      "Fangzhou Cheng",
      "Sergey Zakharov",
      "Vitor Campagnolo Guizilini"
    ],
    "abstract": "Modern generative video models excel at producing convincing, high-quality outputs, but struggle to maintain multi-view and spatiotemporal consistency in highly dynamic real-world environments. In this work, we introduce \\textbf{AnyView}, a diffusion-based video generation framework for \\emph{dynamic view synthesis} with minimal inductive biases or geometric assumptions. We leverage multiple data sources with various levels of supervision, including monocular (2D), multi-view static (3D) and multi-view dynamic (4D) datasets, to train a generalist spatiotemporal implicit representation capable of producing zero-shot novel videos from arbitrary camera locations and trajectories. We evaluate AnyView on standard benchmarks, showing competitive results with the current state of the art, and propose \\textbf{AnyViewBench}, a challenging new benchmark tailored towards \\emph{extreme} dynamic view synthesis in diverse real-world scenarios. In this more dramatic setting, we find that most baselines drastically degrade in performance, as they require significant overlap between viewpoints, while AnyView maintains the ability to produce realistic, plausible, and spatiotemporally consistent videos when prompted from \\emph{any} viewpoint. Results, data, code, and models can be viewed at: https://tri-ml.github.io/AnyView/",
    "arxiv_url": "https://arxiv.org/abs/2601.16982v1",
    "pdf_url": "https://arxiv.org/pdf/2601.16982v1",
    "published_date": "2026-01-23",
    "categories": [
      "cs.CV",
      "cs.LG",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "benchmark",
      "novel view",
      "video generation",
      "temporal consistency"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.16982v1",
      "pdf": "https://arxiv.org/pdf/2601.16982v1",
      "project": "https://tri-ml.github.io/AnyView"
    },
    "bibtex": ""
  },
  {
    "title": "Reward-Forcing: Autoregressive Video Generation with Reward Feedback",
    "authors": [
      "Jingran Zhang",
      "Ning Li",
      "Yuanhao Ban",
      "Andrew Bai",
      "Justin Cui"
    ],
    "abstract": "While most prior work in video generation relies on bidirectional architectures, recent efforts have sought to adapt these models into autoregressive variants to support near real-time generation. However, such adaptations often depend heavily on teacher models, which can limit performance, particularly in the absence of a strong autoregressive teacher, resulting in output quality that typically lags behind their bidirectional counterparts. In this paper, we explore an alternative approach that uses reward signals to guide the generation process, enabling more efficient and scalable autoregressive generation. By using reward signals to guide the model, our method simplifies training while preserving high visual fidelity and temporal consistency. Through extensive experiments on standard benchmarks, we find that our approach performs comparably to existing autoregressive models and, in some cases, surpasses similarly sized bidirectional models by avoiding constraints imposed by teacher architectures. For example, on VBench, our method achieves a total score of 84.92, closely matching state-of-the-art autoregressive methods that score 84.31 but require significant heterogeneous distillation.",
    "arxiv_url": "https://arxiv.org/abs/2601.16933v1",
    "pdf_url": "https://arxiv.org/pdf/2601.16933v1",
    "published_date": "2026-01-23",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "temporal consistency",
      "distillation",
      "video generation",
      "efficient",
      "architecture",
      "autoregressive",
      "benchmark"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.16933v1",
      "pdf": "https://arxiv.org/pdf/2601.16933v1"
    },
    "bibtex": ""
  },
  {
    "title": "LoL: Longer than Longer, Scaling Video Generation to Hour",
    "authors": [
      "Justin Cui",
      "Jie Wu",
      "Ming Li",
      "Tao Yang",
      "Xiaojie Li",
      "Rui Wang",
      "Andrew Bai",
      "Yuanhao Ban",
      "Cho-Jui Hsieh"
    ],
    "abstract": "Recent research in long-form video generation has shifted from bidirectional to autoregressive models, yet these methods commonly suffer from error accumulation and a loss of long-term coherence. While attention sink frames have been introduced to mitigate this performance decay, they often induce a critical failure mode we term sink-collapse: the generated content repeatedly reverts to the sink frame, resulting in abrupt scene resets and cyclic motion patterns. Our analysis reveals that sink-collapse originates from an inherent conflict between the periodic structure of Rotary Position Embedding (RoPE) and the multi-head attention mechanisms prevalent in current generative models. To address it, we propose a lightweight, training-free approach that effectively suppresses this behavior by introducing multi-head RoPE jitter that breaks inter-head attention homogenization and mitigates long-horizon collapse. Extensive experiments show that our method successfully alleviates sink-collapse while preserving generation quality. To the best of our knowledge, this work achieves the first demonstration of real-time, streaming, and infinite-length video generation with little quality decay. As an illustration of this robustness, we generate continuous videos up to 12 hours in length, which, to our knowledge, is among the longest publicly demonstrated results in streaming video generation.",
    "arxiv_url": "https://arxiv.org/abs/2601.16914v1",
    "pdf_url": "https://arxiv.org/pdf/2601.16914v1",
    "published_date": "2026-01-23",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "long-form",
      "video generation",
      "autoregressive",
      "streaming"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.16914v1",
      "pdf": "https://arxiv.org/pdf/2601.16914v1"
    },
    "bibtex": ""
  },
  {
    "title": "SALAD: Achieve High-Sparsity Attention via Efficient Linear Attention Tuning for Video Diffusion Transformer",
    "authors": [
      "Tongcheng Fang",
      "Hanling Zhang",
      "Ruiqi Xie",
      "Zhuo Han",
      "Xin Tao",
      "Tianchen Zhao",
      "Pengfei Wan",
      "Wenbo Ding",
      "Wanli Ouyang",
      "Xuefei Ning",
      "Yu Wang"
    ],
    "abstract": "Diffusion Transformers have recently demonstrated remarkable performance in video generation. However, the long input sequences result in high computational latency due to the quadratic complexity of full attention. Various sparse attention mechanisms have been proposed. Training-free sparse attention is constrained by limited sparsity and thus offers modest acceleration, whereas training-based methods can reach much higher sparsity but demand substantial data and computation for training. In this work, we propose SALAD, introducing a lightweight linear attention branch in parallel with the sparse attention. By incorporating an input-dependent gating mechanism to finely balance the two branches, our method attains 90% sparsity and 1.72x inference speedup, while maintaining generation quality comparable to the full attention baseline. Moreover, our finetuning process is highly efficient, requiring only 2,000 video samples and 1,600 training steps with a batch size of 8.",
    "arxiv_url": "https://arxiv.org/abs/2601.16515v1",
    "pdf_url": "https://arxiv.org/pdf/2601.16515v1",
    "published_date": "2026-01-23",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "acceleration",
      "video diffusion",
      "diffusion transformer",
      "video generation",
      "efficient"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.16515v1",
      "pdf": "https://arxiv.org/pdf/2601.16515v1"
    },
    "bibtex": ""
  },
  {
    "title": "Memory-V2V: Augmenting Video-to-Video Diffusion Models with Memory",
    "authors": [
      "Dohun Lee",
      "Chun-Hao Paul Huang",
      "Xuelin Chen",
      "Jong Chul Ye",
      "Duygu Ceylan",
      "Hyeonho Jeong"
    ],
    "abstract": "Recent foundational video-to-video diffusion models have achieved impressive results in editing user provided videos by modifying appearance, motion, or camera movement. However, real-world video editing is often an iterative process, where users refine results across multiple rounds of interaction. In this multi-turn setting, current video editors struggle to maintain cross-consistency across sequential edits. In this work, we tackle, for the first time, the problem of cross-consistency in multi-turn video editing and introduce Memory-V2V, a simple, yet effective framework that augments existing video-to-video models with explicit memory. Given an external cache of previously edited videos, Memory-V2V employs accurate retrieval and dynamic tokenization strategies to condition the current editing step on prior results. To further mitigate redundancy and computational overhead, we propose a learnable token compressor within the DiT backbone that compresses redundant conditioning tokens while preserving essential visual cues, achieving an overall speedup of 30%. We validate Memory-V2V on challenging tasks including video novel view synthesis and text-conditioned long video editing. Extensive experiments show that Memory-V2V produces videos that are significantly more cross-consistent with minimal computational overhead, while maintaining or even improving task-specific performance over state-of-the-art baselines. Project page: https://dohunlee1.github.io/MemoryV2V",
    "arxiv_url": "https://arxiv.org/abs/2601.16296v1",
    "pdf_url": "https://arxiv.org/pdf/2601.16296v1",
    "published_date": "2026-01-22",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "video diffusion",
      "dit",
      "video editing",
      "video-to-video",
      "diffusion model",
      "long video",
      "novel view"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.16296v1",
      "pdf": "https://arxiv.org/pdf/2601.16296v1",
      "project": "https://dohunlee1.github.io/MemoryV2V"
    },
    "bibtex": ""
  },
  {
    "title": "GR3EN: Generative Relighting for 3D Environments",
    "authors": [
      "Xiaoyan Xing",
      "Philipp Henzler",
      "Junhwa Hur",
      "Runze Li",
      "Jonathan T. Barron",
      "Pratul P. Srinivasan",
      "Dor Verbin"
    ],
    "abstract": "We present a method for relighting 3D reconstructions of large room-scale environments. Existing solutions for 3D scene relighting often require solving under-determined or ill-conditioned inverse rendering problems, and are as such unable to produce high-quality results on complex real-world scenes. Though recent progress in using generative image and video diffusion models for relighting has been promising, these techniques are either limited to 2D image and video relighting or 3D relighting of individual objects. Our approach enables controllable 3D relighting of room-scale scenes by distilling the outputs of a video-to-video relighting diffusion model into a 3D reconstruction. This side-steps the need to solve a difficult inverse rendering problem, and results in a flexible system that can relight 3D reconstructions of complex real-world scenes. We validate our approach on both synthetic and real-world datasets to show that it can faithfully render novel views of scenes under new lighting conditions.",
    "arxiv_url": "https://arxiv.org/abs/2601.16272v2",
    "pdf_url": "https://arxiv.org/pdf/2601.16272v2",
    "published_date": "2026-01-22",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "controllable",
      "video diffusion",
      "dit",
      "video-to-video",
      "diffusion model",
      "novel view"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.16272v2",
      "pdf": "https://arxiv.org/pdf/2601.16272v2"
    },
    "bibtex": ""
  },
  {
    "title": "A Mechanistic View on Video Generation as World Models: State and Dynamics",
    "authors": [
      "Luozhou Wang",
      "Zhifei Chen",
      "Yihua Du",
      "Dongyu Yan",
      "Wenhang Ge",
      "Guibao Shen",
      "Xinli Xu",
      "Leyi Wu",
      "Man Chen",
      "Tianshuo Xu",
      "Peiran Ren",
      "Xin Tao",
      "Pengfei Wan",
      "Ying-Cong Chen"
    ],
    "abstract": "Large-scale video generation models have demonstrated emergent physical coherence, positioning them as potential world models. However, a gap remains between contemporary \"stateless\" video architectures and classic state-centric world model theories. This work bridges this gap by proposing a novel taxonomy centered on two pillars: State Construction and Dynamics Modeling. We categorize state construction into implicit paradigms (context management) and explicit paradigms (latent compression), while dynamics modeling is analyzed through knowledge integration and architectural reformulation. Furthermore, we advocate for a transition in evaluation from visual fidelity to functional benchmarks, testing physical persistence and causal reasoning. We conclude by identifying two critical frontiers: enhancing persistence via data-driven memory and compressed fidelity, and advancing causality through latent factor decoupling and reasoning-prior integration. By addressing these challenges, the field can evolve from generating visually plausible videos to building robust, general-purpose world simulators.",
    "arxiv_url": "https://arxiv.org/abs/2601.17067v1",
    "pdf_url": "https://arxiv.org/pdf/2601.17067v1",
    "published_date": "2026-01-22",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "world model",
      "physical",
      "video generation",
      "evaluation",
      "world simulator",
      "architecture",
      "dynamics",
      "benchmark"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.17067v1",
      "pdf": "https://arxiv.org/pdf/2601.17067v1"
    },
    "bibtex": ""
  },
  {
    "title": "CamPilot: Improving Camera Control in Video Diffusion Model with Efficient Camera Reward Feedback",
    "authors": [
      "Wenhang Ge",
      "Guibao Shen",
      "Jiawei Feng",
      "Luozhou Wang",
      "Hao Lu",
      "Xingye Tian",
      "Xin Tao",
      "Ying-Cong Chen"
    ],
    "abstract": "Recent advances in camera-controlled video diffusion models have significantly improved video-camera alignment. However, the camera controllability still remains limited. In this work, we build upon Reward Feedback Learning and aim to further improve camera controllability. However, directly borrowing existing ReFL approaches faces several challenges. First, current reward models lack the capacity to assess video-camera alignment. Second, decoding latent into RGB videos for reward computation introduces substantial computational overhead. Third, 3D geometric information is typically neglected during video decoding. To address these limitations, we introduce an efficient camera-aware 3D decoder that decodes video latent into 3D representations for reward quantization. Specifically, video latent along with the camera pose are decoded into 3D Gaussians. In this process, the camera pose not only acts as input, but also serves as a projection parameter. Misalignment between the video latent and camera pose will cause geometric distortions in the 3D structure, resulting in blurry renderings. Based on this property, we explicitly optimize pixel-level consistency between the rendered novel views and ground-truth ones as reward. To accommodate the stochastic nature, we further introduce a visibility term that selectively supervises only deterministic regions derived via geometric warping. Extensive experiments conducted on RealEstate10K and WorldScore benchmarks demonstrate the effectiveness of our proposed method. Project page: \\href{https://a-bigbao.github.io/CamPilot/}{CamPilot Page}.",
    "arxiv_url": "https://arxiv.org/abs/2601.16214v1",
    "pdf_url": "https://arxiv.org/pdf/2601.16214v1",
    "published_date": "2026-01-22",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "video diffusion",
      "efficient",
      "diffusion model",
      "benchmark",
      "novel view",
      "camera control"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.16214v1",
      "pdf": "https://arxiv.org/pdf/2601.16214v1",
      "project": "https://a-bigbao.github.io/CamPilot"
    },
    "bibtex": ""
  },
  {
    "title": "PyraTok: Language-Aligned Pyramidal Tokenizer for Video Understanding and Generation",
    "authors": [
      "Onkar Susladkar",
      "Tushar Prakash",
      "Adheesh Juvekar",
      "Kiet A. Nguyen",
      "Dong-Hwan Jang",
      "Inderjit S Dhillon",
      "Ismini Lourentzou"
    ],
    "abstract": "Discrete video VAEs underpin modern text-to-video generation and video understanding systems, yet existing tokenizers typically learn visual codebooks at a single scale with limited vocabularies and shallow language supervision, leading to poor cross-modal alignment and zero-shot transfer. We introduce PyraTok, a language-aligned pyramidal tokenizer that learns semantically structured discrete latents across multiple spatiotemporal resolutions. PyraTok builds on a pretrained video VAE and a novel Language aligned Pyramidal Quantization (LaPQ) module that discretizes encoder features at several depths using a shared large binary codebook, yielding compact yet expressive video token sequences. To tightly couple visual tokens with language, PyraTok jointly optimizes multi-scale text-guided quantization and a global autoregressive objective over the token hierarchy. Across ten benchmarks, PyraTok delivers state-of-the-art (SOTA) video reconstruction, consistently improves text-to-video quality, and sets new SOTA zero-shot performance on video segmentation, temporal action localization, and video understanding, scaling robustly to up to 4K/8K resolutions.",
    "arxiv_url": "https://arxiv.org/abs/2601.16210v1",
    "pdf_url": "https://arxiv.org/pdf/2601.16210v1",
    "published_date": "2026-01-22",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "benchmark",
      "text-to-video",
      "video generation",
      "autoregressive"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.16210v1",
      "pdf": "https://arxiv.org/pdf/2601.16210v1"
    },
    "bibtex": ""
  },
  {
    "title": "360Anything: Geometry-Free Lifting of Images and Videos to 360",
    "authors": [
      "Ziyi Wu",
      "Daniel Watson",
      "Andrea Tagliasacchi",
      "David J. Fleet",
      "Marcus A. Brubaker",
      "Saurabh Saxena"
    ],
    "abstract": "Lifting perspective images and videos to 360 panoramas enables immersive 3D world generation. Existing approaches often rely on explicit geometric alignment between the perspective and the equirectangular projection (ERP) space. Yet, this requires known camera metadata, obscuring the application to in-the-wild data where such calibration is typically absent or noisy. We propose 360Anything, a geometry-free framework built upon pre-trained diffusion transformers. By treating the perspective input and the panorama target simply as token sequences, 360Anything learns the perspective-to-equirectangular mapping in a purely data-driven way, eliminating the need for camera information. Our approach achieves state-of-the-art performance on both image and video perspective-to-360 generation, outperforming prior works that use ground-truth camera information. We also trace the root cause of the seam artifacts at ERP boundaries to zero-padding in the VAE encoder, and introduce Circular Latent Encoding to facilitate seamless generation. Finally, we show competitive results in zero-shot camera FoV and orientation estimation benchmarks, demonstrating 360Anything's deep geometric understanding and broader utility in computer vision tasks. Additional results are available at https://360anything.github.io/.",
    "arxiv_url": "https://arxiv.org/abs/2601.16192v1",
    "pdf_url": "https://arxiv.org/pdf/2601.16192v1",
    "published_date": "2026-01-22",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "benchmark",
      "diffusion transformer",
      "dit"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.16192v1",
      "pdf": "https://arxiv.org/pdf/2601.16192v1",
      "project": "https://360anything.github.io"
    },
    "bibtex": ""
  },
  {
    "title": "Cosmos Policy: Fine-Tuning Video Models for Visuomotor Control and Planning",
    "authors": [
      "Moo Jin Kim",
      "Yihuai Gao",
      "Tsung-Yi Lin",
      "Yen-Chen Lin",
      "Yunhao Ge",
      "Grace Lam",
      "Percy Liang",
      "Shuran Song",
      "Ming-Yu Liu",
      "Chelsea Finn",
      "Jinwei Gu"
    ],
    "abstract": "Recent video generation models demonstrate remarkable ability to capture complex physical interactions and scene evolution over time. To leverage their spatiotemporal priors, robotics works have adapted video models for policy learning but introduce complexity by requiring multiple stages of post-training and new architectural components for action generation. In this work, we introduce Cosmos Policy, a simple approach for adapting a large pretrained video model (Cosmos-Predict2) into an effective robot policy through a single stage of post-training on the robot demonstration data collected on the target platform, with no architectural modifications. Cosmos Policy learns to directly generate robot actions encoded as latent frames within the video model's latent diffusion process, harnessing the model's pretrained priors and core learning algorithm to capture complex action distributions. Additionally, Cosmos Policy generates future state images and values (expected cumulative rewards), which are similarly encoded as latent frames, enabling test-time planning of action trajectories with higher likelihood of success. In our evaluations, Cosmos Policy achieves state-of-the-art performance on the LIBERO and RoboCasa simulation benchmarks (98.5% and 67.1% average success rates, respectively) and the highest average score in challenging real-world bimanual manipulation tasks, outperforming strong diffusion policies trained from scratch, video model-based policies, and state-of-the-art vision-language-action models fine-tuned on the same robot demonstrations. Furthermore, given policy rollout data, Cosmos Policy can learn from experience to refine its world model and value function and leverage model-based planning to achieve even higher success rates in challenging tasks. We release code, models, and training data at https://research.nvidia.com/labs/dir/cosmos-policy/",
    "arxiv_url": "https://arxiv.org/abs/2601.16163v1",
    "pdf_url": "https://arxiv.org/pdf/2601.16163v1",
    "published_date": "2026-01-22",
    "categories": [
      "cs.AI",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "dit",
      "world model",
      "physical",
      "video generation",
      "robotics",
      "evaluation",
      "simulation",
      "benchmark"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.16163v1",
      "pdf": "https://arxiv.org/pdf/2601.16163v1",
      "project": "https://research.nvidia.com/labs/dir/cosmos-policy"
    },
    "bibtex": ""
  },
  {
    "title": "Multimodal Climate Disinformation Detection: Integrating Vision-Language Models with External Knowledge Sources",
    "authors": [
      "Marzieh Adeli Shamsabad",
      "Hamed Ghodrati"
    ],
    "abstract": "Climate disinformation has become a major challenge in today digital world, especially with the rise of misleading images and videos shared widely on social media. These false claims are often convincing and difficult to detect, which can delay actions on climate change. While vision-language models (VLMs) have been used to identify visual disinformation, they rely only on the knowledge available at the time of training. This limits their ability to reason about recent events or updates. The main goal of this paper is to overcome that limitation by combining VLMs with external knowledge. By retrieving up-to-date information such as reverse image results, online fact-checks, and trusted expert content, the system can better assess whether an image and its claim are accurate, misleading, false, or unverifiable. This approach improves the model ability to handle real-world climate disinformation and supports efforts to protect public understanding of science in a rapidly changing information landscape.",
    "arxiv_url": "https://arxiv.org/abs/2601.16108v1",
    "pdf_url": "https://arxiv.org/pdf/2601.16108v1",
    "published_date": "2026-01-22",
    "categories": [
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.16108v1",
      "pdf": "https://arxiv.org/pdf/2601.16108v1"
    },
    "bibtex": ""
  },
  {
    "title": "PhysicsMind: Sim and Real Mechanics Benchmarking for Physical Reasoning and Prediction in Foundational VLMs and World Models",
    "authors": [
      "Chak-Wing Mak",
      "Guanyu Zhu",
      "Boyi Zhang",
      "Hongji Li",
      "Xiaowei Chi",
      "Kevin Zhang",
      "Yichen Wu",
      "Yangfan He",
      "Chun-Kai Fan",
      "Wentao Lu",
      "Kuangzhi Ge",
      "Xinyu Fang",
      "Hongyang He",
      "Kuan Lu",
      "Tianxiang Xu",
      "Li Zhang",
      "Yongxin Ni",
      "Youhua Li",
      "Shanghang Zhang"
    ],
    "abstract": "Modern foundational Multimodal Large Language Models (MLLMs) and video world models have advanced significantly in mathematical, common-sense, and visual reasoning, but their grasp of the underlying physics remains underexplored. Existing benchmarks attempting to measure this matter rely on synthetic, Visual Question Answer templates or focus on perceptual video quality that is tangential to measuring how well the video abides by physical laws. To address this fragmentation, we introduce PhysicsMind, a unified benchmark with both real and simulation environments that evaluates law-consistent reasoning and generation over three canonical principles: Center of Mass, Lever Equilibrium, and Newton's First Law. PhysicsMind comprises two main tasks: i) VQA tasks, testing whether models can reason and determine physical quantities and values from images or short videos, and ii) Video Generation(VG) tasks, evaluating if predicted motion trajectories obey the same center-of-mass, torque, and inertial constraints as the ground truth. A broad range of recent models and video generation models is evaluated on PhysicsMind and found to rely on appearance heuristics while often violating basic mechanics. These gaps indicate that current scaling and training are still insufficient for robust physical understanding, underscoring PhysicsMind as a focused testbed for physics-aware multimodal models. Our data will be released upon acceptance.",
    "arxiv_url": "https://arxiv.org/abs/2601.16007v1",
    "pdf_url": "https://arxiv.org/pdf/2601.16007v1",
    "published_date": "2026-01-22",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "world model",
      "physical",
      "video generation",
      "simulation",
      "physics-aware",
      "benchmark",
      "physics"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.16007v1",
      "pdf": "https://arxiv.org/pdf/2601.16007v1"
    },
    "bibtex": ""
  },
  {
    "title": "Explainable Deepfake Detection with RL Enhanced Self-Blended Images",
    "authors": [
      "Ning Jiang",
      "Dingheng Zeng",
      "Yanhong Liu",
      "Haiyang Yi",
      "Shijie Yu",
      "Minghe Weng",
      "Haifeng Shen",
      "Ying Li"
    ],
    "abstract": "Most prior deepfake detection methods lack explainable outputs. With the growing interest in multimodal large language models (MLLMs), researchers have started exploring their use in interpretable deepfake detection. However, a major obstacle in applying MLLMs to this task is the scarcity of high-quality datasets with detailed forgery attribution annotations, as textual annotation is both costly and challenging - particularly for high-fidelity forged images or videos. Moreover, multiple studies have shown that reinforcement learning (RL) can substantially enhance performance in visual tasks, especially in improving cross-domain generalization. To facilitate the adoption of mainstream MLLM frameworks in deepfake detection with reduced annotation cost, and to investigate the potential of RL in this context, we propose an automated Chain-of-Thought (CoT) data generation framework based on Self-Blended Images, along with an RL-enhanced deepfake detection framework. Extensive experiments validate the effectiveness of our CoT data construction pipeline, tailored reward mechanism, and feedback-driven synthetic data generation approach. Our method achieves performance competitive with state-of-the-art (SOTA) approaches across multiple cross-dataset benchmarks. Implementation details are available at https://github.com/deon1219/rlsbi.",
    "arxiv_url": "https://arxiv.org/abs/2601.15624v1",
    "pdf_url": "https://arxiv.org/pdf/2601.15624v1",
    "published_date": "2026-01-22",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/deon1219/rlsbi",
    "keywords": [
      "benchmark"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.15624v1",
      "pdf": "https://arxiv.org/pdf/2601.15624v1",
      "github": "https://github.com/deon1219/rlsbi"
    },
    "bibtex": ""
  },
  {
    "title": "From Generative Engines to Actionable Simulators: The Imperative of Physical Grounding in World Models",
    "authors": [
      "Zhikang Chen",
      "Tingting Zhu"
    ],
    "abstract": "A world model is an AI system that simulates how an environment evolves under actions, enabling planning through imagined futures rather than reactive perception. Current world models, however, suffer from visual conflation: the mistaken assumption that high-fidelity video generation implies an understanding of physical and causal dynamics. We show that while modern models excel at predicting pixels, they frequently violate invariant constraints, fail under intervention, and break down in safety-critical decision-making. This survey argues that visual realism is an unreliable proxy for world understanding. Instead, effective world models must encode causal structure, respect domain-specific constraints, and remain stable over long horizons. We propose a reframing of world models as actionable simulators rather than visual engines, emphasizing structured 4D interfaces, constraint-aware dynamics, and closed-loop evaluation. Using medical decision-making as an epistemic stress test, where trial-and-error is impossible and errors are irreversible, we demonstrate that a world model's value is determined not by how realistic its rollouts appear, but by its ability to support counterfactual reasoning, intervention planning, and robust long-horizon foresight.",
    "arxiv_url": "https://arxiv.org/abs/2601.15533v1",
    "pdf_url": "https://arxiv.org/pdf/2601.15533v1",
    "published_date": "2026-01-21",
    "categories": [
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "world model",
      "physical",
      "video generation",
      "evaluation",
      "survey",
      "dynamics",
      "medical"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.15533v1",
      "pdf": "https://arxiv.org/pdf/2601.15533v1"
    },
    "bibtex": ""
  },
  {
    "title": "Walk through Paintings: Egocentric World Models from Internet Priors",
    "authors": [
      "Anurag Bagchi",
      "Zhipeng Bao",
      "Homanga Bharadhwaj",
      "Yu-Xiong Wang",
      "Pavel Tokmakov",
      "Martial Hebert"
    ],
    "abstract": "What if a video generation model could not only imagine a plausible future, but the correct one, accurately reflecting how the world changes with each action? We address this question by presenting the Egocentric World Model (EgoWM), a simple, architecture-agnostic method that transforms any pretrained video diffusion model into an action-conditioned world model, enabling controllable future prediction. Rather than training from scratch, we repurpose the rich world priors of Internet-scale video models and inject motor commands through lightweight conditioning layers. This allows the model to follow actions faithfully while preserving realism and strong generalization. Our approach scales naturally across embodiments and action spaces, ranging from 3-DoF mobile robots to 25-DoF humanoids, where predicting egocentric joint-angle-driven dynamics is substantially more challenging. The model produces coherent rollouts for both navigation and manipulation tasks, requiring only modest fine-tuning. To evaluate physical correctness independently of visual appearance, we introduce the Structural Consistency Score (SCS), which measures whether stable scene elements evolve consistently with the provided actions. EgoWM improves SCS by up to 80 percent over prior state-of-the-art navigation world models, while achieving up to six times lower inference latency and robust generalization to unseen environments, including navigation inside paintings.",
    "arxiv_url": "https://arxiv.org/abs/2601.15284v1",
    "pdf_url": "https://arxiv.org/pdf/2601.15284v1",
    "published_date": "2026-01-21",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "controllable",
      "video diffusion",
      "dit",
      "world model",
      "physical",
      "video generation",
      "diffusion model",
      "architecture",
      "dynamics",
      "action-conditioned"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.15284v1",
      "pdf": "https://arxiv.org/pdf/2601.15284v1"
    },
    "bibtex": ""
  },
  {
    "title": "Rethinking Video Generation Model for the Embodied World",
    "authors": [
      "Yufan Deng",
      "Zilin Pan",
      "Hongyu Zhang",
      "Xiaojie Li",
      "Ruoqing Hu",
      "Yufei Ding",
      "Yiming Zou",
      "Yan Zeng",
      "Daquan Zhou"
    ],
    "abstract": "Video generation models have significantly advanced embodied intelligence, unlocking new possibilities for generating diverse robot data that capture perception, reasoning, and action in the physical world. However, synthesizing high-quality videos that accurately reflect real-world robotic interactions remains challenging, and the lack of a standardized benchmark limits fair comparisons and progress. To address this gap, we introduce a comprehensive robotics benchmark, RBench, designed to evaluate robot-oriented video generation across five task domains and four distinct embodiments. It assesses both task-level correctness and visual fidelity through reproducible sub-metrics, including structural consistency, physical plausibility, and action completeness. Evaluation of 25 representative models highlights significant deficiencies in generating physically realistic robot behaviors. Furthermore, the benchmark achieves a Spearman correlation coefficient of 0.96 with human evaluations, validating its effectiveness. While RBench provides the necessary lens to identify these deficiencies, achieving physical realism requires moving beyond evaluation to address the critical shortage of high-quality training data. Driven by these insights, we introduce a refined four-stage data pipeline, resulting in RoVid-X, the largest open-source robotic dataset for video generation with 4 million annotated video clips, covering thousands of tasks and enriched with comprehensive physical property annotations. Collectively, this synergistic ecosystem of evaluation and data establishes a robust foundation for rigorous assessment and scalable training of video models, accelerating the evolution of embodied AI toward general intelligence.",
    "arxiv_url": "https://arxiv.org/abs/2601.15282v1",
    "pdf_url": "https://arxiv.org/pdf/2601.15282v1",
    "published_date": "2026-01-21",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "physical",
      "video generation",
      "efficient",
      "robotics",
      "evaluation",
      "benchmark"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.15282v1",
      "pdf": "https://arxiv.org/pdf/2601.15282v1"
    },
    "bibtex": ""
  },
  {
    "title": "StableWorld: Towards Stable and Consistent Long Interactive Video Generation",
    "authors": [
      "Ying Yang",
      "Zhengyao Lv",
      "Tianlin Pan",
      "Haofan Wang",
      "Binxin Yang",
      "Hubery Yin",
      "Chen Li",
      "Ziwei Liu",
      "Chenyang Si"
    ],
    "abstract": "In this paper, we explore the overlooked challenge of stability and temporal consistency in interactive video generation, which synthesizes dynamic and controllable video worlds through interactive behaviors such as camera movements and text prompts. Despite remarkable progress in world modeling, current methods still suffer from severe instability and temporal degradation, often leading to spatial drift and scene collapse during long-horizon interactions. To better understand this issue, we initially investigate the underlying causes of instability and identify that the major source of error accumulation originates from the same scene, where generated frames gradually deviate from the initial clean state and propagate errors to subsequent frames. Building upon this observation, we propose a simple yet effective method, \\textbf{StableWorld}, a Dynamic Frame Eviction Mechanism. By continuously filtering out degraded frames while retaining geometrically consistent ones, StableWorld effectively prevents cumulative drift at its source, leading to more stable and temporal consistency of interactive generation. Promising results on multiple interactive video models, \\eg, Matrix-Game, Open-Oasis, and Hunyuan-GameCraft, demonstrate that StableWorld is model-agnostic and can be applied to different interactive video generation frameworks to substantially improve stability, temporal consistency, and generalization across diverse interactive scenarios.",
    "arxiv_url": "https://arxiv.org/abs/2601.15281v1",
    "pdf_url": "https://arxiv.org/pdf/2601.15281v1",
    "published_date": "2026-01-21",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "interactive",
      "temporal consistency",
      "controllable",
      "world model",
      "video generation"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.15281v1",
      "pdf": "https://arxiv.org/pdf/2601.15281v1"
    },
    "bibtex": ""
  },
  {
    "title": "ScenDi: 3D-to-2D Scene Diffusion Cascades for Urban Generation",
    "authors": [
      "Hanlei Guo",
      "Jiahao Shao",
      "Xinya Chen",
      "Xiyang Tan",
      "Sheng Miao",
      "Yujun Shen",
      "Yiyi Liao"
    ],
    "abstract": "Recent advancements in 3D object generation using diffusion models have achieved remarkable success, but generating realistic 3D urban scenes remains challenging. Existing methods relying solely on 3D diffusion models tend to suffer a degradation in appearance details, while those utilizing only 2D diffusion models typically compromise camera controllability. To overcome this limitation, we propose ScenDi, a method for urban scene generation that integrates both 3D and 2D diffusion models. We first train a 3D latent diffusion model to generate 3D Gaussians, enabling the rendering of images at a relatively low resolution. To enable controllable synthesis, this 3DGS generation process can be optionally conditioned by specifying inputs such as 3d bounding boxes, road maps, or text prompts. Then, we train a 2D video diffusion model to enhance appearance details conditioned on rendered images from the 3D Gaussians. By leveraging the coarse 3D scene as guidance for 2D video diffusion, ScenDi generates desired scenes based on input conditions and successfully adheres to accurate camera trajectories. Experiments on two challenging real-world datasets, Waymo and KITTI-360, demonstrate the effectiveness of our approach.",
    "arxiv_url": "https://arxiv.org/abs/2601.15221v1",
    "pdf_url": "https://arxiv.org/pdf/2601.15221v1",
    "published_date": "2026-01-21",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "controllable",
      "video diffusion",
      "dit",
      "diffusion model",
      "camera control"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.15221v1",
      "pdf": "https://arxiv.org/pdf/2601.15221v1"
    },
    "bibtex": ""
  },
  {
    "title": "LaVR: Scene Latent Conditioned Generative Video Trajectory Re-Rendering using Large 4D Reconstruction Models",
    "authors": [
      "Mingyang Xie",
      "Numair Khan",
      "Tianfu Wang",
      "Naina Dhingra",
      "Seonghyeon Nam",
      "Haitao Yang",
      "Zhuo Hui",
      "Christopher Metzler",
      "Andrea Vedaldi",
      "Hamed Pirsiavash",
      "Lei Luo"
    ],
    "abstract": "Given a monocular video, the goal of video re-rendering is to generate views of the scene from a novel camera trajectory. Existing methods face two distinct challenges. Geometrically unconditioned models lack spatial awareness, leading to drift and deformation under viewpoint changes. On the other hand, geometrically-conditioned models depend on estimated depth and explicit reconstruction, making them susceptible to depth inaccuracies and calibration errors.   We propose to address these challenges by using the implicit geometric knowledge embedded in the latent space of a large 4D reconstruction model to condition the video generation process. These latents capture scene structure in a continuous space without explicit reconstruction. Therefore, they provide a flexible representation that allows the pretrained diffusion prior to regularize errors more effectively. By jointly conditioning on these latents and source camera poses, we demonstrate that our model achieves state-of-the-art results on the video re-rendering task. Project webpage is https://lavr-4d-scene-rerender.github.io/",
    "arxiv_url": "https://arxiv.org/abs/2601.14674v1",
    "pdf_url": "https://arxiv.org/pdf/2601.14674v1",
    "published_date": "2026-01-21",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "trajectory",
      "video generation",
      "dit"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.14674v1",
      "pdf": "https://arxiv.org/pdf/2601.14674v1",
      "project": "https://lavr-4d-scene-rerender.github.io"
    },
    "bibtex": ""
  },
  {
    "title": "VideoMaMa: Mask-Guided Video Matting via Generative Prior",
    "authors": [
      "Sangbeom Lim",
      "Seoung Wug Oh",
      "Jiahui Huang",
      "Heeji Yoon",
      "Seungryong Kim",
      "Joon-Young Lee"
    ],
    "abstract": "Generalizing video matting models to real-world videos remains a significant challenge due to the scarcity of labeled data. To address this, we present Video Mask-to-Matte Model (VideoMaMa) that converts coarse segmentation masks into pixel accurate alpha mattes, by leveraging pretrained video diffusion models. VideoMaMa demonstrates strong zero-shot generalization to real-world footage, even though it is trained solely on synthetic data. Building on this capability, we develop a scalable pseudo-labeling pipeline for large-scale video matting and construct the Matting Anything in Video (MA-V) dataset, which offers high-quality matting annotations for more than 50K real-world videos spanning diverse scenes and motions. To validate the effectiveness of this dataset, we fine-tune the SAM2 model on MA-V to obtain SAM2-Matte, which outperforms the same model trained on existing matting datasets in terms of robustness on in-the-wild videos. These findings emphasize the importance of large-scale pseudo-labeled video matting and showcase how generative priors and accessible segmentation cues can drive scalable progress in video matting research.",
    "arxiv_url": "https://arxiv.org/abs/2601.14255v1",
    "pdf_url": "https://arxiv.org/pdf/2601.14255v1",
    "published_date": "2026-01-20",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "diffusion model",
      "video diffusion"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.14255v1",
      "pdf": "https://arxiv.org/pdf/2601.14255v1"
    },
    "bibtex": ""
  },
  {
    "title": "OmniTransfer: All-in-one Framework for Spatio-temporal Video Transfer",
    "authors": [
      "Pengze Zhang",
      "Yanze Wu",
      "Mengtian Li",
      "Xu Bai",
      "Songtao Zhao",
      "Fulong Ye",
      "Chong Mou",
      "Xinghui Li",
      "Zhuowei Chen",
      "Qian He",
      "Mingyuan Gao"
    ],
    "abstract": "Videos convey richer information than images or text, capturing both spatial and temporal dynamics. However, most existing video customization methods rely on reference images or task-specific temporal priors, failing to fully exploit the rich spatio-temporal information inherent in videos, thereby limiting flexibility and generalization in video generation. To address these limitations, we propose OmniTransfer, a unified framework for spatio-temporal video transfer. It leverages multi-view information across frames to enhance appearance consistency and exploits temporal cues to enable fine-grained temporal control. To unify various video transfer tasks, OmniTransfer incorporates three key designs: Task-aware Positional Bias that adaptively leverages reference video information to improve temporal alignment or appearance consistency; Reference-decoupled Causal Learning separating reference and target branches to enable precise reference transfer while improving efficiency; and Task-adaptive Multimodal Alignment using multimodal semantic guidance to dynamically distinguish and tackle different tasks. Extensive experiments show that OmniTransfer outperforms existing methods in appearance (ID and style) and temporal transfer (camera movement and video effects), while matching pose-guided methods in motion transfer without using pose, establishing a new paradigm for flexible, high-fidelity video generation.",
    "arxiv_url": "https://arxiv.org/abs/2601.14250v1",
    "pdf_url": "https://arxiv.org/pdf/2601.14250v1",
    "published_date": "2026-01-20",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "video generation",
      "style",
      "dynamics",
      "pose-guided",
      "customization"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.14250v1",
      "pdf": "https://arxiv.org/pdf/2601.14250v1"
    },
    "bibtex": ""
  },
  {
    "title": "LAViG-FLOW: Latent Autoregressive Video Generation for Fluid Flow Simulations",
    "authors": [
      "Vittoria De Pellegrini",
      "Tariq Alkhalifah"
    ],
    "abstract": "Modeling and forecasting subsurface multiphase fluid flow fields underpin applications ranging from geological CO2 sequestration (GCS) operations to geothermal production. This is essential for ensuring both operational performance and long-term safety. While high fidelity multiphase simulators are widely used for this purpose, they become prohibitively expensive once many forward runs are required for inversion purposes and quantify uncertainty. To tackle this challenge we propose LAViG-FLOW, a latent autoregressive video generation diffusion framework that explicitly learns the coupled evolution of saturation and pressure fields. Each state variable is compressed by a dedicated 2D autoencoder, and a Video Diffusion Transformer (VDiT) models their coupled distribution across time. We first train the model on a given time horizon to learn their coupled relationship and then fine-tune it autoregressively so it can extrapolate beyond the observed time window. Evaluated on an open-source CO2 sequestration dataset, LAViG-FLOW generates saturation and pressure fields that stay consistent across time while running orders of magnitude faster than traditional numerical solvers.",
    "arxiv_url": "https://arxiv.org/abs/2601.13190v1",
    "pdf_url": "https://arxiv.org/pdf/2601.13190v1",
    "published_date": "2026-01-19",
    "categories": [
      "cs.LG",
      "physics.flu-dyn"
    ],
    "github_url": "",
    "keywords": [
      "video diffusion",
      "dit",
      "diffusion transformer",
      "video generation",
      "simulation",
      "autoregressive"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.13190v1",
      "pdf": "https://arxiv.org/pdf/2601.13190v1"
    },
    "bibtex": ""
  },
  {
    "title": "Think3D: Thinking with Space for Spatial Reasoning",
    "authors": [
      "Zaibin Zhang",
      "Yuhan Wu",
      "Lianjie Jia",
      "Yifan Wang",
      "Zhongbo Zhang",
      "Yijiang Li",
      "Binghao Ran",
      "Fuxi Zhang",
      "Zhuohan Sun",
      "Zhenfei Yin",
      "Lijun Wang",
      "Huchuan Lu"
    ],
    "abstract": "Understanding and reasoning about the physical world requires spatial intelligence: the ability to interpret geometry, perspective, and spatial relations beyond 2D perception. While recent vision large models (VLMs) excel at visual understanding, they remain fundamentally 2D perceivers and struggle with genuine 3D reasoning. We introduce Think3D, a framework that enables VLM agents to think with 3D space. By leveraging 3D reconstruction models that recover point clouds and camera poses from images or videos, Think3D allows the agent to actively manipulate space through camera-based operations and ego/global-view switching, transforming spatial reasoning into an interactive 3D chain-of-thought process. Without additional training, Think3D significantly improves the spatial reasoning performance of advanced models such as GPT-4.1 and Gemini 2.5 Pro, yielding average gains of +7.8% on BLINK Multi-view and MindCube, and +4.7% on VSI-Bench. We further show that smaller models, which struggle with spatial exploration, benefit significantly from a reinforcement learning policy that enables the model to select informative viewpoints and operations. With RL, the benefit from tool usage increases from +0.7% to +6.8%. Our findings demonstrate that training-free, tool-augmented spatial exploration is a viable path toward more flexible and human-like 3D reasoning in multimodal agents, establishing a new dimension of multimodal intelligence. Code and weights are released at https://github.com/zhangzaibin/spagent.",
    "arxiv_url": "https://arxiv.org/abs/2601.13029v2",
    "pdf_url": "https://arxiv.org/pdf/2601.13029v2",
    "published_date": "2026-01-19",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/zhangzaibin/spagent",
    "keywords": [
      "interactive",
      "physical",
      "dit"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.13029v2",
      "pdf": "https://arxiv.org/pdf/2601.13029v2",
      "github": "https://github.com/zhangzaibin/spagent"
    },
    "bibtex": ""
  },
  {
    "title": "Exploring Talking Head Models With Adjacent Frame Prior for Speech-Preserving Facial Expression Manipulation",
    "authors": [
      "Zhenxuan Lu",
      "Zhihua Xu",
      "Zhijing Yang",
      "Feng Gao",
      "Yongyi Lu",
      "Keze Wang",
      "Tianshui Chen"
    ],
    "abstract": "Speech-Preserving Facial Expression Manipulation (SPFEM) is an innovative technique aimed at altering facial expressions in images and videos while retaining the original mouth movements. Despite advancements, SPFEM still struggles with accurate lip synchronization due to the complex interplay between facial expressions and mouth shapes. Capitalizing on the advanced capabilities of audio-driven talking head generation (AD-THG) models in synthesizing precise lip movements, our research introduces a novel integration of these models with SPFEM. We present a new framework, Talking Head Facial Expression Manipulation (THFEM), which utilizes AD-THG models to generate frames with accurately synchronized lip movements from audio inputs and SPFEM-altered images. However, increasing the number of frames generated by AD-THG models tends to compromise the realism and expression fidelity of the images. To counter this, we develop an adjacent frame learning strategy that finetunes AD-THG models to predict sequences of consecutive frames. This strategy enables the models to incorporate information from neighboring frames, significantly improving image quality during testing. Our extensive experimental evaluations demonstrate that this framework effectively preserves mouth shapes during expression manipulations, highlighting the substantial benefits of integrating AD-THG with SPFEM.",
    "arxiv_url": "https://arxiv.org/abs/2601.12876v1",
    "pdf_url": "https://arxiv.org/pdf/2601.12876v1",
    "published_date": "2026-01-19",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "evaluation",
      "talking head",
      "audio-driven"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.12876v1",
      "pdf": "https://arxiv.org/pdf/2601.12876v1"
    },
    "bibtex": ""
  },
  {
    "title": "Moaw: Unleashing Motion Awareness for Video Diffusion Models",
    "authors": [
      "Tianqi Zhang",
      "Ziyi Wang",
      "Wenzhao Zheng",
      "Weiliang Chen",
      "Yuanhui Huang",
      "Zhengyang Huang",
      "Jie Zhou",
      "Jiwen Lu"
    ],
    "abstract": "Video diffusion models, trained on large-scale datasets, naturally capture correspondences of shared features across frames. Recent works have exploited this property for tasks such as optical flow prediction and tracking in a zero-shot setting. Motivated by these findings, we investigate whether supervised training can more fully harness the tracking capability of video diffusion models. To this end, we propose Moaw, a framework that unleashes motion awareness for video diffusion models and leverages it to facilitate motion transfer. Specifically, we train a diffusion model for motion perception, shifting its modality from image-to-video generation to video-to-dense-tracking. We then construct a motion-labeled dataset to identify features that encode the strongest motion information, and inject them into a structurally identical video generation model. Owing to the homogeneity between the two networks, these features can be naturally adapted in a zero-shot manner, enabling motion transfer without additional adapters. Our work provides a new paradigm for bridging generative modeling and motion understanding, paving the way for more unified and controllable video learning frameworks.",
    "arxiv_url": "https://arxiv.org/abs/2601.12761v1",
    "pdf_url": "https://arxiv.org/pdf/2601.12761v1",
    "published_date": "2026-01-19",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "controllable",
      "video diffusion",
      "dit",
      "video generation",
      "image-to-video",
      "diffusion model"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.12761v1",
      "pdf": "https://arxiv.org/pdf/2601.12761v1"
    },
    "bibtex": ""
  },
  {
    "title": "S2DiT: Sandwich Diffusion Transformer for Mobile Streaming Video Generation",
    "authors": [
      "Lin Zhao",
      "Yushu Wu",
      "Aleksei Lebedev",
      "Dishani Lahiri",
      "Meng Dong",
      "Arpit Sahni",
      "Michael Vasilkovsky",
      "Hao Chen",
      "Ju Hu",
      "Aliaksandr Siarohin",
      "Sergey Tulyakov",
      "Yanzhi Wang",
      "Anil Kag",
      "Yanyu Li"
    ],
    "abstract": "Diffusion Transformers (DiTs) have recently improved video generation quality. However, their heavy computational cost makes real-time or on-device generation infeasible. In this work, we introduce S2DiT, a Streaming Sandwich Diffusion Transformer designed for efficient, high-fidelity, and streaming video generation on mobile hardware. S2DiT generates more tokens but maintains efficiency with novel efficient attentions: a mixture of LinConv Hybrid Attention (LCHA) and Stride Self-Attention (SSA). Based on this, we uncover the sandwich design via a budget-aware dynamic programming search, achieving superior quality and efficiency. We further propose a 2-in-1 distillation framework that transfers the capacity of large teacher models (e.g., Wan 2.2-14B) to the compact few-step sandwich model. Together, S2DiT achieves quality on par with state-of-the-art server video models, while streaming at over 10 FPS on an iPhone.",
    "arxiv_url": "https://arxiv.org/abs/2601.12719v1",
    "pdf_url": "https://arxiv.org/pdf/2601.12719v1",
    "published_date": "2026-01-19",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "distillation",
      "dit",
      "streaming",
      "diffusion transformer",
      "video generation",
      "efficient"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.12719v1",
      "pdf": "https://arxiv.org/pdf/2601.12719v1"
    },
    "bibtex": ""
  },
  {
    "title": "Enhancing Vision Language Models with Logic Reasoning for Situational Awareness",
    "authors": [
      "Pavana Pradeep",
      "Krishna Kant",
      "Suya Yu"
    ],
    "abstract": "Vision-Language Models (VLMs) offer the ability to generate high-level, interpretable descriptions of complex activities from images and videos, making them valuable for situational awareness (SA) applications. In such settings, the focus is on identifying infrequent but significant events with high reliability and accuracy, while also extracting fine-grained details and assessing recognition quality. In this paper, we propose an approach that integrates VLMs with traditional computer vision methods through explicit logic reasoning to enhance SA in three key ways: (a) extracting fine-grained event details, (b) employing an intelligent fine-tuning (FT) strategy that achieves substantially higher accuracy than uninformed selection, and (c) generating justifications for VLM outputs during inference. We demonstrate that our intelligent FT mechanism improves the accuracy and provides a valuable means, during inferencing, to either confirm the validity of the VLM output or indicate why it may be questionable.",
    "arxiv_url": "https://arxiv.org/abs/2601.11322v1",
    "pdf_url": "https://arxiv.org/pdf/2601.11322v1",
    "published_date": "2026-01-16",
    "categories": [
      "cs.CV",
      "cs.LO"
    ],
    "github_url": "",
    "keywords": [
      "dit"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.11322v1",
      "pdf": "https://arxiv.org/pdf/2601.11322v1"
    },
    "bibtex": ""
  },
  {
    "title": "AI-based System for Transforming text and sound to Educational Videos",
    "authors": [
      "M. E. ElAlami",
      "S. M. Khater",
      "M. El. R. Rehan"
    ],
    "abstract": "Technological developments have produced methods that can generate educational videos from input text or sound. Recently, the use of deep learning techniques for image and video generation has been widely explored, particularly in education. However, generating video content from conditional inputs such as text or speech remains a challenging area. In this paper, we introduce a novel method to the educational structure, Generative Adversarial Network (GAN), which develop frame-for-frame frameworks and are able to create full educational videos. The proposed system is structured into three main phases In the first phase, the input (either text or speech) is transcribed using speech recognition. In the second phase, key terms are extracted and relevant images are generated using advanced models such as CLIP and diffusion models to enhance visual quality and semantic alignment. In the final phase, the generated images are synthesized into a video format, integrated with either pre-recorded or synthesized sound, resulting in a fully interactive educational video. The proposed system is compared with other systems such as TGAN, MoCoGAN, and TGANS-C, achieving a Frchet Inception Distance (FID) score of 28.75%, which indicates improved visual quality and better over existing methods.",
    "arxiv_url": "https://arxiv.org/abs/2601.17022v1",
    "pdf_url": "https://arxiv.org/pdf/2601.17022v1",
    "published_date": "2026-01-16",
    "categories": [
      "cs.MM",
      "cs.AI",
      "cs.CV",
      "cs.CY"
    ],
    "github_url": "",
    "keywords": [
      "interactive",
      "education",
      "dit",
      "video generation",
      "diffusion model",
      "sound"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.17022v1",
      "pdf": "https://arxiv.org/pdf/2601.17022v1"
    },
    "bibtex": ""
  },
  {
    "title": "VidLeaks: Membership Inference Attacks Against Text-to-Video Models",
    "authors": [
      "Li Wang",
      "Wenyu Chen",
      "Ning Yu",
      "Zheng Li",
      "Shanqing Guo"
    ],
    "abstract": "The proliferation of powerful Text-to-Video (T2V) models, trained on massive web-scale datasets, raises urgent concerns about copyright and privacy violations. Membership inference attacks (MIAs) provide a principled tool for auditing such risks, yet existing techniques - designed for static data like images or text - fail to capture the spatio-temporal complexities of video generation. In particular, they overlook the sparsity of memorization signals in keyframes and the instability introduced by stochastic temporal dynamics. In this paper, we conduct the first systematic study of MIAs against T2V models and introduce a novel framework VidLeaks, which probes sparse-temporal memorization through two complementary signals: 1) Spatial Reconstruction Fidelity (SRF), using a Top-K similarity to amplify spatial memorization signals from sparsely memorized keyframes, and 2) Temporal Generative Stability (TGS), which measures semantic consistency across multiple queries to capture temporal leakage. We evaluate VidLeaks under three progressively restrictive black-box settings - supervised, reference-based, and query-only. Experiments on three representative T2V models reveal severe vulnerabilities: VidLeaks achieves AUC of 82.92% on AnimateDiff and 97.01% on InstructVideo even in the strict query-only setting, posing a realistic and exploitable privacy risk. Our work provides the first concrete evidence that T2V models leak substantial membership information through both sparse and temporal memorization, establishing a foundation for auditing video generation systems and motivating the development of new defenses. Code is available at: https://zenodo.org/records/17972831.",
    "arxiv_url": "https://arxiv.org/abs/2601.11210v1",
    "pdf_url": "https://arxiv.org/pdf/2601.11210v1",
    "published_date": "2026-01-16",
    "categories": [
      "cs.CR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "dit",
      "text-to-video",
      "video generation",
      "dynamics",
      "t2v"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.11210v1",
      "pdf": "https://arxiv.org/pdf/2601.11210v1",
      "project": "https://zenodo.org/records/17972831"
    },
    "bibtex": ""
  },
  {
    "title": "ATATA: One Algorithm to Align Them All",
    "authors": [
      "Boyi Pang",
      "Savva Ignatyev",
      "Vladimir Ippolitov",
      "Ramil Khafizov",
      "Yurii Melnik",
      "Oleg Voynov",
      "Maksim Nakhodnov",
      "Aibek Alanov",
      "Xiaopeng Fan",
      "Peter Wonka",
      "Evgeny Burnaev"
    ],
    "abstract": "We suggest a new multi-modal algorithm for joint inference of paired structurally aligned samples with Rectified Flow models. While some existing methods propose a codependent generation process, they do not view the problem of joint generation from a structural alignment perspective. Recent work uses Score Distillation Sampling to generate aligned 3D models, but SDS is known to be time-consuming, prone to mode collapse, and often provides cartoonish results. By contrast, our suggested approach relies on the joint transport of a segment in the sample space, yielding faster computation at inference time. Our approach can be built on top of an arbitrary Rectified Flow model operating on the structured latent space. We show the applicability of our method to the domains of image, video, and 3D shape generation using state-of-the-art baselines and evaluate it against both editing-based and joint inference-based competing approaches. We demonstrate a high degree of structural alignment for the sample pairs obtained with our method and a high visual quality of the samples. Our method improves the state-of-the-art for image and video generation pipelines. For 3D generation, it is able to show comparable quality while working orders of magnitude faster.",
    "arxiv_url": "https://arxiv.org/abs/2601.11194v1",
    "pdf_url": "https://arxiv.org/pdf/2601.11194v1",
    "published_date": "2026-01-16",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "multi-modal",
      "rectified flow",
      "distillation",
      "dit",
      "video generation"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.11194v1",
      "pdf": "https://arxiv.org/pdf/2601.11194v1"
    },
    "bibtex": ""
  },
  {
    "title": "PhysRVG: Physics-Aware Unified Reinforcement Learning for Video Generative Models",
    "authors": [
      "Qiyuan Zhang",
      "Biao Gong",
      "Shuai Tan",
      "Zheng Zhang",
      "Yujun Shen",
      "Xing Zhu",
      "Yuyuan Li",
      "Kelu Yao",
      "Chunhua Shen",
      "Changqing Zou"
    ],
    "abstract": "Physical principles are fundamental to realistic visual simulation, but remain a significant oversight in transformer-based video generation. This gap highlights a critical limitation in rendering rigid body motion, a core tenet of classical mechanics. While computer graphics and physics-based simulators can easily model such collisions using Newton formulas, modern pretrain-finetune paradigms discard the concept of object rigidity during pixel-level global denoising. Even perfectly correct mathematical constraints are treated as suboptimal solutions (i.e., conditions) during model optimization in post-training, fundamentally limiting the physical realism of generated videos. Motivated by these considerations, we introduce, for the first time, a physics-aware reinforcement learning paradigm for video generation models that enforces physical collision rules directly in high-dimensional spaces, ensuring the physics knowledge is strictly applied rather than treated as conditions. Subsequently, we extend this paradigm to a unified framework, termed Mimicry-Discovery Cycle (MDcycle), which allows substantial fine-tuning while fully preserving the model's ability to leverage physics-grounded feedback. To validate our approach, we construct new benchmark PhysRVGBench and perform extensive qualitative and quantitative experiments to thoroughly assess its effectiveness.",
    "arxiv_url": "https://arxiv.org/abs/2601.11087v1",
    "pdf_url": "https://arxiv.org/pdf/2601.11087v1",
    "published_date": "2026-01-16",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "dit",
      "physical",
      "video generation",
      "concept",
      "simulation",
      "denoising",
      "physics-aware",
      "benchmark",
      "physics",
      "body motion"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.11087v1",
      "pdf": "https://arxiv.org/pdf/2601.11087v1"
    },
    "bibtex": ""
  },
  {
    "title": "Future Optical Flow Prediction Improves Robot Control & Video Generation",
    "authors": [
      "Kanchana Ranasinghe",
      "Honglu Zhou",
      "Yu Fang",
      "Luyu Yang",
      "Le Xue",
      "Ran Xu",
      "Caiming Xiong",
      "Silvio Savarese",
      "Michael S Ryoo",
      "Juan Carlos Niebles"
    ],
    "abstract": "Future motion representations, such as optical flow, offer immense value for control and generative tasks. However, forecasting generalizable spatially dense motion representations remains a key challenge, and learning such forecasting from noisy, real-world data remains relatively unexplored. We introduce FOFPred, a novel language-conditioned optical flow forecasting model featuring a unified Vision-Language Model (VLM) and Diffusion architecture. This unique combination enables strong multimodal reasoning with pixel-level generative fidelity for future motion prediction. Our model is trained on web-scale human activity data-a highly scalable but unstructured source. To extract meaningful signals from this noisy video-caption data, we employ crucial data preprocessing techniques and our unified architecture with strong image pretraining. The resulting trained model is then extended to tackle two distinct downstream tasks in control and generation. Evaluations across robotic manipulation and video generation under language-driven settings establish the cross-domain versatility of FOFPred, confirming the value of a unified VLM-Diffusion architecture and scalable learning from diverse web data for future optical flow prediction.",
    "arxiv_url": "https://arxiv.org/abs/2601.10781v1",
    "pdf_url": "https://arxiv.org/pdf/2601.10781v1",
    "published_date": "2026-01-15",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "evaluation",
      "architecture",
      "video generation",
      "dit"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.10781v1",
      "pdf": "https://arxiv.org/pdf/2601.10781v1"
    },
    "bibtex": ""
  },
  {
    "title": "Human detectors are surprisingly powerful reward models",
    "authors": [
      "Kumar Ashutosh",
      "XuDong Wang",
      "Xi Yin",
      "Kristen Grauman",
      "Adam Polyak",
      "Ishan Misra",
      "Rohit Girdhar"
    ],
    "abstract": "Video generation models have recently achieved impressive visual fidelity and temporal coherence. Yet, they continue to struggle with complex, non-rigid motions, especially when synthesizing humans performing dynamic actions such as sports, dance, etc. Generated videos often exhibit missing or extra limbs, distorted poses, or physically implausible actions. In this work, we propose a remarkably simple reward model, HuDA, to quantify and improve the human motion in generated videos. HuDA integrates human detection confidence for appearance quality, and a temporal prompt alignment score to capture motion realism. We show this simple reward function that leverages off-the-shelf models without any additional training, outperforms specialized models finetuned with manually annotated data. Using HuDA for Group Reward Policy Optimization (GRPO) post-training of video models, we significantly enhance video generation, especially when generating complex human motions, outperforming state-of-the-art models like Wan 2.1, with win-rate of 73%. Finally, we demonstrate that HuDA improves generation quality beyond just humans, for instance, significantly improving generation of animal videos and human-object interactions.",
    "arxiv_url": "https://arxiv.org/abs/2601.14037v2",
    "pdf_url": "https://arxiv.org/pdf/2601.14037v2",
    "published_date": "2026-01-15",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "human motion",
      "physical",
      "video generation",
      "dit"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.14037v2",
      "pdf": "https://arxiv.org/pdf/2601.14037v2"
    },
    "bibtex": ""
  },
  {
    "title": "CoMoVi: Co-Generation of 3D Human Motions and Realistic Videos",
    "authors": [
      "Chengfeng Zhao",
      "Jiazhi Shu",
      "Yubo Zhao",
      "Tianyu Huang",
      "Jiahao Lu",
      "Zekai Gu",
      "Chengwei Ren",
      "Zhiyang Dou",
      "Qing Shuai",
      "Yuan Liu"
    ],
    "abstract": "In this paper, we find that the generation of 3D human motions and 2D human videos is intrinsically coupled. 3D motions provide the structural prior for plausibility and consistency in videos, while pre-trained video models offer strong generalization capabilities for motions, which necessitate coupling their generation processes. Based on this, we present CoMoVi, a co-generative framework that couples two video diffusion models (VDMs) to generate 3D human motions and videos synchronously within a single diffusion denoising loop. To achieve this, we first propose an effective 2D human motion representation that can inherit the powerful prior of pre-trained VDMs. Then, we design a dual-branch diffusion model to couple human motion and video generation process with mutual feature interaction and 3D-2D cross attentions. Moreover, we curate CoMoVi Dataset, a large-scale real-world human video dataset with text and motion annotations, covering diverse and challenging human motions. Extensive experiments demonstrate the effectiveness of our method in both 3D human motion and video generation tasks.",
    "arxiv_url": "https://arxiv.org/abs/2601.10632v1",
    "pdf_url": "https://arxiv.org/pdf/2601.10632v1",
    "published_date": "2026-01-15",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "video diffusion",
      "video generation",
      "diffusion model",
      "human motion",
      "denoising"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.10632v1",
      "pdf": "https://arxiv.org/pdf/2601.10632v1"
    },
    "bibtex": ""
  },
  {
    "title": "Molmo2: Open Weights and Data for Vision-Language Models with Video Understanding and Grounding",
    "authors": [
      "Christopher Clark",
      "Jieyu Zhang",
      "Zixian Ma",
      "Jae Sung Park",
      "Mohammadreza Salehi",
      "Rohun Tripathi",
      "Sangho Lee",
      "Zhongzheng Ren",
      "Chris Dongjoo Kim",
      "Yinuo Yang",
      "Vincent Shao",
      "Yue Yang",
      "Weikai Huang",
      "Ziqi Gao",
      "Taira Anderson",
      "Jianrui Zhang",
      "Jitesh Jain",
      "George Stoica",
      "Winson Han",
      "Ali Farhadi",
      "Ranjay Krishna"
    ],
    "abstract": "Today's strongest video-language models (VLMs) remain proprietary. The strongest open-weight models either rely on synthetic data from proprietary VLMs, effectively distilling from them, or do not disclose their training data or recipe. As a result, the open-source community lacks the foundations needed to improve on the state-of-the-art video (and image) language models. Crucially, many downstream applications require more than just high-level video understanding; they require grounding -- either by pointing or by tracking in pixels. Even proprietary models lack this capability. We present Molmo2, a new family of VLMs that are state-of-the-art among open-source models and demonstrate exceptional new capabilities in point-driven grounding in single image, multi-image, and video tasks. Our key contribution is a collection of 7 new video datasets and 2 multi-image datasets, including a dataset of highly detailed video captions for pre-training, a free-form video Q&A dataset for fine-tuning, a new object tracking dataset with complex queries, and an innovative new video pointing dataset, all collected without the use of closed VLMs. We also present a training recipe for this data utilizing an efficient packing and message-tree encoding scheme, and show bi-directional attention on vision tokens and a novel token-weight strategy improves performance. Our best-in-class 8B model outperforms others in the class of open weight and data models on short videos, counting, and captioning, and is competitive on long-videos. On video-grounding Molmo2 significantly outperforms existing open-weight models like Qwen3-VL (35.5 vs 29.6 accuracy on video counting) and surpasses proprietary models like Gemini 3 Pro on some tasks (38.4 vs 20.0 F1 on video pointing and 56.2 vs 41.1 J&F on video tracking).",
    "arxiv_url": "https://arxiv.org/abs/2601.10611v1",
    "pdf_url": "https://arxiv.org/pdf/2601.10611v1",
    "published_date": "2026-01-15",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "efficient"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.10611v1",
      "pdf": "https://arxiv.org/pdf/2601.10611v1"
    },
    "bibtex": ""
  },
  {
    "title": "Inference-time Physics Alignment of Video Generative Models with Latent World Models",
    "authors": [
      "Jianhao Yuan",
      "Xiaofeng Zhang",
      "Felix Friedrich",
      "Nicolas Beltran-Velez",
      "Melissa Hall",
      "Reyhane Askari-Hemmat",
      "Xiaochuang Han",
      "Nicolas Ballas",
      "Michal Drozdzal",
      "Adriana Romero-Soriano"
    ],
    "abstract": "State-of-the-art video generative models produce promising visual content yet often violate basic physics principles, limiting their utility. While some attribute this deficiency to insufficient physics understanding from pre-training, we find that the shortfall in physics plausibility also stems from suboptimal inference strategies. We therefore introduce WMReward and treat improving physics plausibility of video generation as an inference-time alignment problem. In particular, we leverage the strong physics prior of a latent world model (here, VJEPA-2) as a reward to search and steer multiple candidate denoising trajectories, enabling scaling test-time compute for better generation performance. Empirically, our approach substantially improves physics plausibility across image-conditioned, multiframe-conditioned, and text-conditioned generation settings, with validation from human preference study. Notably, in the ICCV 2025 Perception Test PhysicsIQ Challenge, we achieve a final score of 62.64%, winning first place and outperforming the previous state of the art by 7.42%. Our work demonstrates the viability of using latent world models to improve physics plausibility of video generation, beyond this specific instantiation or parameterization.",
    "arxiv_url": "https://arxiv.org/abs/2601.10553v1",
    "pdf_url": "https://arxiv.org/pdf/2601.10553v1",
    "published_date": "2026-01-15",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "dit",
      "world model",
      "video generation",
      "denoising",
      "physics"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.10553v1",
      "pdf": "https://arxiv.org/pdf/2601.10553v1"
    },
    "bibtex": ""
  },
  {
    "title": "Beyond Inpainting: Unleash 3D Understanding for Precise Camera-Controlled Video Generation",
    "authors": [
      "Dong-Yu Chen",
      "Yixin Guo",
      "Shuojin Yang",
      "Tai-Jiang Mu",
      "Shi-Min Hu"
    ],
    "abstract": "Camera control has been extensively studied in conditioned video generation; however, performing precisely altering the camera trajectories while faithfully preserving the video content remains a challenging task. The mainstream approach to achieving precise camera control is warping a 3D representation according to the target trajectory. However, such methods fail to fully leverage the 3D priors of video diffusion models (VDMs) and often fall into the Inpainting Trap, resulting in subject inconsistency and degraded generation quality. To address this problem, we propose DepthDirector, a video re-rendering framework with precise camera controllability. By leveraging the depth video from explicit 3D representation as camera-control guidance, our method can faithfully reproduce the dynamic scene of an input video under novel camera trajectories. Specifically, we design a View-Content Dual-Stream Condition mechanism that injects both the source video and the warped depth sequence rendered under the target viewpoint into the pretrained video generation model. This geometric guidance signal enables VDMs to comprehend camera movements and leverage their 3D understanding capabilities, thereby facilitating precise camera control and consistent content generation. Next, we introduce a lightweight LoRA-based video diffusion adapter to train our framework, fully preserving the knowledge priors of VDMs. Additionally, we construct a large-scale multi-camera synchronized dataset named MultiCam-WarpData using Unreal Engine 5, containing 8K videos across 1K dynamic scenes. Extensive experiments show that DepthDirector outperforms existing methods in both camera controllability and visual quality. Our code and dataset will be publicly available.",
    "arxiv_url": "https://arxiv.org/abs/2601.10214v2",
    "pdf_url": "https://arxiv.org/pdf/2601.10214v2",
    "published_date": "2026-01-15",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "video diffusion",
      "dit",
      "trajectory",
      "video generation",
      "diffusion model",
      "camera control"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.10214v2",
      "pdf": "https://arxiv.org/pdf/2601.10214v2"
    },
    "bibtex": ""
  },
  {
    "title": "FlowAct-R1: Towards Interactive Humanoid Video Generation",
    "authors": [
      "Lizhen Wang",
      "Yongming Zhu",
      "Zhipeng Ge",
      "Youwei Zheng",
      "Longhao Zhang",
      "Tianshu Hu",
      "Shiyang Qin",
      "Mingshuang Luo",
      "Jiaxu Zhang",
      "Xin Chen",
      "Yulong Wang",
      "Zerong Zheng",
      "Jianwen Jiang",
      "Chao Liang",
      "Weifeng Chen",
      "Xing Wang",
      "Yuan Zhang",
      "Mingyuan Gao"
    ],
    "abstract": "Interactive humanoid video generation aims to synthesize lifelike visual agents that can engage with humans through continuous and responsive video. Despite recent advances in video synthesis, existing methods often grapple with the trade-off between high-fidelity synthesis and real-time interaction requirements. In this paper, we propose FlowAct-R1, a framework specifically designed for real-time interactive humanoid video generation. Built upon a MMDiT architecture, FlowAct-R1 enables the streaming synthesis of video with arbitrary durations while maintaining low-latency responsiveness. We introduce a chunkwise diffusion forcing strategy, complemented by a novel self-forcing variant, to alleviate error accumulation and ensure long-term temporal consistency during continuous interaction. By leveraging efficient distillation and system-level optimizations, our framework achieves a stable 25fps at 480p resolution with a time-to-first-frame (TTFF) of only around 1.5 seconds. The proposed method provides holistic and fine-grained full-body control, enabling the agent to transition naturally between diverse behavioral states in interactive scenarios. Experimental results demonstrate that FlowAct-R1 achieves exceptional behavioral vividness and perceptual realism, while maintaining robust generalization across diverse character styles.",
    "arxiv_url": "https://arxiv.org/abs/2601.10103v1",
    "pdf_url": "https://arxiv.org/pdf/2601.10103v1",
    "published_date": "2026-01-15",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "interactive",
      "temporal consistency",
      "distillation",
      "streaming",
      "dit",
      "video generation",
      "style",
      "video synthesis",
      "architecture",
      "efficient"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.10103v1",
      "pdf": "https://arxiv.org/pdf/2601.10103v1"
    },
    "bibtex": ""
  },
  {
    "title": "CoF-T2I: Video Models as Pure Visual Reasoners for Text-to-Image Generation",
    "authors": [
      "Chengzhuo Tong",
      "Mingkun Chang",
      "Shenglong Zhang",
      "Yuran Wang",
      "Cheng Liang",
      "Zhizheng Zhao",
      "Ruichuan An",
      "Bohan Zeng",
      "Yang Shi",
      "Yifan Dai",
      "Ziming Zhao",
      "Guanbin Li",
      "Pengfei Wan",
      "Yuanxing Zhang",
      "Wentao Zhang"
    ],
    "abstract": "Recent video generation models have revealed the emergence of Chain-of-Frame (CoF) reasoning, enabling frame-by-frame visual inference. With this capability, video models have been successfully applied to various visual tasks (e.g., maze solving, visual puzzles). However, their potential to enhance text-to-image (T2I) generation remains largely unexplored due to the absence of a clearly defined visual reasoning starting point and interpretable intermediate states in the T2I generation process. To bridge this gap, we propose CoF-T2I, a model that integrates CoF reasoning into T2I generation via progressive visual refinement, where intermediate frames act as explicit reasoning steps and the final frame is taken as output. To establish such an explicit generation process, we curate CoF-Evol-Instruct, a dataset of CoF trajectories that model the generation process from semantics to aesthetics. To further improve quality and avoid motion artifacts, we enable independent encoding operation for each frame. Experiments show that CoF-T2I significantly outperforms the base video model and achieves competitive performance on challenging benchmarks, reaching 0.86 on GenEval and 7.468 on Imagine-Bench. These results indicate the substantial promise of video models for advancing high-quality text-to-image generation.",
    "arxiv_url": "https://arxiv.org/abs/2601.10061v1",
    "pdf_url": "https://arxiv.org/pdf/2601.10061v1",
    "published_date": "2026-01-15",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "benchmark",
      "video generation"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.10061v1",
      "pdf": "https://arxiv.org/pdf/2601.10061v1"
    },
    "bibtex": ""
  },
  {
    "title": "Transition Matching Distillation for Fast Video Generation",
    "authors": [
      "Weili Nie",
      "Julius Berner",
      "Nanye Ma",
      "Chao Liu",
      "Saining Xie",
      "Arash Vahdat"
    ],
    "abstract": "Large video diffusion and flow models have achieved remarkable success in high-quality video generation, but their use in real-time interactive applications remains limited due to their inefficient multi-step sampling process. In this work, we present Transition Matching Distillation (TMD), a novel framework for distilling video diffusion models into efficient few-step generators. The central idea of TMD is to match the multi-step denoising trajectory of a diffusion model with a few-step probability transition process, where each transition is modeled as a lightweight conditional flow. To enable efficient distillation, we decompose the original diffusion backbone into two components: (1) a main backbone, comprising the majority of early layers, that extracts semantic representations at each outer transition step; and (2) a flow head, consisting of the last few layers, that leverages these representations to perform multiple inner flow updates. Given a pretrained video diffusion model, we first introduce a flow head to the model, and adapt it into a conditional flow map. We then apply distribution matching distillation to the student model with flow head rollout in each transition step. Extensive experiments on distilling Wan2.1 1.3B and 14B text-to-video models demonstrate that TMD provides a flexible and strong trade-off between generation speed and visual quality. In particular, TMD outperforms existing distilled models under comparable inference costs in terms of visual fidelity and prompt adherence. Project page: https://research.nvidia.com/labs/genair/tmd",
    "arxiv_url": "https://arxiv.org/abs/2601.09881v1",
    "pdf_url": "https://arxiv.org/pdf/2601.09881v1",
    "published_date": "2026-01-14",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "interactive",
      "video diffusion",
      "distillation",
      "dit",
      "trajectory",
      "text-to-video",
      "video generation",
      "diffusion model",
      "denoising",
      "efficient"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.09881v1",
      "pdf": "https://arxiv.org/pdf/2601.09881v1",
      "project": "https://research.nvidia.com/labs/genair/tmd"
    },
    "bibtex": ""
  },
  {
    "title": "Efficient Camera-Controlled Video Generation of Static Scenes via Sparse Diffusion and 3D Rendering",
    "authors": [
      "Jieying Chen",
      "Jeffrey Hu",
      "Joan Lasenby",
      "Ayush Tewari"
    ],
    "abstract": "Modern video generative models based on diffusion models can produce very realistic clips, but they are computationally inefficient, often requiring minutes of GPU time for just a few seconds of video. This inefficiency poses a critical barrier to deploying generative video in applications that require real-time interactions, such as embodied AI and VR/AR. This paper explores a new strategy for camera-conditioned video generation of static scenes: using diffusion-based generative models to generate a sparse set of keyframes, and then synthesizing the full video through 3D reconstruction and rendering. By lifting keyframes into a 3D representation and rendering intermediate views, our approach amortizes the generation cost across hundreds of frames while enforcing geometric consistency. We further introduce a model that predicts the optimal number of keyframes for a given camera trajectory, allowing the system to adaptively allocate computation. Our final method, SRENDER, uses very sparse keyframes for simple trajectories and denser ones for complex camera motion. This results in video generation that is more than 40 times faster than the diffusion-based baseline in generating 20 seconds of video, while maintaining high visual fidelity and temporal stability, offering a practical path toward efficient and controllable video synthesis.",
    "arxiv_url": "https://arxiv.org/abs/2601.09697v1",
    "pdf_url": "https://arxiv.org/pdf/2601.09697v1",
    "published_date": "2026-01-14",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "controllable",
      "dit",
      "trajectory",
      "video generation",
      "video synthesis",
      "diffusion model",
      "efficient"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.09697v1",
      "pdf": "https://arxiv.org/pdf/2601.09697v1"
    },
    "bibtex": ""
  },
  {
    "title": "Mixture of Distributions Matters: Dynamic Sparse Attention for Efficient Video Diffusion Transformers",
    "authors": [
      "Yuxi Liu",
      "Yipeng Hu",
      "Zekun Zhang",
      "Kunze Jiang",
      "Kun Yuan"
    ],
    "abstract": "While Diffusion Transformers (DiTs) have achieved notable progress in video generation, this long-sequence generation task remains constrained by the quadratic complexity inherent to self-attention mechanisms, creating significant barriers to practical deployment. Although sparse attention methods attempt to address this challenge, existing approaches either rely on oversimplified static patterns or require computationally expensive sampling operations to achieve dynamic sparsity, resulting in inaccurate pattern predictions and degraded generation quality. To overcome these limitations, we propose a \\underline{\\textbf{M}}ixture-\\underline{\\textbf{O}}f-\\underline{\\textbf{D}}istribution \\textbf{DiT} (\\textbf{MOD-DiT}), a novel sampling-free dynamic attention framework that accurately models evolving attention patterns through a two-stage process. First, MOD-DiT leverages prior information from early denoising steps and adopts a {distributed mixing approach} to model an efficient linear approximation model, which is then used to predict mask patterns for a specific denoising interval. Second, an online block masking strategy dynamically applies these predicted masks while maintaining historical sparsity information, eliminating the need for repetitive sampling operations. Extensive evaluations demonstrate consistent acceleration and quality improvements across multiple benchmarks and model architectures, validating MOD-DiT's effectiveness for efficient, high-quality video generation while overcoming the computational limitations of traditional sparse attention approaches.",
    "arxiv_url": "https://arxiv.org/abs/2601.11641v2",
    "pdf_url": "https://arxiv.org/pdf/2601.11641v2",
    "published_date": "2026-01-14",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "acceleration",
      "video diffusion",
      "dit",
      "diffusion transformer",
      "video generation",
      "efficient",
      "evaluation",
      "architecture",
      "denoising",
      "benchmark"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.11641v2",
      "pdf": "https://arxiv.org/pdf/2601.11641v2"
    },
    "bibtex": ""
  },
  {
    "title": "MAD: Motion Appearance Decoupling for efficient Driving World Models",
    "authors": [
      "Ahmad Rahimi",
      "Valentin Gerard",
      "Eloi Zablocki",
      "Matthieu Cord",
      "Alexandre Alahi"
    ],
    "abstract": "Recent video diffusion models generate photorealistic, temporally coherent videos, yet they fall short as reliable world models for autonomous driving, where structured motion and physically consistent interactions are essential. Adapting these generalist video models to driving domains has shown promise but typically requires massive domain-specific data and costly fine-tuning. We propose an efficient adaptation framework that converts generalist video diffusion models into controllable driving world models with minimal supervision. The key idea is to decouple motion learning from appearance synthesis. First, the model is adapted to predict structured motion in a simplified form: videos of skeletonized agents and scene elements, focusing learning on physical and social plausibility. Then, the same backbone is reused to synthesize realistic RGB videos conditioned on these motion sequences, effectively \"dressing\" the motion with texture and lighting. This two-stage process mirrors a reasoning-rendering paradigm: first infer dynamics, then render appearance. Our experiments show this decoupled approach is exceptionally efficient: adapting SVD, we match prior SOTA models with less than 6% of their compute. Scaling to LTX, our MAD-LTX model outperforms all open-source competitors, and supports a comprehensive suite of text, ego, and object controls. Project page: https://vita-epfl.github.io/MAD-World-Model/",
    "arxiv_url": "https://arxiv.org/abs/2601.09452v1",
    "pdf_url": "https://arxiv.org/pdf/2601.09452v1",
    "published_date": "2026-01-14",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "controllable",
      "video diffusion",
      "dit",
      "world model",
      "physical",
      "autonomous driving",
      "diffusion model",
      "dynamics",
      "efficient"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.09452v1",
      "pdf": "https://arxiv.org/pdf/2601.09452v1",
      "project": "https://vita-epfl.github.io/MAD-World-Model"
    },
    "bibtex": ""
  },
  {
    "title": "PhyRPR: Training-Free Physics-Constrained Video Generation",
    "authors": [
      "Yibo Zhao",
      "Hengjia Li",
      "Xiaofei He",
      "Boxi Wu"
    ],
    "abstract": "Recent diffusion-based video generation models can synthesize visually plausible videos, yet they often struggle to satisfy physical constraints. A key reason is that most existing approaches remain single-stage: they entangle high-level physical understanding with low-level visual synthesis, making it hard to generate content that require explicit physical reasoning. To address this limitation, we propose a training-free three-stage pipeline,\\textit{PhyRPR}:\\textit{Phy\\uline{R}eason}--\\textit{Phy\\uline{P}lan}--\\textit{Phy\\uline{R}efine}, which decouples physical understanding from visual synthesis. Specifically, \\textit{PhyReason} uses a large multimodal model for physical state reasoning and an image generator for keyframe synthesis; \\textit{PhyPlan} deterministically synthesizes a controllable coarse motion scaffold; and \\textit{PhyRefine} injects this scaffold into diffusion sampling via a latent fusion strategy to refine appearance while preserving the planned dynamics. This staged design enables explicit physical control during generation. Extensive experiments under physics constraints show that our method consistently improves physical plausibility and motion controllability.",
    "arxiv_url": "https://arxiv.org/abs/2601.09255v1",
    "pdf_url": "https://arxiv.org/pdf/2601.09255v1",
    "published_date": "2026-01-14",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "controllable",
      "physical",
      "video generation",
      "dynamics",
      "motion control",
      "physics"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.09255v1",
      "pdf": "https://arxiv.org/pdf/2601.09255v1"
    },
    "bibtex": ""
  },
  {
    "title": "Motion Attribution for Video Generation",
    "authors": [
      "Xindi Wu",
      "Despoina Paschalidou",
      "Jun Gao",
      "Antonio Torralba",
      "Laura Leal-Taix",
      "Olga Russakovsky",
      "Sanja Fidler",
      "Jonathan Lorraine"
    ],
    "abstract": "Despite the rapid progress of video generation models, the role of data in influencing motion is poorly understood. We present Motive (MOTIon attribution for Video gEneration), a motion-centric, gradient-based data attribution framework that scales to modern, large, high-quality video datasets and models. We use this to study which fine-tuning clips improve or degrade temporal dynamics. Motive isolates temporal dynamics from static appearance via motion-weighted loss masks, yielding efficient and scalable motion-specific influence computation. On text-to-video models, Motive identifies clips that strongly affect motion and guides data curation that improves temporal consistency and physical plausibility. With Motive-selected high-influence data, our method improves both motion smoothness and dynamic degree on VBench, achieving a 74.1% human preference win rate compared with the pretrained base model. To our knowledge, this is the first framework to attribute motion rather than visual appearance in video generative models and to use it to curate fine-tuning data.",
    "arxiv_url": "https://arxiv.org/abs/2601.08828v1",
    "pdf_url": "https://arxiv.org/pdf/2601.08828v1",
    "published_date": "2026-01-13",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.MM",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "temporal consistency",
      "physical",
      "text-to-video",
      "video generation",
      "dynamics",
      "efficient"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.08828v1",
      "pdf": "https://arxiv.org/pdf/2601.08828v1"
    },
    "bibtex": ""
  },
  {
    "title": "Rewriting Video: Text-Driven Reauthoring of Video Footage",
    "authors": [
      "Sitong Wang",
      "Anh Truong",
      "Lydia B. Chilton",
      "Dingzeyu Li"
    ],
    "abstract": "Video is a powerful medium for communication and storytelling, yet reauthoring existing footage remains challenging. Even simple edits often demand expertise, time, and careful planning, constraining how creators envision and shape their narratives. Recent advances in generative AI suggest a new paradigm: what if editing a video were as straightforward as rewriting text? To investigate this, we present a tech probe and a study on text-driven video reauthoring. Our approach involves two technical contributions: (1) a generative reconstruction algorithm that reverse-engineers video into an editable text prompt, and (2) an interactive probe, Rewrite Kit, that allows creators to manipulate these prompts. A technical evaluation of the algorithm reveals a critical human-AI perceptual gap. A probe study with 12 creators surfaced novel use cases such as virtual reshooting, synthetic continuity, and aesthetic restyling. It also highlighted key tensions around coherence, control, and creative alignment in this new paradigm. Our work contributes empirical insights into the opportunities and challenges of text-driven video reauthoring, offering design implications for future co-creative video tools.",
    "arxiv_url": "https://arxiv.org/abs/2601.08565v1",
    "pdf_url": "https://arxiv.org/pdf/2601.08565v1",
    "published_date": "2026-01-13",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "creative",
      "interactive",
      "dit",
      "evaluation",
      "text-driven video"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.08565v1",
      "pdf": "https://arxiv.org/pdf/2601.08565v1"
    },
    "bibtex": ""
  },
  {
    "title": "Spectral Generative Flow Models: A Physics-Inspired Replacement for Vectorized Large Language Models",
    "authors": [
      "Andrew Kiruluta"
    ],
    "abstract": "We introduce Spectral Generative Flow Models (SGFMs), a physics-inspired alternative to transformer-based large language models. Instead of representing text or video as sequences of discrete tokens processed by attention, SGFMs treat generation as the evolution of a continuous field governed by constrained stochastic dynamics in a multiscale wavelet basis. This formulation replaces global attention with local operators, spectral projections, and Navier--Stokes-like transport, yielding a generative mechanism grounded in continuity, geometry, and physical structure.   Our framework provides three key innovations: (i) a field-theoretic ontology in which text and video are unified as trajectories of a stochastic partial differential equation; (ii) a wavelet-domain representation that induces sparsity, scale separation, and computational efficiency; and (iii) a constrained stochastic flow that enforces stability, coherence, and uncertainty propagation. Together, these components define a generative architecture that departs fundamentally from autoregressive modeling and diffusion-based approaches. SGFMs offer a principled path toward long-range coherence, multimodal generality, and physically structured inductive bias in next-generation generative models.",
    "arxiv_url": "https://arxiv.org/abs/2601.08893v2",
    "pdf_url": "https://arxiv.org/pdf/2601.08893v2",
    "published_date": "2026-01-13",
    "categories": [
      "cs.LG",
      "cs.CL"
    ],
    "github_url": "",
    "keywords": [
      "physical",
      "architecture",
      "dynamics",
      "autoregressive",
      "physics"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.08893v2",
      "pdf": "https://arxiv.org/pdf/2601.08893v2"
    },
    "bibtex": ""
  },
  {
    "title": "CASHEW: Stabilizing Multimodal Reasoning via Iterative Trajectory Aggregation",
    "authors": [
      "Chaoyu Li",
      "Deeparghya Dutta Barua",
      "Fei Tao",
      "Pooyan Fazli"
    ],
    "abstract": "Vision-language models achieve strong performance across a wide range of multimodal understanding and reasoning tasks, yet their multi-step reasoning remains unstable. Repeated sampling over the same input often produces divergent reasoning trajectories and inconsistent final predictions. To address this, we introduce two complementary approaches inspired by test-time scaling: (1) CASHEW, an inference-time framework that stabilizes reasoning by iteratively aggregating multiple candidate trajectories into higher-quality reasoning traces, with explicit visual verification filtering hallucinated steps and grounding reasoning in visual evidence, and (2) CASHEW-RL, a learned variant that internalizes this aggregation behavior within a single model. CASHEW-RL is trained using Group Sequence Policy Optimization (GSPO) with a composite reward that encourages correct answers grounded in minimal yet sufficient visual evidence, while adaptively allocating reasoning effort based on task difficulty. This training objective enables robust self-aggregation at inference. Extensive experiments on 13 image understanding, video understanding, and video reasoning benchmarks show significant performance improvements, including gains of up to +23.6 percentage points on ScienceQA and +8.1 percentage points on EgoSchema.",
    "arxiv_url": "https://arxiv.org/abs/2601.08010v1",
    "pdf_url": "https://arxiv.org/pdf/2601.08010v1",
    "published_date": "2026-01-12",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "benchmark",
      "trajectory"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.08010v1",
      "pdf": "https://arxiv.org/pdf/2601.08010v1"
    },
    "bibtex": ""
  },
  {
    "title": "Tuning-free Visual Effect Transfer across Videos",
    "authors": [
      "Maxwell Jones",
      "Rameen Abdal",
      "Or Patashnik",
      "Ruslan Salakhutdinov",
      "Sergey Tulyakov",
      "Jun-Yan Zhu",
      "Kuan-Chieh Jackson Wang"
    ],
    "abstract": "We present RefVFX, a new framework that transfers complex temporal effects from a reference video onto a target video or image in a feed-forward manner. While existing methods excel at prompt-based or keyframe-conditioned editing, they struggle with dynamic temporal effects such as dynamic lighting changes or character transformations, which are difficult to describe via text or static conditions. Transferring a video effect is challenging, as the model must integrate the new temporal dynamics with the input video's existing motion and appearance. % To address this, we introduce a large-scale dataset of triplets, where each triplet consists of a reference effect video, an input image or video, and a corresponding output video depicting the transferred effect. Creating this data is non-trivial, especially the video-to-video effect triplets, which do not exist naturally. To generate these, we propose a scalable automated pipeline that creates high-quality paired videos designed to preserve the input's motion and structure while transforming it based on some fixed, repeatable effect. We then augment this data with image-to-video effects derived from LoRA adapters and code-based temporal effects generated through programmatic composition. Building on our new dataset, we train our reference-conditioned model using recent text-to-video backbones. Experimental results demonstrate that RefVFX produces visually consistent and temporally coherent edits, generalizes across unseen effect categories, and outperforms prompt-only baselines in both quantitative metrics and human preference. See our website at https://tuningfreevisualeffects-maker.github.io/Tuning-free-Visual-Effect-Transfer-across-Videos-Project-Page/",
    "arxiv_url": "https://arxiv.org/abs/2601.07833v3",
    "pdf_url": "https://arxiv.org/pdf/2601.07833v3",
    "published_date": "2026-01-12",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "dit",
      "text-to-video",
      "image-to-video",
      "video-to-video",
      "dynamics"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.07833v3",
      "pdf": "https://arxiv.org/pdf/2601.07833v3",
      "project": "https://tuningfreevisualeffects-maker.github.io/Tuning-free-Visual-Effect-Transfer-across-Videos-Project-Page"
    },
    "bibtex": ""
  },
  {
    "title": "MHLA: Restoring Expressivity of Linear Attention via Token-Level Multi-Head",
    "authors": [
      "Kewei Zhang",
      "Ye Huang",
      "Yufan Deng",
      "Jincheng Yu",
      "Junsong Chen",
      "Huan Ling",
      "Enze Xie",
      "Daquan Zhou"
    ],
    "abstract": "While the Transformer architecture dominates many fields, its quadratic self-attention complexity hinders its use in large-scale applications. Linear attention offers an efficient alternative, but its direct application often degrades performance, with existing fixes typically re-introducing computational overhead through extra modules (e.g., depthwise separable convolution) that defeat the original purpose. In this work, we identify a key failure mode in these methods: global context collapse, where the model loses representational diversity. To address this, we propose Multi-Head Linear Attention (MHLA), which preserves this diversity by computing attention within divided heads along the token dimension. We prove that MHLA maintains linear complexity while recovering much of the expressive power of softmax attention, and verify its effectiveness across multiple domains, achieving a 3.6\\% improvement on ImageNet classification, a 6.3\\% gain on NLP, a 12.6\\% improvement on image generation, and a 41\\% enhancement on video generation under the same time complexity.",
    "arxiv_url": "https://arxiv.org/abs/2601.07832v2",
    "pdf_url": "https://arxiv.org/pdf/2601.07832v2",
    "published_date": "2026-01-12",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "architecture",
      "video generation"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.07832v2",
      "pdf": "https://arxiv.org/pdf/2601.07832v2"
    },
    "bibtex": ""
  },
  {
    "title": "StdGEN++: A Comprehensive System for Semantic-Decomposed 3D Character Generation",
    "authors": [
      "Yuze He",
      "Yanning Zhou",
      "Wang Zhao",
      "Jingwen Ye",
      "Zhongkai Wu",
      "Ran Yi",
      "Yong-Jin Liu"
    ],
    "abstract": "We present StdGEN++, a novel and comprehensive system for generating high-fidelity, semantically decomposed 3D characters from diverse inputs. Existing 3D generative methods often produce monolithic meshes that lack the structural flexibility required by industrial pipelines in gaming and animation. Addressing this gap, StdGEN++ is built upon a Dual-branch Semantic-aware Large Reconstruction Model (Dual-Branch S-LRM), which jointly reconstructs geometry, color, and per-component semantics in a feed-forward manner. To achieve production-level fidelity, we introduce a novel semantic surface extraction formalism compatible with hybrid implicit fields. This mechanism is accelerated by a coarse-to-fine proposal scheme, which significantly reduces memory footprint and enables high-resolution mesh generation. Furthermore, we propose a video-diffusion-based texture decomposition module that disentangles appearance into editable layers (e.g., separated iris and skin), resolving semantic confusion in facial regions. Experiments demonstrate that StdGEN++ achieves state-of-the-art performance, significantly outperforming existing methods in geometric accuracy and semantic disentanglement. Crucially, the resulting structural independence unlocks advanced downstream capabilities, including non-destructive editing, physics-compliant animation, and gaze tracking, making it a robust solution for automated character asset production.",
    "arxiv_url": "https://arxiv.org/abs/2601.07660v1",
    "pdf_url": "https://arxiv.org/pdf/2601.07660v1",
    "published_date": "2026-01-12",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "physics",
      "dit"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.07660v1",
      "pdf": "https://arxiv.org/pdf/2601.07660v1"
    },
    "bibtex": ""
  },
  {
    "title": "Forecast the Principal, Stabilize the Residual: Subspace-Aware Feature Caching for Efficient Diffusion Transformers",
    "authors": [
      "Guantao Chen",
      "Shikang Zheng",
      "Yuqi Lin",
      "Linfeng Zhang"
    ],
    "abstract": "Diffusion Transformer (DiT) models have achieved unprecedented quality in image and video generation, yet their iterative sampling process remains computationally prohibitive. To accelerate inference, feature caching methods have emerged by reusing intermediate representations across timesteps. However, existing caching approaches treat all feature components uniformly. We reveal that DiT feature spaces contain distinct principal and residual subspaces with divergent temporal behavior: the principal subspace evolves smoothly and predictably, while the residual subspace exhibits volatile, low-energy oscillations that resist accurate prediction. Building on this insight, we propose SVD-Cache, a subspace-aware caching framework that decomposes diffusion features via Singular Value Decomposition (SVD), applies exponential moving average (EMA) prediction to the dominant low-rank components, and directly reuses the residual subspace. Extensive experiments demonstrate that SVD-Cache achieves near-lossless across diverse models and methods, including 5.55$\\times$ speedup on FLUX and HunyuanVideo, and compatibility with model acceleration techniques including distillation, quantization and sparse attention. Our code is in supplementary material and will be released on Github.",
    "arxiv_url": "https://arxiv.org/abs/2601.07396v1",
    "pdf_url": "https://arxiv.org/pdf/2601.07396v1",
    "published_date": "2026-01-12",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "acceleration",
      "distillation",
      "dit",
      "diffusion transformer",
      "video generation",
      "efficient"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.07396v1",
      "pdf": "https://arxiv.org/pdf/2601.07396v1"
    },
    "bibtex": ""
  },
  {
    "title": "Focal Guidance: Unlocking Controllability from Semantic-Weak Layers in Video Diffusion Models",
    "authors": [
      "Yuanyang Yin",
      "Yufan Deng",
      "Shenghai Yuan",
      "Kaipeng Zhang",
      "Xiao Yang",
      "Feng Zhao"
    ],
    "abstract": "The task of Image-to-Video (I2V) generation aims to synthesize a video from a reference image and a text prompt. This requires diffusion models to reconcile high-frequency visual constraints and low-frequency textual guidance during the denoising process. However, while existing I2V models prioritize visual consistency, how to effectively couple this dual guidance to ensure strong adherence to the text prompt remains underexplored. In this work, we observe that in Diffusion Transformer (DiT)-based I2V models, certain intermediate layers exhibit weak semantic responses (termed Semantic-Weak Layers), as indicated by a measurable drop in text-visual similarity. We attribute this to a phenomenon called Condition Isolation, where attention to visual features becomes partially detached from text guidance and overly relies on learned visual priors. To address this, we propose Focal Guidance (FG), which enhances the controllability from Semantic-Weak Layers. FG comprises two mechanisms: (1) Fine-grained Semantic Guidance (FSG) leverages CLIP to identify key regions in the reference frame and uses them as anchors to guide Semantic-Weak Layers. (2) Attention Cache transfers attention maps from semantically responsive layers to Semantic-Weak Layers, injecting explicit semantic signals and alleviating their over-reliance on the model's learned visual priors, thereby enhancing adherence to textual instructions. To further validate our approach and address the lack of evaluation in this direction, we introduce a benchmark for assessing instruction following in I2V models. On this benchmark, Focal Guidance proves its effectiveness and generalizability, raising the total score on Wan2.1-I2V to 0.7250 (+3.97\\%) and boosting the MMDiT-based HunyuanVideo-I2V to 0.5571 (+7.44\\%).",
    "arxiv_url": "https://arxiv.org/abs/2601.07287v1",
    "pdf_url": "https://arxiv.org/pdf/2601.07287v1",
    "published_date": "2026-01-12",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "video diffusion",
      "dit",
      "diffusion transformer",
      "image-to-video",
      "i2v",
      "evaluation",
      "diffusion model",
      "denoising",
      "benchmark"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.07287v1",
      "pdf": "https://arxiv.org/pdf/2601.07287v1"
    },
    "bibtex": ""
  },
  {
    "title": "HOSC: A Periodic Activation with Saturation Control for High-Fidelity Implicit Neural Representations",
    "authors": [
      "Michal Jan Wlodarczyk",
      "Danzel Serrano",
      "Przemyslaw Musialski"
    ],
    "abstract": "Periodic activations such as sine preserve high-frequency information in implicit neural representations (INRs) through their oscillatory structure, but often suffer from gradient instability and limited control over multi-scale behavior. We introduce the Hyperbolic Oscillator with Saturation Control (HOSC) activation, $\\text{HOSC}(x) = \\tanh\\bigl(\\sin(_0 x)\\bigr)$, which exposes an explicit parameter $$ that controls the Lipschitz bound of the activation by $_0$. This provides a direct mechanism to tune gradient magnitudes while retaining a periodic carrier. We provide a mathematical analysis and conduct a comprehensive empirical study across images, audio, video, NeRFs, and SDFs using standardized training protocols. Comparative analysis against SIREN, FINER, and related methods shows where HOSC provides substantial benefits and where it achieves competitive parity. Results establish HOSC as a practical periodic activation for INR applications, with domain-specific guidance on hyperparameter selection. For code visit the project page https://hosc-nn.github.io/ .",
    "arxiv_url": "https://arxiv.org/abs/2601.07870v1",
    "pdf_url": "https://arxiv.org/pdf/2601.07870v1",
    "published_date": "2026-01-10",
    "categories": [
      "cs.LG",
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.07870v1",
      "pdf": "https://arxiv.org/pdf/2601.07870v1",
      "project": "https://hosc-nn.github.io"
    },
    "bibtex": ""
  },
  {
    "title": "QCaption: Video Captioning and Q&A through Fusion of Large Multimodal Models",
    "authors": [
      "Jiale Wang",
      "Gee Wah Ng",
      "Lee Onn Mak",
      "Randall Cher",
      "Ng Ding Hei Ryan",
      "Davis Wang"
    ],
    "abstract": "This paper introduces QCaption, a novel video captioning and Q&A pipeline that enhances video analytics by fusing three models: key frame extraction, a Large Multimodal Model (LMM) for image-text analysis, and a Large Language Model (LLM) for text analysis. This approach enables integrated analysis of text, images, and video, achieving performance improvements over existing video captioning and Q&A models; all while remaining fully self-contained, adept for on-premises deployment. Experimental results using QCaption demonstrated up to 44.2% and 48.9% improvements in video captioning and Q&A tasks, respectively. Ablation studies were also performed to assess the role of LLM on the fusion on the results. Moreover, the paper proposes and evaluates additional video captioning approaches, benchmarking them against QCaption and existing methodologies. QCaption demonstrate the potential of adopting a model fusion approach in advancing video analytics.",
    "arxiv_url": "https://arxiv.org/abs/2601.06566v1",
    "pdf_url": "https://arxiv.org/pdf/2601.06566v1",
    "published_date": "2026-01-10",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "benchmark",
      "dit"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.06566v1",
      "pdf": "https://arxiv.org/pdf/2601.06566v1"
    },
    "bibtex": ""
  },
  {
    "title": "Object-WIPER : Training-Free Object and Associated Effect Removal in Videos",
    "authors": [
      "Saksham Singh Kushwaha",
      "Sayan Nag",
      "Yapeng Tian",
      "Kuldeep Kulkarni"
    ],
    "abstract": "In this paper, we introduce Object-WIPER, a training-free framework for removing dynamic objects and their associated visual effects from videos, and inpainting them with semantically consistent and temporally coherent content. Our approach leverages a pre-trained text-to-video diffusion transformer (DiT). Given an input video, a user-provided object mask, and query tokens describing the target object and its effects, we localize relevant visual tokens via visual-text cross-attention and visual self-attention. This produces an intermediate effect mask that we fuse with the user mask to obtain a final foreground token mask to replace. We first invert the video through the DiT to obtain structured noise, then reinitialize the masked tokens with Gaussian noise while preserving background tokens. During denoising, we copy values for the background tokens saved during inversion to maintain scene fidelity. To address the lack of suitable evaluation, we introduce a new object removal metric that rewards temporal consistency among foreground tokens across consecutive frames, coherence between foreground and background tokens within each frame, and dissimilarity between the input and output foreground tokens. Experiments on DAVIS and a newly curated real-world associated effect benchmark (WIPER-Bench) show that Object-WIPER surpasses both training-based and training-free baselines in terms of the metric, achieving clean removal and temporally stable reconstruction without any retraining. Our new benchmark, source code, and pre-trained models will be publicly available.",
    "arxiv_url": "https://arxiv.org/abs/2601.06391v1",
    "pdf_url": "https://arxiv.org/pdf/2601.06391v1",
    "published_date": "2026-01-10",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "temporal consistency",
      "video diffusion",
      "dit",
      "diffusion transformer",
      "text-to-video",
      "evaluation",
      "denoising",
      "benchmark"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.06391v1",
      "pdf": "https://arxiv.org/pdf/2601.06391v1"
    },
    "bibtex": ""
  },
  {
    "title": "Perception Test 2025: Challenge Summary and a Unified VQA Extension",
    "authors": [
      "Joseph Heyward",
      "Nikhil Pathasarathy",
      "Tyler Zhu",
      "Aravindh Mahendran",
      "Joo Carreira",
      "Dima Damen",
      "Andrew Zisserman",
      "Viorica Ptrucean"
    ],
    "abstract": "The Third Perception Test challenge was organised as a full-day workshop alongside the IEEE/CVF International Conference on Computer Vision (ICCV) 2025. Its primary goal is to benchmark state-of-the-art video models and measure the progress in multimodal perception. This year, the workshop featured 2 guest tracks as well: KiVA (an image understanding challenge) and Physic-IQ (a video generation challenge). In this report, we summarise the results from the main Perception Test challenge, detailing both the existing tasks as well as novel additions to the benchmark. In this iteration, we placed an emphasis on task unification, as this poses a more challenging test for current SOTA multimodal models. The challenge included five consolidated tracks: unified video QA, unified object and point tracking, unified action and sound localisation, grounded video QA, and hour-long video QA, alongside an analysis and interpretability track that is still open for submissions. Notably, the unified video QA track introduced a novel subset that reformulates traditional perception tasks (such as point tracking and temporal action localisation) as multiple-choice video QA questions that video-language models can natively tackle. The unified object and point tracking merged the original object tracking and point tracking tasks, whereas the unified action and sound localisation merged the original temporal action localisation and temporal sound localisation tracks. Accordingly, we required competitors to use unified approaches rather than engineered pipelines with task-specific models. By proposing such a unified challenge, Perception Test 2025 highlights the significant difficulties existing models face when tackling diverse perception tasks through unified interfaces.",
    "arxiv_url": "https://arxiv.org/abs/2601.06287v1",
    "pdf_url": "https://arxiv.org/pdf/2601.06287v1",
    "published_date": "2026-01-09",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "dit",
      "video generation",
      "sound",
      "long video",
      "benchmark"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.06287v1",
      "pdf": "https://arxiv.org/pdf/2601.06287v1"
    },
    "bibtex": ""
  },
  {
    "title": "VideoAR: Autoregressive Video Generation via Next-Frame & Scale Prediction",
    "authors": [
      "Longbin Ji",
      "Xiaoxiong Liu",
      "Junyuan Shang",
      "Shuohuan Wang",
      "Yu Sun",
      "Hua Wu",
      "Haifeng Wang"
    ],
    "abstract": "Recent advances in video generation have been dominated by diffusion and flow-matching models, which produce high-quality results but remain computationally intensive and difficult to scale. In this work, we introduce VideoAR, the first large-scale Visual Autoregressive (VAR) framework for video generation that combines multi-scale next-frame prediction with autoregressive modeling. VideoAR disentangles spatial and temporal dependencies by integrating intra-frame VAR modeling with causal next-frame prediction, supported by a 3D multi-scale tokenizer that efficiently encodes spatio-temporal dynamics. To improve long-term consistency, we propose Multi-scale Temporal RoPE, Cross-Frame Error Correction, and Random Frame Mask, which collectively mitigate error propagation and stabilize temporal coherence. Our multi-stage pretraining pipeline progressively aligns spatial and temporal learning across increasing resolutions and durations. Empirically, VideoAR achieves new state-of-the-art results among autoregressive models, improving FVD on UCF-101 from 99.5 to 88.6 while reducing inference steps by over 10x, and reaching a VBench score of 81.74-competitive with diffusion-based models an order of magnitude larger. These results demonstrate that VideoAR narrows the performance gap between autoregressive and diffusion paradigms, offering a scalable, efficient, and temporally consistent foundation for future video generation research.",
    "arxiv_url": "https://arxiv.org/abs/2601.05966v2",
    "pdf_url": "https://arxiv.org/pdf/2601.05966v2",
    "published_date": "2026-01-09",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "dynamics",
      "video generation",
      "autoregressive"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.05966v2",
      "pdf": "https://arxiv.org/pdf/2601.05966v2"
    },
    "bibtex": ""
  },
  {
    "title": "Goal Force: Teaching Video Models To Accomplish Physics-Conditioned Goals",
    "authors": [
      "Nate Gillman",
      "Yinghua Zhou",
      "Zitian Tang",
      "Evan Luo",
      "Arjan Chakravarthy",
      "Daksh Aggarwal",
      "Michael Freeman",
      "Charles Herrmann",
      "Chen Sun"
    ],
    "abstract": "Recent advancements in video generation have enabled the development of ``world models'' capable of simulating potential futures for robotics and planning. However, specifying precise goals for these models remains a challenge; text instructions are often too abstract to capture physical nuances, while target images are frequently infeasible to specify for dynamic tasks. To address this, we introduce Goal Force, a novel framework that allows users to define goals via explicit force vectors and intermediate dynamics, mirroring how humans conceptualize physical tasks. We train a video generation model on a curated dataset of synthetic causal primitives-such as elastic collisions and falling dominos-teaching it to propagate forces through time and space. Despite being trained on simple physics data, our model exhibits remarkable zero-shot generalization to complex, real-world scenarios, including tool manipulation and multi-object causal chains. Our results suggest that by grounding video generation in fundamental physical interactions, models can emerge as implicit neural physics simulators, enabling precise, physics-aware planning without reliance on external engines. We release all datasets, code, model weights, and interactive video demos at our project page.",
    "arxiv_url": "https://arxiv.org/abs/2601.05848v1",
    "pdf_url": "https://arxiv.org/pdf/2601.05848v1",
    "published_date": "2026-01-09",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "interactive",
      "dit",
      "world model",
      "physical",
      "video generation",
      "robotics",
      "concept",
      "dynamics",
      "physics-aware",
      "physics"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.05848v1",
      "pdf": "https://arxiv.org/pdf/2601.05848v1"
    },
    "bibtex": ""
  },
  {
    "title": "TAGRPO: Boosting GRPO on Image-to-Video Generation with Direct Trajectory Alignment",
    "authors": [
      "Jin Wang",
      "Jianxiang Lu",
      "Guangzheng Xu",
      "Comi Chen",
      "Haoyu Yang",
      "Linqing Wang",
      "Peng Chen",
      "Mingtao Chen",
      "Zhichao Hu",
      "Longhuang Wu",
      "Shuai Shao",
      "Qinglin Lu",
      "Ping Luo"
    ],
    "abstract": "Recent studies have demonstrated the efficacy of integrating Group Relative Policy Optimization (GRPO) into flow matching models, particularly for text-to-image and text-to-video generation. However, we find that directly applying these techniques to image-to-video (I2V) models often fails to yield consistent reward improvements. To address this limitation, we present TAGRPO, a robust post-training framework for I2V models inspired by contrastive learning. Our approach is grounded in the observation that rollout videos generated from identical initial noise provide superior guidance for optimization. Leveraging this insight, we propose a novel GRPO loss applied to intermediate latents, encouraging direct alignment with high-reward trajectories while maximizing distance from low-reward counterparts. Furthermore, we introduce a memory bank for rollout videos to enhance diversity and reduce computational overhead. Despite its simplicity, TAGRPO achieves significant improvements over DanceGRPO in I2V generation.",
    "arxiv_url": "https://arxiv.org/abs/2601.05729v1",
    "pdf_url": "https://arxiv.org/pdf/2601.05729v1",
    "published_date": "2026-01-09",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "flow matching",
      "trajectory",
      "text-to-video",
      "video generation",
      "image-to-video",
      "i2v"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.05729v1",
      "pdf": "https://arxiv.org/pdf/2601.05729v1"
    },
    "bibtex": ""
  },
  {
    "title": "Rotate Your Character: Revisiting Video Diffusion Models for High-Quality 3D Character Generation",
    "authors": [
      "Jin Wang",
      "Jianxiang Lu",
      "Comi Chen",
      "Guangzheng Xu",
      "Haoyu Yang",
      "Peng Chen",
      "Na Zhang",
      "Yifan Xu",
      "Longhuang Wu",
      "Shuai Shao",
      "Qinglin Lu",
      "Ping Luo"
    ],
    "abstract": "Generating high-quality 3D characters from single images remains a significant challenge in digital content creation, particularly due to complex body poses and self-occlusion. In this paper, we present RCM (Rotate your Character Model), an advanced image-to-video diffusion framework tailored for high-quality novel view synthesis (NVS) and 3D character generation. Compared to existing diffusion-based approaches, RCM offers several key advantages: (1) transferring characters with any complex poses into a canonical pose, enabling consistent novel view synthesis across the entire viewing orbit, (2) high-resolution orbital video generation at 1024x1024 resolution, (3) controllable observation positions given different initial camera poses, and (4) multi-view conditioning supporting up to 4 input images, accommodating diverse user scenarios. Extensive experiments demonstrate that RCM outperforms state-of-the-art methods in both novel view synthesis and 3D generation quality.",
    "arxiv_url": "https://arxiv.org/abs/2601.05722v1",
    "pdf_url": "https://arxiv.org/pdf/2601.05722v1",
    "published_date": "2026-01-09",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "controllable",
      "video diffusion",
      "dit",
      "video generation",
      "image-to-video",
      "diffusion model",
      "novel view"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.05722v1",
      "pdf": "https://arxiv.org/pdf/2601.05722v1"
    },
    "bibtex": ""
  },
  {
    "title": "UniFinEval: Towards Unified Evaluation of Financial Multimodal Models across Text, Images and Videos",
    "authors": [
      "Zhi Yang",
      "Lingfeng Zeng",
      "Fangqi Lou",
      "Qi Qi",
      "Wei Zhang",
      "Zhenyu Wu",
      "Zhenxiong Yu",
      "Jun Han",
      "Zhiheng Jin",
      "Lejie Zhang",
      "Xiaoming Huang",
      "Xiaolong Liang",
      "Zheng Wei",
      "Junbo Zou",
      "Dongpo Cheng",
      "Zhaowei Liu",
      "Xin Guo",
      "Rongjunchen Zhang",
      "Liwen Zhang"
    ],
    "abstract": "Multimodal large language models are playing an increasingly significant role in empowering the financial domain, however, the challenges they face, such as multimodal and high-density information and cross-modal multi-hop reasoning, go beyond the evaluation scope of existing multimodal benchmarks. To address this gap, we propose UniFinEval, the first unified multimodal benchmark designed for high-information-density financial environments, covering text, images, and videos. UniFinEval systematically constructs five core financial scenarios grounded in real-world financial systems: Financial Statement Auditing, Company Fundamental Reasoning, Industry Trend Insights, Financial Risk Sensing, and Asset Allocation Analysis. We manually construct a high-quality dataset consisting of 3,767 question-answer pairs in both chinese and english and systematically evaluate 10 mainstream MLLMs under Zero-Shot and CoT settings. Results show that Gemini-3-pro-preview achieves the best overall performance, yet still exhibits a substantial gap compared to financial experts. Further error analysis reveals systematic deficiencies in current models. UniFinEval aims to provide a systematic assessment of MLLMs' capabilities in fine-grained, high-information-density financial environments, thereby enhancing the robustness of MLLMs applications in real-world financial scenarios. Data and code are available at https://github.com/aifinlab/UniFinEval.",
    "arxiv_url": "https://arxiv.org/abs/2601.22162v1",
    "pdf_url": "https://arxiv.org/pdf/2601.22162v1",
    "published_date": "2026-01-09",
    "categories": [
      "q-fin.GN",
      "cs.AI",
      "cs.CL"
    ],
    "github_url": "https://github.com/aifinlab/UniFinEval",
    "keywords": [
      "benchmark",
      "evaluation",
      "dit"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.22162v1",
      "pdf": "https://arxiv.org/pdf/2601.22162v1",
      "github": "https://github.com/aifinlab/UniFinEval"
    },
    "bibtex": ""
  },
  {
    "title": "GaussianSwap: Animatable Video Face Swapping with 3D Gaussian Splatting",
    "authors": [
      "Xuan Cheng",
      "Jiahao Rao",
      "Chengyang Li",
      "Wenhao Wang",
      "Weilin Chen",
      "Lvqing Yang"
    ],
    "abstract": "We introduce GaussianSwap, a novel video face swapping framework that constructs a 3D Gaussian Splatting based face avatar from a target video while transferring identity from a source image to the avatar. Conventional video swapping frameworks are limited to generating facial representations in pixel-based formats. The resulting swapped faces exist merely as a set of unstructured pixels without any capacity for animation or interactive manipulation. Our work introduces a paradigm shift from conventional pixel-based video generation to the creation of high-fidelity avatar with swapped faces. The framework first preprocesses target video to extract FLAME parameters, camera poses and segmentation masks, and then rigs 3D Gaussian splats to the FLAME model across frames, enabling dynamic facial control. To ensure identity preserving, we propose an compound identity embedding constructed from three state-of-the-art face recognition models for avatar finetuning. Finally, we render the face-swapped avatar on the background frames to obtain the face-swapped video. Experimental results demonstrate that GaussianSwap achieves superior identity preservation, visual clarity and temporal consistency, while enabling previously unattainable interactive applications.",
    "arxiv_url": "https://arxiv.org/abs/2601.05511v1",
    "pdf_url": "https://arxiv.org/pdf/2601.05511v1",
    "published_date": "2026-01-09",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "interactive",
      "temporal consistency",
      "identity",
      "video generation",
      "avatar"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.05511v1",
      "pdf": "https://arxiv.org/pdf/2601.05511v1"
    },
    "bibtex": ""
  },
  {
    "title": "RoboVIP: Multi-View Video Generation with Visual Identity Prompting Augments Robot Manipulation",
    "authors": [
      "Boyang Wang",
      "Haoran Zhang",
      "Shujie Zhang",
      "Jinkun Hao",
      "Mingda Jia",
      "Qi Lv",
      "Yucheng Mao",
      "Zhaoyang Lyu",
      "Jia Zeng",
      "Xudong Xu",
      "Jiangmiao Pang"
    ],
    "abstract": "The diversity, quantity, and quality of manipulation data are critical for training effective robot policies. However, due to hardware and physical setup constraints, collecting large-scale real-world manipulation data remains difficult to scale across diverse environments. Recent work uses text-prompt conditioned image diffusion models to augment manipulation data by altering the backgrounds and tabletop objects in the visual observations. However, these approaches often overlook the practical need for multi-view and temporally coherent observations required by state-of-the-art policy models. Further, text prompts alone cannot reliably specify the scene setup. To provide the diffusion model with explicit visual guidance, we introduce visual identity prompting, which supplies exemplar images as conditioning inputs to guide the generation of the desired scene setup. To this end, we also build a scalable pipeline to curate a visual identity pool from large robotics datasets. Using our augmented manipulation data to train downstream vision-language-action and visuomotor policy models yields consistent performance gains in both simulation and real-robot settings.",
    "arxiv_url": "https://arxiv.org/abs/2601.05241v1",
    "pdf_url": "https://arxiv.org/pdf/2601.05241v1",
    "published_date": "2026-01-08",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "dit",
      "identity",
      "physical",
      "video generation",
      "robotics",
      "multi-view video",
      "diffusion model",
      "simulation"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.05241v1",
      "pdf": "https://arxiv.org/pdf/2601.05241v1"
    },
    "bibtex": ""
  },
  {
    "title": "Plenoptic Video Generation",
    "authors": [
      "Xiao Fu",
      "Shitao Tang",
      "Min Shi",
      "Xian Liu",
      "Jinwei Gu",
      "Ming-Yu Liu",
      "Dahua Lin",
      "Chen-Hsuan Lin"
    ],
    "abstract": "Camera-controlled generative video re-rendering methods, such as ReCamMaster, have achieved remarkable progress. However, despite their success in single-view setting, these works often struggle to maintain consistency across multi-view scenarios. Ensuring spatio-temporal coherence in hallucinated regions remains challenging due to the inherent stochasticity of generative models. To address it, we introduce PlenopticDreamer, a framework that synchronizes generative hallucinations to maintain spatio-temporal memory. The core idea is to train a multi-in-single-out video-conditioned model in an autoregressive manner, aided by a camera-guided video retrieval strategy that adaptively selects salient videos from previous generations as conditional inputs. In addition, Our training incorporates progressive context-scaling to improve convergence, self-conditioning to enhance robustness against long-range visual degradation caused by error accumulation, and a long-video conditioning mechanism to support extended video generation. Extensive experiments on the Basic and Agibot benchmarks demonstrate that PlenopticDreamer achieves state-of-the-art video re-rendering, delivering superior view synchronization, high-fidelity visuals, accurate camera control, and diverse view transformations (e.g., third-person to third-person, and head-view to gripper-view in robotic manipulation). Project page: https://research.nvidia.com/labs/dir/plenopticdreamer/",
    "arxiv_url": "https://arxiv.org/abs/2601.05239v1",
    "pdf_url": "https://arxiv.org/pdf/2601.05239v1",
    "published_date": "2026-01-08",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "dit",
      "video generation",
      "autoregressive",
      "benchmark",
      "camera control"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.05239v1",
      "pdf": "https://arxiv.org/pdf/2601.05239v1",
      "project": "https://research.nvidia.com/labs/dir/plenopticdreamer"
    },
    "bibtex": ""
  },
  {
    "title": "VerseCrafter: Dynamic Realistic Video World Model with 4D Geometric Control",
    "authors": [
      "Sixiao Zheng",
      "Minghao Yin",
      "Wenbo Hu",
      "Xiaoyu Li",
      "Ying Shan",
      "Yanwei Fu"
    ],
    "abstract": "Video world models aim to simulate dynamic, real-world environments, yet existing methods struggle to provide unified and precise control over camera and multi-object motion, as videos inherently operate dynamics in the projected 2D image plane. To bridge this gap, we introduce VerseCrafter, a 4D-aware video world model that enables explicit and coherent control over both camera and object dynamics within a unified 4D geometric world state. Our approach is centered on a novel 4D Geometric Control representation, which encodes the world state through a static background point cloud and per-object 3D Gaussian trajectories. This representation captures not only an object's path but also its probabilistic 3D occupancy over time, offering a flexible, category-agnostic alternative to rigid bounding boxes or parametric models. These 4D controls are rendered into conditioning signals for a pretrained video diffusion model, enabling the generation of high-fidelity, view-consistent videos that precisely adhere to the specified dynamics. Unfortunately, another major challenge lies in the scarcity of large-scale training data with explicit 4D annotations. We address this by developing an automatic data engine that extracts the required 4D controls from in-the-wild videos, allowing us to train our model on a massive and diverse dataset.",
    "arxiv_url": "https://arxiv.org/abs/2601.05138v1",
    "pdf_url": "https://arxiv.org/pdf/2601.05138v1",
    "published_date": "2026-01-08",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "video diffusion",
      "dit",
      "world model",
      "dynamics",
      "diffusion model"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.05138v1",
      "pdf": "https://arxiv.org/pdf/2601.05138v1"
    },
    "bibtex": ""
  },
  {
    "title": "From Understanding to Engagement: Personalized pharmacy Video Clips via Vision Language Models (VLMs)",
    "authors": [
      "Suyash Mishra",
      "Qiang Li",
      "Srikanth Patil",
      "Anubhav Girdhar"
    ],
    "abstract": "Vision Language Models (VLMs) are poised to revolutionize the digital transformation of pharmacyceutical industry by enabling intelligent, scalable, and automated multi-modality content processing. Traditional manual annotation of heterogeneous data modalities (text, images, video, audio, and web links), is prone to inconsistencies, quality degradation, and inefficiencies in content utilization. The sheer volume of long video and audio data further exacerbates these challenges, (e.g. long clinical trial interviews and educational seminars).   Here, we introduce a domain adapted Video to Video Clip Generation framework that integrates Audio Language Models (ALMs) and Vision Language Models (VLMs) to produce highlight clips. Our contributions are threefold: (i) a reproducible Cut & Merge algorithm with fade in/out and timestamp normalization, ensuring smooth transitions and audio/visual alignment; (ii) a personalization mechanism based on role definition and prompt injection for tailored outputs (marketing, training, regulatory); (iii) a cost efficient e2e pipeline strategy balancing ALM/VLM enhanced processing. Evaluations on Video MME benchmark (900) and our proprietary dataset of 16,159 pharmacy videos across 14 disease areas demonstrate 3 to 4 times speedup, 4 times cost reduction, and competitive clip quality. Beyond efficiency gains, we also report our methods improved clip coherence scores (0.348) and informativeness scores (0.721) over state of the art VLM baselines (e.g., Gemini 2.5 Pro), highlighting the potential of transparent, custom extractive, and compliance supporting video summarization for life sciences.",
    "arxiv_url": "https://arxiv.org/abs/2601.05059v1",
    "pdf_url": "https://arxiv.org/pdf/2601.05059v1",
    "published_date": "2026-01-08",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "multi-modal",
      "education",
      "dit",
      "efficient",
      "evaluation",
      "long video",
      "personalization",
      "benchmark"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.05059v1",
      "pdf": "https://arxiv.org/pdf/2601.05059v1"
    },
    "bibtex": ""
  },
  {
    "title": "SOVABench: A Vehicle Surveillance Action Retrieval Benchmark for Multimodal Large Language Models",
    "authors": [
      "Oriol Rabasseda",
      "Zenjie Li",
      "Kamal Nasrollahi",
      "Sergio Escalera"
    ],
    "abstract": "Automatic identification of events and recurrent behavior analysis are critical for video surveillance. However, most existing content-based video retrieval benchmarks focus on scene-level similarity and do not evaluate the action discrimination required in surveillance. To address this gap, we introduce SOVABench (Surveillance Opposite Vehicle Actions Benchmark), a real-world retrieval benchmark built from surveillance footage and centered on vehicle-related actions. SOVABench defines two evaluation protocols (inter-pair and intra-pair) to assess cross-action discrimination and temporal direction understanding. Although action distinctions are generally intuitive for human observers, our experiments show that they remain challenging for state-of-the-art vision and multimodal models.   Leveraging the visual reasoning and instruction-following capabilities of Multimodal Large Language Models (MLLMs), we present a training-free framework for producing interpretable embeddings from MLLM-generated descriptions for both images and videos. The framework achieves strong performance on SOVABench as well as on several spatial and counting benchmarks where contrastive Vision-Language Models often fail. The code, annotations, and instructions to construct the benchmark are publicly available.",
    "arxiv_url": "https://arxiv.org/abs/2601.04824v2",
    "pdf_url": "https://arxiv.org/pdf/2601.04824v2",
    "published_date": "2026-01-08",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "benchmark",
      "evaluation"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.04824v2",
      "pdf": "https://arxiv.org/pdf/2601.04824v2"
    },
    "bibtex": ""
  },
  {
    "title": "CounterVid: Counterfactual Video Generation for Mitigating Action and Temporal Hallucinations in Video-Language Models",
    "authors": [
      "Tobia Poppi",
      "Burak Uzkent",
      "Amanmeet Garg",
      "Lucas Porto",
      "Garin Kessler",
      "Yezhou Yang",
      "Marcella Cornia",
      "Lorenzo Baraldi",
      "Rita Cucchiara",
      "Florian Schiffers"
    ],
    "abstract": "Video-language models (VLMs) achieve strong multimodal understanding but remain prone to hallucinations, especially when reasoning about actions and temporal order. Existing mitigation strategies, such as textual filtering or random video perturbations, often fail to address the root cause: over-reliance on language priors rather than fine-grained visual dynamics. We propose a scalable framework for counterfactual video generation that synthesizes videos differing only in actions or temporal structure while preserving scene context. Our pipeline combines multimodal LLMs for action proposal and editing guidance with diffusion-based image and video models to generate semantic hard negatives at scale. Using this framework, we build CounterVid, a synthetic dataset of ~26k preference pairs targeting action recognition and temporal reasoning. We further introduce MixDPO, a unified Direct Preference Optimization approach that jointly leverages textual and visual preferences. Fine-tuning Qwen2.5-VL with MixDPO yields consistent improvements, notably in temporal ordering, and transfers effectively to standard video hallucination benchmarks. Code and models will be made publicly available.",
    "arxiv_url": "https://arxiv.org/abs/2601.04778v1",
    "pdf_url": "https://arxiv.org/pdf/2601.04778v1",
    "published_date": "2026-01-08",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.MM"
    ],
    "github_url": "",
    "keywords": [
      "benchmark",
      "dynamics",
      "video generation",
      "dit"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.04778v1",
      "pdf": "https://arxiv.org/pdf/2601.04778v1"
    },
    "bibtex": ""
  },
  {
    "title": "PackCache: A Training-Free Acceleration Method for Unified Autoregressive Video Generation via Compact KV-Cache",
    "authors": [
      "Kunyang Li",
      "Mubarak Shah",
      "Yuzhang Shang"
    ],
    "abstract": "A unified autoregressive model is a Transformer-based framework that addresses diverse multimodal tasks (e.g., text, image, video) as a single sequence modeling problem under a shared token space. Such models rely on the KV-cache mechanism to reduce attention computation from O(T^2) to O(T); however, KV-cache size grows linearly with the number of generated tokens, and it rapidly becomes the dominant bottleneck limiting inference efficiency and generative length. Unified autoregressive video generation inherits this limitation. Our analysis reveals that KV-cache tokens exhibit distinct spatiotemporal properties: (i) text and conditioning-image tokens act as persistent semantic anchors that consistently receive high attention, and (ii) attention to previous frames naturally decays with temporal distance. Leveraging these observations, we introduce PackCache, a training-free KV-cache management method that dynamically compacts the KV cache through three coordinated mechanisms: condition anchoring that preserves semantic references, cross-frame decay modeling that allocates cache budget according to temporal distance, and spatially preserving position embedding that maintains coherent 3D structure under cache removal. In terms of efficiency, PackCache accelerates end-to-end generation by 1.7-2.2x on 48-frame long sequences, showcasing its strong potential for enabling longer-sequence video generation. Notably, the final four frames - the portion most impacted by the progressively expanding KV-cache and thus the most expensive segment of the clip - PackCache delivers a 2.6x and 3.7x acceleration on A40 and H200, respectively, for 48-frame videos.",
    "arxiv_url": "https://arxiv.org/abs/2601.04359v1",
    "pdf_url": "https://arxiv.org/pdf/2601.04359v1",
    "published_date": "2026-01-07",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "acceleration",
      "video generation",
      "autoregressive",
      "dit"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.04359v1",
      "pdf": "https://arxiv.org/pdf/2601.04359v1"
    },
    "bibtex": ""
  },
  {
    "title": "ReHyAt: Recurrent Hybrid Attention for Video Diffusion Transformers",
    "authors": [
      "Mohsen Ghafoorian",
      "Amirhossein Habibian"
    ],
    "abstract": "Recent advances in video diffusion models have shifted towards transformer-based architectures, achieving state-of-the-art video generation but at the cost of quadratic attention complexity, which severely limits scalability for longer sequences. We introduce ReHyAt, a Recurrent Hybrid Attention mechanism that combines the fidelity of softmax attention with the efficiency of linear attention, enabling chunk-wise recurrent reformulation and constant memory usage. Unlike the concurrent linear-only SANA Video, ReHyAt's hybrid design allows efficient distillation from existing softmax-based models, reducing the training cost by two orders of magnitude to ~160 GPU hours, while being competitive in the quality. Our light-weight distillation and finetuning pipeline provides a recipe that can be applied to future state-of-the-art bidirectional softmax-based models. Experiments on VBench and VBench-2.0, as well as a human preference study, demonstrate that ReHyAt achieves state-of-the-art video quality while reducing attention cost from quadratic to linear, unlocking practical scalability for long-duration and on-device video generation. Project page is available at https://qualcomm-ai-research.github.io/rehyat.",
    "arxiv_url": "https://arxiv.org/abs/2601.04342v1",
    "pdf_url": "https://arxiv.org/pdf/2601.04342v1",
    "published_date": "2026-01-07",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "video diffusion",
      "distillation",
      "diffusion transformer",
      "video generation",
      "diffusion model",
      "architecture",
      "efficient"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.04342v1",
      "pdf": "https://arxiv.org/pdf/2601.04342v1",
      "project": "https://qualcomm-ai-research.github.io/rehyat"
    },
    "bibtex": ""
  },
  {
    "title": "Choreographing a World of Dynamic Objects",
    "authors": [
      "Yanzhe Lyu",
      "Chen Geng",
      "Karthik Dharmarajan",
      "Yunzhi Zhang",
      "Hadi Alzayer",
      "Shangzhe Wu",
      "Jiajun Wu"
    ],
    "abstract": "Dynamic objects in our physical 4D (3D + time) world are constantly evolving, deforming, and interacting with other objects, leading to diverse 4D scene dynamics. In this paper, we present a universal generative pipeline, CHORD, for CHOReographing Dynamic objects and scenes and synthesizing this type of phenomena. Traditional rule-based graphics pipelines to create these dynamics are based on category-specific heuristics, yet are labor-intensive and not scalable. Recent learning-based methods typically demand large-scale datasets, which may not cover all object categories in interest. Our approach instead inherits the universality from the video generative models by proposing a distillation-based pipeline to extract the rich Lagrangian motion information hidden in the Eulerian representations of 2D videos. Our method is universal, versatile, and category-agnostic. We demonstrate its effectiveness by conducting experiments to generate a diverse range of multi-body 4D dynamics, show its advantage compared to existing methods, and demonstrate its applicability in generating robotics manipulation policies. Project page: https://yanzhelyu.github.io/chord",
    "arxiv_url": "https://arxiv.org/abs/2601.04194v1",
    "pdf_url": "https://arxiv.org/pdf/2601.04194v1",
    "published_date": "2026-01-07",
    "categories": [
      "cs.CV",
      "cs.GR",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "distillation",
      "dit",
      "physical",
      "robotics",
      "dynamics"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.04194v1",
      "pdf": "https://arxiv.org/pdf/2601.04194v1",
      "project": "https://yanzhelyu.github.io/chord"
    },
    "bibtex": ""
  },
  {
    "title": "Diffusion-DRF: Differentiable Reward Flow for Video Diffusion Fine-Tuning",
    "authors": [
      "Yifan Wang",
      "Yanyu Li",
      "Sergey Tulyakov",
      "Yun Fu",
      "Anil Kag"
    ],
    "abstract": "Direct Preference Optimization (DPO) has recently improved Text-to-Video (T2V) generation by enhancing visual fidelity and text alignment. However, current methods rely on non-differentiable preference signals from human annotations or learned reward models. This reliance makes training label-intensive, bias-prone, and easy-to-game, which often triggers reward hacking and unstable training. We propose Diffusion-DRF, a differentiable reward flow for fine-tuning video diffusion models using a frozen, off-the-shelf Vision-Language Model (VLM) as a training-free critic. Diffusion-DRF directly backpropagates VLM feedback through the diffusion denoising chain, converting logit-level responses into token-aware gradients for optimization. We propose an automated, aspect-structured prompting pipeline to obtain reliable multi-dimensional VLM feedback, while gradient checkpointing enables efficient updates through the final denoising steps. Diffusion-DRF improves video quality and semantic alignment while mitigating reward hacking and collapse -- without additional reward models or preference datasets. It is model-agnostic and readily generalizes to other diffusion-based generative tasks.",
    "arxiv_url": "https://arxiv.org/abs/2601.04153v1",
    "pdf_url": "https://arxiv.org/pdf/2601.04153v1",
    "published_date": "2026-01-07",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "video diffusion",
      "dit",
      "text-to-video",
      "diffusion model",
      "denoising",
      "t2v",
      "efficient"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.04153v1",
      "pdf": "https://arxiv.org/pdf/2601.04153v1"
    },
    "bibtex": ""
  },
  {
    "title": "Apollo: Unified Multi-Task Audio-Video Joint Generation",
    "authors": [
      "Jun Wang",
      "Chunyu Qiang",
      "Yuxin Guo",
      "Yiran Wang",
      "Xijuan Zeng",
      "Feng Deng"
    ],
    "abstract": "Audio-video joint generation has progressed rapidly, yet substantial challenges still remain. Non-commercial approaches still suffer audio-visual asynchrony, poor lip-speech alignment, and unimodal degradation, which can be stemmed from weak audio-visual correspondence modeling, limited generalization, and scarce high-quality dense-caption data. To address these issues, we introduce Apollo and delve into three axes--model architecture, training strategy, and data curation. Architecturally, we adopt a single-tower design with unified DiT blocks and an Omni-Full Attention mechanism, achieving tight audio-visual alignment and strong scalability. Training-wise, we adopt a progressive multitask regime--random modality masking to joint optimization across tasks, and a multistage curriculum, yielding robust representations, strengthening A-V aligned world knowledge, and preventing unimodal collapse. For datasets, we present the first large-scale audio-video dataset with dense captions, and introduce a novel automated data-construction pipeline which annotates and filters millions of diverse, high-quality, strictly aligned audio-video-caption triplets. Building on this, Apollo scales to large datasets, delivering high-fidelity, semantically and temporally aligned, instruction-following generation in both joint and unimodal settings while generalizing robustly to out-of-distribution scenarios. Across tasks, it substantially outperforms prior methods by a large margin and achieves performance comparable to Veo 3, offering a unified, scalable path toward next-generation audio-video synthesis.",
    "arxiv_url": "https://arxiv.org/abs/2601.04151v2",
    "pdf_url": "https://arxiv.org/pdf/2601.04151v2",
    "published_date": "2026-01-07",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.MM",
      "cs.SD"
    ],
    "github_url": "",
    "keywords": [
      "video synthesis",
      "architecture",
      "dit"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.04151v2",
      "pdf": "https://arxiv.org/pdf/2601.04151v2"
    },
    "bibtex": ""
  },
  {
    "title": "Gen3R: 3D Scene Generation Meets Feed-Forward Reconstruction",
    "authors": [
      "Jiaxin Huang",
      "Yuanbo Yang",
      "Bangbang Yang",
      "Lin Ma",
      "Yuewen Ma",
      "Yiyi Liao"
    ],
    "abstract": "We present Gen3R, a method that bridges the strong priors of foundational reconstruction models and video diffusion models for scene-level 3D generation. We repurpose the VGGT reconstruction model to produce geometric latents by training an adapter on its tokens, which are regularized to align with the appearance latents of pre-trained video diffusion models. By jointly generating these disentangled yet aligned latents, Gen3R produces both RGB videos and corresponding 3D geometry, including camera poses, depth maps, and global point clouds. Experiments demonstrate that our approach achieves state-of-the-art results in single- and multi-image conditioned 3D scene generation. Additionally, our method can enhance the robustness of reconstruction by leveraging generative priors, demonstrating the mutual benefit of tightly coupling reconstruction and generative models.",
    "arxiv_url": "https://arxiv.org/abs/2601.04090v1",
    "pdf_url": "https://arxiv.org/pdf/2601.04090v1",
    "published_date": "2026-01-07",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "diffusion model",
      "video diffusion",
      "dit"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.04090v1",
      "pdf": "https://arxiv.org/pdf/2601.04090v1"
    },
    "bibtex": ""
  },
  {
    "title": "Mind the Generative Details: Direct Localized Detail Preference Optimization for Video Diffusion Models",
    "authors": [
      "Zitong Huang",
      "Kaidong Zhang",
      "Yukang Ding",
      "Chao Gao",
      "Rui Ding",
      "Ying Chen",
      "Wangmeng Zuo"
    ],
    "abstract": "Aligning text-to-video diffusion models with human preferences is crucial for generating high-quality videos. Existing Direct Preference Otimization (DPO) methods rely on multi-sample ranking and task-specific critic models, which is inefficient and often yields ambiguous global supervision. To address these limitations, we propose LocalDPO, a novel post-training framework that constructs localized preference pairs from real videos and optimizes alignment at the spatio-temporal region level. We design an automated pipeline to efficiently collect preference pair data that generates preference pairs with a single inference per prompt, eliminating the need for external critic models or manual annotation. Specifically, we treat high-quality real videos as positive samples and generate corresponding negatives by locally corrupting them with random spatio-temporal masks and restoring only the masked regions using the frozen base model. During training, we introduce a region-aware DPO loss that restricts preference learning to corrupted areas for rapid convergence. Experiments on Wan2.1 and CogVideoX demonstrate that LocalDPO consistently improves video fidelity, temporal coherence and human preference scores over other post-training approaches, establishing a more efficient and fine-grained paradigm for video generator alignment.",
    "arxiv_url": "https://arxiv.org/abs/2601.04068v2",
    "pdf_url": "https://arxiv.org/pdf/2601.04068v2",
    "published_date": "2026-01-07",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "diffusion model",
      "text-to-video",
      "video diffusion"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.04068v2",
      "pdf": "https://arxiv.org/pdf/2601.04068v2"
    },
    "bibtex": ""
  },
  {
    "title": "Thinking with Frames: Generative Video Distortion Evaluation via Frame Reward Model",
    "authors": [
      "Yuan Wang",
      "Borui Liao",
      "Huijuan Huang",
      "Jinda Lu",
      "Ouxiang Li",
      "Kuien Liu",
      "Meng Wang",
      "Xiang Wang"
    ],
    "abstract": "Recent advances in video reward models and post-training strategies have improved text-to-video (T2V) generation. While these models typically assess visual quality, motion quality, and text alignment, they often overlook key structural distortions, such as abnormal object appearances and interactions, which can degrade the overall quality of the generative video. To address this gap, we introduce REACT, a frame-level reward model designed specifically for structural distortions evaluation in generative videos. REACT assigns point-wise scores and attribution labels by reasoning over video frames, focusing on recognizing distortions. To support this, we construct a large-scale human preference dataset, annotated based on our proposed taxonomy of structural distortions, and generate additional data using a efficient Chain-of-Thought (CoT) synthesis pipeline. REACT is trained with a two-stage framework: ((1) supervised fine-tuning with masked loss for domain knowledge injection, followed by (2) reinforcement learning with Group Relative Policy Optimization (GRPO) and pairwise rewards to enhance reasoning capability and align output scores with human preferences. During inference, a dynamic sampling mechanism is introduced to focus on frames most likely to exhibit distortion. We also present REACT-Bench, a benchmark for generative video distortion evaluation. Experimental results demonstrate that REACT complements existing reward models in assessing structutal distortion, achieving both accurate quantitative evaluations and interpretable attribution analysis.",
    "arxiv_url": "https://arxiv.org/abs/2601.04033v1",
    "pdf_url": "https://arxiv.org/pdf/2601.04033v1",
    "published_date": "2026-01-07",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "dit",
      "text-to-video",
      "efficient",
      "evaluation",
      "t2v",
      "benchmark"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.04033v1",
      "pdf": "https://arxiv.org/pdf/2601.04033v1"
    },
    "bibtex": ""
  },
  {
    "title": "e5-omni: Explicit Cross-modal Alignment for Omni-modal Embeddings",
    "authors": [
      "Haonan Chen",
      "Sicheng Gao",
      "Radu Timofte",
      "Tetsuya Sakai",
      "Zhicheng Dou"
    ],
    "abstract": "Modern information systems often involve different types of items, e.g., a text query, an image, a video clip, or an audio segment. This motivates omni-modal embedding models that map heterogeneous modalities into a shared space for direct comparison. However, most recent omni-modal embeddings still rely heavily on implicit alignment inherited from pretrained vision-language model (VLM) backbones. In practice, this causes three common issues: (i) similarity logits have modality-dependent sharpness, so scores are not on a consistent scale; (ii) in-batch negatives become less effective over time because mixed-modality batches create an imbalanced hardness distribution; as a result, many negatives quickly become trivial and contribute little gradient; and (iii) embeddings across modalities show mismatched first- and second-order statistics, which makes rankings less stable. To tackle these problems, we propose e5-omni, a lightweight explicit alignment recipe that adapts off-the-shelf VLMs into robust omni-modal embedding models. e5-omni combines three simple components: (1) modality-aware temperature calibration to align similarity scales, (2) a controllable negative curriculum with debiasing to focus on confusing negatives while reducing the impact of false negatives, and (3) batch whitening with covariance regularization to better match cross-modal geometry in the shared embedding space. Experiments on MMEB-V2 and AudioCaps show consistent gains over strong bi-modal and omni-modal baselines, and the same recipe also transfers well to other VLM backbones. We release our model checkpoint at https://huggingface.co/Haon-Chen/e5-omni-7B.",
    "arxiv_url": "https://arxiv.org/abs/2601.03666v2",
    "pdf_url": "https://arxiv.org/pdf/2601.03666v2",
    "published_date": "2026-01-07",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "controllable"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.03666v2",
      "pdf": "https://arxiv.org/pdf/2601.03666v2",
      "huggingface": "https://huggingface.co/Haon-Chen/e5-omni-7B"
    },
    "bibtex": ""
  },
  {
    "title": "PhysVideoGenerator: Towards Physically Aware Video Generation via Latent Physics Guidance",
    "authors": [
      "Siddarth Nilol Kundur Satish",
      "Devesh Jaiswal",
      "Hongyu Chen",
      "Abhishek Bakshi"
    ],
    "abstract": "Current video generation models produce high-quality aesthetic videos but often struggle to learn representations of real-world physics dynamics, resulting in artifacts such as unnatural object collisions, inconsistent gravity, and temporal flickering. In this work, we propose PhysVideoGenerator, a proof-of-concept framework that explicitly embeds a learnable physics prior into the video generation process. We introduce a lightweight predictor network, PredictorP, which regresses high-level physical features extracted from a pre-trained Video Joint Embedding Predictive Architecture (V-JEPA 2) directly from noisy diffusion latents. These predicted physics tokens are injected into the temporal attention layers of a DiT-based generator (Latte) via a dedicated cross-attention mechanism. Our primary contribution is demonstrating the technical feasibility of this joint training paradigm: we show that diffusion latents contain sufficient information to recover V-JEPA 2 physical representations, and that multi-task optimization remains stable over training. This report documents the architectural design, technical challenges, and validation of training stability, establishing a foundation for future large-scale evaluation of physics-aware generative models.",
    "arxiv_url": "https://arxiv.org/abs/2601.03665v1",
    "pdf_url": "https://arxiv.org/pdf/2601.03665v1",
    "published_date": "2026-01-07",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "dit",
      "physical",
      "video generation",
      "evaluation",
      "architecture",
      "concept",
      "dynamics",
      "physics-aware",
      "physics"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.03665v1",
      "pdf": "https://arxiv.org/pdf/2601.03665v1"
    },
    "bibtex": ""
  },
  {
    "title": "VideoMemory: Toward Consistent Video Generation via Memory Integration",
    "authors": [
      "Jinsong Zhou",
      "Yihua Du",
      "Xinli Xu",
      "Luozhou Wang",
      "Zijie Zhuang",
      "Yehang Zhang",
      "Shuaibo Li",
      "Xiaojun Hu",
      "Bolan Su",
      "Ying-cong Chen"
    ],
    "abstract": "Maintaining consistent characters, props, and environments across multiple shots is a central challenge in narrative video generation. Existing models can produce high-quality short clips but often fail to preserve entity identity and appearance when scenes change or when entities reappear after long temporal gaps. We present VideoMemory, an entity-centric framework that integrates narrative planning with visual generation through a Dynamic Memory Bank. Given a structured script, a multi-agent system decomposes the narrative into shots, retrieves entity representations from memory, and synthesizes keyframes and videos conditioned on these retrieved states. The Dynamic Memory Bank stores explicit visual and semantic descriptors for characters, props, and backgrounds, and is updated after each shot to reflect story-driven changes while preserving identity. This retrieval-update mechanism enables consistent portrayal of entities across distant shots and supports coherent long-form generation. To evaluate this setting, we construct a 54-case multi-shot consistency benchmark covering character-, prop-, and background-persistent scenarios. Extensive experiments show that VideoMemory achieves strong entity-level coherence and high perceptual quality across diverse narrative sequences.",
    "arxiv_url": "https://arxiv.org/abs/2601.03655v1",
    "pdf_url": "https://arxiv.org/pdf/2601.03655v1",
    "published_date": "2026-01-07",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "dit",
      "identity",
      "long-form",
      "video generation",
      "benchmark"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.03655v1",
      "pdf": "https://arxiv.org/pdf/2601.03655v1"
    },
    "bibtex": ""
  },
  {
    "title": "A Novel Unified Approach to Deepfake Detection",
    "authors": [
      "Lord Sen",
      "Shyamapada Mukherjee"
    ],
    "abstract": "The advancements in the field of AI is increasingly giving rise to various threats. One of the most prominent of them is the synthesis and misuse of Deepfakes. To sustain trust in this digital age, detection and tagging of deepfakes is very necessary. In this paper, a novel architecture for Deepfake detection in images and videos is presented. The architecture uses cross attention between spatial and frequency domain features along with a blood detection module to classify an image as real or fake. This paper aims to develop a unified architecture and provide insights into each step. Though this approach we achieve results better than SOTA, specifically 99.80%, 99.88% AUC on FF++ and Celeb-DF upon using Swin Transformer and BERT and 99.55, 99.38 while using EfficientNet-B4 and BERT. The approach also generalizes very well achieving great cross dataset results as well.",
    "arxiv_url": "https://arxiv.org/abs/2601.03382v1",
    "pdf_url": "https://arxiv.org/pdf/2601.03382v1",
    "published_date": "2026-01-06",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "architecture"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.03382v1",
      "pdf": "https://arxiv.org/pdf/2601.03382v1"
    },
    "bibtex": ""
  },
  {
    "title": "A Versatile Multimodal Agent for Multimedia Content Generation",
    "authors": [
      "Daoan Zhang",
      "Wenlin Yao",
      "Xiaoyang Wang",
      "Yebowen Hu",
      "Jiebo Luo",
      "Dong Yu"
    ],
    "abstract": "With the advancement of AIGC (AI-generated content) technologies, an increasing number of generative models are revolutionizing fields such as video editing, music generation, and even film production. However, due to the limitations of current AIGC models, most models can only serve as individual components within specific application scenarios and are not capable of completing tasks end-to-end in real-world applications. In real-world applications, editing experts often work with a wide variety of images and video inputs, producing multimodal outputs -- a video typically includes audio, text, and other elements. This level of integration across multiple modalities is something current models are unable to achieve effectively. However, the rise of agent-based systems has made it possible to use AI tools to tackle complex content generation tasks. To deal with the complex scenarios, in this paper, we propose a MultiMedia-Agent designed to automate complex content creation. Our agent system includes a data generation pipeline, a tool library for content creation, and a set of metrics for evaluating preference alignment. Notably, we introduce the skill acquisition theory to model the training data curation and agent training. We designed a two-stage correlation strategy for plan optimization, including self-correlation and model preference correlation. Additionally, we utilized the generated plans to train the MultiMedia-Agent via a three stage approach including base/success plan finetune and preference optimization. The comparison results demonstrate that the our approaches are effective and the MultiMedia-Agent can generate better multimedia content compared to novel models.",
    "arxiv_url": "https://arxiv.org/abs/2601.03250v1",
    "pdf_url": "https://arxiv.org/pdf/2601.03250v1",
    "published_date": "2026-01-06",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "video editing",
      "film",
      "dit"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.03250v1",
      "pdf": "https://arxiv.org/pdf/2601.03250v1"
    },
    "bibtex": ""
  },
  {
    "title": "LTX-2: Efficient Joint Audio-Visual Foundation Model",
    "authors": [
      "Yoav HaCohen",
      "Benny Brazowski",
      "Nisan Chiprut",
      "Yaki Bitterman",
      "Andrew Kvochko",
      "Avishai Berkowitz",
      "Daniel Shalem",
      "Daphna Lifschitz",
      "Dudu Moshe",
      "Eitan Porat",
      "Eitan Richardson",
      "Guy Shiran",
      "Itay Chachy",
      "Jonathan Chetboun",
      "Michael Finkelson",
      "Michael Kupchick",
      "Nir Zabari",
      "Nitzan Guetta",
      "Noa Kotler",
      "Ofir Bibi",
      "Ori Gordon",
      "Poriya Panet",
      "Roi Benita",
      "Shahar Armon",
      "Victor Kulikov",
      "Yaron Inger",
      "Yonatan Shiftan",
      "Zeev Melumian",
      "Zeev Farbman"
    ],
    "abstract": "Recent text-to-video diffusion models can generate compelling video sequences, yet they remain silent -- missing the semantic, emotional, and atmospheric cues that audio provides. We introduce LTX-2, an open-source foundational model capable of generating high-quality, temporally synchronized audiovisual content in a unified manner. LTX-2 consists of an asymmetric dual-stream transformer with a 14B-parameter video stream and a 5B-parameter audio stream, coupled through bidirectional audio-video cross-attention layers with temporal positional embeddings and cross-modality AdaLN for shared timestep conditioning. This architecture enables efficient training and inference of a unified audiovisual model while allocating more capacity for video generation than audio generation. We employ a multilingual text encoder for broader prompt understanding and introduce a modality-aware classifier-free guidance (modality-CFG) mechanism for improved audiovisual alignment and controllability. Beyond generating speech, LTX-2 produces rich, coherent audio tracks that follow the characters, environment, style, and emotion of each scene -- complete with natural background and foley elements. In our evaluations, the model achieves state-of-the-art audiovisual quality and prompt adherence among open-source systems, while delivering results comparable to proprietary models at a fraction of their computational cost and inference time. All model weights and code are publicly released.",
    "arxiv_url": "https://arxiv.org/abs/2601.03233v1",
    "pdf_url": "https://arxiv.org/pdf/2601.03233v1",
    "published_date": "2026-01-06",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "video diffusion",
      "dit",
      "text-to-video",
      "video generation",
      "style",
      "evaluation",
      "diffusion model",
      "architecture",
      "efficient"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.03233v1",
      "pdf": "https://arxiv.org/pdf/2601.03233v1"
    },
    "bibtex": ""
  },
  {
    "title": "DiffBench Meets DiffAgent: End-to-End LLM-Driven Diffusion Acceleration Code Generation",
    "authors": [
      "Jiajun jiao",
      "Haowei Zhu",
      "Puyuan Yang",
      "Jianghui Wang",
      "Ji Liu",
      "Ziqiong Liu",
      "Dong Li",
      "Yuejian Fang",
      "Junhai Yong",
      "Bin Wang",
      "Emad Barsoum"
    ],
    "abstract": "Diffusion models have achieved remarkable success in image and video generation. However, their inherently multiple step inference process imposes substantial computational overhead, hindering real-world deployment. Accelerating diffusion models is therefore essential, yet determining how to combine multiple model acceleration techniques remains a significant challenge. To address this issue, we introduce a framework driven by large language models (LLMs) for automated acceleration code generation and evaluation. First, we present DiffBench, a comprehensive benchmark that implements a three stage automated evaluation pipeline across diverse diffusion architectures, optimization combinations and deployment scenarios. Second, we propose DiffAgent, an agent that generates optimal acceleration strategies and codes for arbitrary diffusion models. DiffAgent employs a closed-loop workflow in which a planning component and a debugging component iteratively refine the output of a code generation component, while a genetic algorithm extracts performance feedback from the execution environment to guide subsequent code refinements. We provide a detailed explanation of the DiffBench construction and the design principles underlying DiffAgent. Extensive experiments show that DiffBench offers a thorough evaluation of generated codes and that DiffAgent significantly outperforms existing LLMs in producing effective diffusion acceleration strategies.",
    "arxiv_url": "https://arxiv.org/abs/2601.03178v1",
    "pdf_url": "https://arxiv.org/pdf/2601.03178v1",
    "published_date": "2026-01-06",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "acceleration",
      "video generation",
      "evaluation",
      "diffusion model",
      "architecture",
      "benchmark"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.03178v1",
      "pdf": "https://arxiv.org/pdf/2601.03178v1"
    },
    "bibtex": ""
  },
  {
    "title": "DreamStyle: A Unified Framework for Video Stylization",
    "authors": [
      "Mengtian Li",
      "Jinshu Chen",
      "Songtao Zhao",
      "Wanquan Feng",
      "Pengqi Tu",
      "Qian He"
    ],
    "abstract": "Video stylization, an important downstream task of video generation models, has not yet been thoroughly explored. Its input style conditions typically include text, style image, and stylized first frame. Each condition has a characteristic advantage: text is more flexible, style image provides a more accurate visual anchor, and stylized first frame makes long-video stylization feasible. However, existing methods are largely confined to a single type of style condition, which limits their scope of application. Additionally, their lack of high-quality datasets leads to style inconsistency and temporal flicker. To address these limitations, we introduce DreamStyle, a unified framework for video stylization, supporting (1) text-guided, (2) style-image-guided, and (3) first-frame-guided video stylization, accompanied by a well-designed data curation pipeline to acquire high-quality paired video data. DreamStyle is built on a vanilla Image-to-Video (I2V) model and trained using a Low-Rank Adaptation (LoRA) with token-specific up matrices that reduces the confusion among different condition tokens. Both qualitative and quantitative evaluations demonstrate that DreamStyle is competent in all three video stylization tasks, and outperforms the competitors in style consistency and video quality.",
    "arxiv_url": "https://arxiv.org/abs/2601.02785v1",
    "pdf_url": "https://arxiv.org/pdf/2601.02785v1",
    "published_date": "2026-01-06",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "dit",
      "video generation",
      "image-to-video",
      "i2v",
      "evaluation",
      "style"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.02785v1",
      "pdf": "https://arxiv.org/pdf/2601.02785v1"
    },
    "bibtex": ""
  },
  {
    "title": "DreamLoop: Controllable Cinemagraph Generation from a Single Photograph",
    "authors": [
      "Aniruddha Mahapatra",
      "Long Mai",
      "Cusuh Ham",
      "Feng Liu"
    ],
    "abstract": "Cinemagraphs, which combine static photographs with selective, looping motion, offer unique artistic appeal. Generating them from a single photograph in a controllable manner is particularly challenging. Existing image-animation techniques are restricted to simple, low-frequency motions and operate only in narrow domains with repetitive textures like water and smoke. In contrast, large-scale video diffusion models are not tailored for cinemagraph constraints and lack the specialized data required to generate seamless, controlled loops. We present DreamLoop, a controllable video synthesis framework dedicated to generating cinemagraphs from a single photo without requiring any cinemagraph training data. Our key idea is to adapt a general video diffusion model by training it on two objectives: temporal bridging and motion conditioning. This strategy enables flexible cinemagraph generation. During inference, by using the input image as both the first- and last- frame condition, we enforce a seamless loop. By conditioning on static tracks, we maintain a static background. Finally, by providing a user-specified motion path for a target object, our method provides intuitive control over the animation's trajectory and timing. To our knowledge, DreamLoop is the first method to enable cinemagraph generation for general scenes with flexible and intuitive controls. We demonstrate that our method produces high-quality, complex cinemagraphs that align with user intent, outperforming existing approaches.",
    "arxiv_url": "https://arxiv.org/abs/2601.02646v1",
    "pdf_url": "https://arxiv.org/pdf/2601.02646v1",
    "published_date": "2026-01-06",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "controllable",
      "video diffusion",
      "dit",
      "trajectory",
      "video synthesis",
      "diffusion model"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.02646v1",
      "pdf": "https://arxiv.org/pdf/2601.02646v1"
    },
    "bibtex": ""
  },
  {
    "title": "VINO: A Unified Visual Generator with Interleaved OmniModal Context",
    "authors": [
      "Junyi Chen",
      "Tong He",
      "Zhoujie Fu",
      "Pengfei Wan",
      "Kun Gai",
      "Weicai Ye"
    ],
    "abstract": "We present VINO, a unified visual generator that performs image and video generation and editing within a single framework. Instead of relying on task-specific models or independent modules for each modality, VINO uses a shared diffusion backbone that conditions on text, images and videos, enabling a broad range of visual creation and editing tasks under one model. Specifically, VINO couples a vision-language model (VLM) with a Multimodal Diffusion Transformer (MMDiT), where multimodal inputs are encoded as interleaved conditioning tokens, and then used to guide the diffusion process. This design supports multi-reference grounding, long-form instruction following, and coherent identity preservation across static and dynamic content, while avoiding modality-specific architectural components. To train such a unified system, we introduce a multi-stage training pipeline that progressively expands a video generation base model into a unified, multi-task generator capable of both image and video input and output. Across diverse generation and editing benchmarks, VINO demonstrates strong visual quality, faithful instruction following, improved reference and attribute preservation, and more controllable multi-identity edits. Our results highlight a practical path toward scalable unified visual generation, and the promise of interleaved, in-context computation as a foundation for general-purpose visual creation.",
    "arxiv_url": "https://arxiv.org/abs/2601.02358v2",
    "pdf_url": "https://arxiv.org/pdf/2601.02358v2",
    "published_date": "2026-01-05",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "controllable",
      "dit",
      "identity",
      "diffusion transformer",
      "long-form",
      "video generation",
      "benchmark"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.02358v2",
      "pdf": "https://arxiv.org/pdf/2601.02358v2"
    },
    "bibtex": ""
  },
  {
    "title": "NextFlow: Unified Sequential Modeling Activates Multimodal Understanding and Generation",
    "authors": [
      "Huichao Zhang",
      "Liao Qu",
      "Yiheng Liu",
      "Hang Chen",
      "Yangyang Song",
      "Yongsheng Dong",
      "Shikun Sun",
      "Xian Li",
      "Xu Wang",
      "Yi Jiang",
      "Hu Ye",
      "Bo Chen",
      "Yiming Gao",
      "Peng Liu",
      "Akide Liu",
      "Zhipeng Yang",
      "Qili Deng",
      "Linjie Xing",
      "Jiyang Liu",
      "Zhao Wang",
      "Yang Zhou",
      "Mingcong Liu",
      "Yi Zhang",
      "Qian He",
      "Xiwei Hu",
      "Zhongqi Qi",
      "Jie Shao",
      "Zhiye Fu",
      "Shuai Wang",
      "Fangmin Chen",
      "Xuezhi Chai",
      "Zhihua Wu",
      "Yitong Wang",
      "Zehuan Yuan",
      "Daniel K. Du",
      "Xinglong Wu"
    ],
    "abstract": "We present NextFlow, a unified decoder-only autoregressive transformer trained on 6 trillion interleaved text-image discrete tokens. By leveraging a unified vision representation within a unified autoregressive architecture, NextFlow natively activates multimodal understanding and generation capabilities, unlocking abilities of image editing, interleaved content and video generation. Motivated by the distinct nature of modalities - where text is strictly sequential and images are inherently hierarchical - we retain next-token prediction for text but adopt next-scale prediction for visual generation. This departs from traditional raster-scan methods, enabling the generation of 1024x1024 images in just 5 seconds - orders of magnitude faster than comparable AR models. We address the instabilities of multi-scale generation through a robust training recipe. Furthermore, we introduce a prefix-tuning strategy for reinforcement learning. Experiments demonstrate that NextFlow achieves state-of-the-art performance among unified models and rivals specialized diffusion baselines in visual quality.",
    "arxiv_url": "https://arxiv.org/abs/2601.02204v1",
    "pdf_url": "https://arxiv.org/pdf/2601.02204v1",
    "published_date": "2026-01-05",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "architecture",
      "video generation",
      "autoregressive",
      "dit"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.02204v1",
      "pdf": "https://arxiv.org/pdf/2601.02204v1"
    },
    "bibtex": ""
  },
  {
    "title": "SingingBot: An Avatar-Driven System for Robotic Face Singing Performance",
    "authors": [
      "Zhuoxiong Xu",
      "Xuanchen Li",
      "Yuhao Cheng",
      "Fei Xu",
      "Yichao Yan",
      "Xiaokang Yang"
    ],
    "abstract": "Equipping robotic faces with singing capabilities is crucial for empathetic Human-Robot Interaction. However, existing robotic face driving research primarily focuses on conversations or mimicking static expressions, struggling to meet the high demands for continuous emotional expression and coherence in singing. To address this, we propose a novel avatar-driven framework for appealing robotic singing. We first leverage portrait video generation models embedded with extensive human priors to synthesize vivid singing avatars, providing reliable expression and emotion guidance. Subsequently, these facial features are transferred to the robot via semantic-oriented mapping functions that span a wide expression space. Furthermore, to quantitatively evaluate the emotional richness of robotic singing, we propose the Emotion Dynamic Range metric to measure the emotional breadth within the Valence-Arousal space, revealing that a broad emotional spectrum is crucial for appealing performances. Comprehensive experiments prove that our method achieves rich emotional expressions while maintaining lip-audio synchronization, significantly outperforming existing approaches.",
    "arxiv_url": "https://arxiv.org/abs/2601.02125v1",
    "pdf_url": "https://arxiv.org/pdf/2601.02125v1",
    "published_date": "2026-01-05",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "avatar",
      "video generation"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.02125v1",
      "pdf": "https://arxiv.org/pdf/2601.02125v1"
    },
    "bibtex": ""
  },
  {
    "title": "MagicFight: Personalized Martial Arts Combat Video Generation",
    "authors": [
      "Jiancheng Huang",
      "Mingfu Yan",
      "Songyan Chen",
      "Yi Huang",
      "Shifeng Chen"
    ],
    "abstract": "Amid the surge in generic text-to-video generation, the field of personalized human video generation has witnessed notable advancements, primarily concentrated on single-person scenarios. However, to our knowledge, the domain of two-person interactions, particularly in the context of martial arts combat, remains uncharted. We identify a significant gap: existing models for single-person dancing generation prove insufficient for capturing the subtleties and complexities of two engaged fighters, resulting in challenges such as identity confusion, anomalous limbs, and action mismatches. To address this, we introduce a pioneering new task, Personalized Martial Arts Combat Video Generation. Our approach, MagicFight, is specifically crafted to overcome these hurdles. Given this pioneering task, we face a lack of appropriate datasets. Thus, we generate a bespoke dataset using the game physics engine Unity, meticulously crafting a multitude of 3D characters, martial arts moves, and scenes designed to represent the diversity of combat. MagicFight refines and adapts existing models and strategies to generate high-fidelity two-person combat videos that maintain individual identities and ensure seamless, coherent action sequences, thereby laying the groundwork for future innovations in the realm of interactive video content creation.   Website: https://MingfuYAN.github.io/MagicFight/   Dataset: https://huggingface.co/datasets/MingfuYAN/KungFu-Fiesta",
    "arxiv_url": "https://arxiv.org/abs/2601.02107v1",
    "pdf_url": "https://arxiv.org/pdf/2601.02107v1",
    "published_date": "2026-01-05",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "interactive",
      "identity",
      "text-to-video",
      "video generation",
      "physics"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.02107v1",
      "pdf": "https://arxiv.org/pdf/2601.02107v1",
      "project": "https://MingfuYAN.github.io/MagicFight",
      "dataset": "https://huggingface.co/datasets/MingfuYAN/KungFu-Fiesta"
    },
    "bibtex": ""
  },
  {
    "title": "MotionAdapter: Video Motion Transfer via Content-Aware Attention Customization",
    "authors": [
      "Zhexin Zhang",
      "Yifeng Zhu",
      "Yangyang Xu",
      "Long Chen",
      "Yong Du",
      "Shengfeng He",
      "Jun Yu"
    ],
    "abstract": "Recent advances in diffusion-based text-to-video models, particularly those built on the diffusion transformer architecture, have achieved remarkable progress in generating high-quality and temporally coherent videos. However, transferring complex motions between videos remains challenging. In this work, we present MotionAdapter, a content-aware motion transfer framework that enables robust and semantically aligned motion transfer within DiT-based T2V models. Our key insight is that effective motion transfer requires \\romannumeral1) explicit disentanglement of motion from appearance and \\romannumeral 2) adaptive customization of motion to target content. MotionAdapter first isolates motion by analyzing cross-frame attention within 3D full-attention modules to extract attention-derived motion fields. To bridge the semantic gap between reference and target videos, we further introduce a DINO-guided motion customization module that rearranges and refines motion fields based on content correspondences. The customized motion field is then used to guide the DiT denoising process, ensuring that the synthesized video inherits the reference motion while preserving target appearance and semantics. Extensive experiments demonstrate that MotionAdapter outperforms state-of-the-art methods in both qualitative and quantitative evaluations. Moreover, MotionAdapter naturally supports complex motion transfer and motion editing tasks such as zooming.",
    "arxiv_url": "https://arxiv.org/abs/2601.01955v1",
    "pdf_url": "https://arxiv.org/pdf/2601.01955v1",
    "published_date": "2026-01-05",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "dit",
      "diffusion transformer",
      "text-to-video",
      "evaluation",
      "architecture",
      "denoising",
      "t2v",
      "customization"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.01955v1",
      "pdf": "https://arxiv.org/pdf/2601.01955v1"
    },
    "bibtex": ""
  },
  {
    "title": "MM-Sonate: Multimodal Controllable Audio-Video Generation with Zero-Shot Voice Cloning",
    "authors": [
      "Chunyu Qiang",
      "Jun Wang",
      "Xiaopeng Wang",
      "Kang Yin",
      "Yuxin Guo"
    ],
    "abstract": "Joint audio-video generation aims to synthesize synchronized multisensory content, yet current unified models struggle with fine-grained acoustic control, particularly for identity-preserving speech. Existing approaches either suffer from temporal misalignment due to cascaded generation or lack the capability to perform zero-shot voice cloning within a joint synthesis framework. In this work, we present MM-Sonate, a multimodal flow-matching framework that unifies controllable audio-video joint generation with zero-shot voice cloning capabilities. Unlike prior works that rely on coarse semantic descriptions, MM-Sonate utilizes a unified instruction-phoneme input to enforce strict linguistic and temporal alignment. To enable zero-shot voice cloning, we introduce a timbre injection mechanism that effectively decouples speaker identity from linguistic content. Furthermore, addressing the limitations of standard classifier-free guidance in multimodal settings, we propose a noise-based negative conditioning strategy that utilizes natural noise priors to significantly enhance acoustic fidelity. Empirical evaluations demonstrate that MM-Sonate establishes new state-of-the-art performance in joint generation benchmarks, significantly outperforming baselines in lip synchronization and speech intelligibility, while achieving voice cloning fidelity comparable to specialized Text-to-Speech systems.",
    "arxiv_url": "https://arxiv.org/abs/2601.01568v2",
    "pdf_url": "https://arxiv.org/pdf/2601.01568v2",
    "published_date": "2026-01-04",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.CV",
      "cs.MM",
      "eess.AS"
    ],
    "github_url": "",
    "keywords": [
      "controllable",
      "dit",
      "identity",
      "video generation",
      "evaluation",
      "benchmark"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.01568v2",
      "pdf": "https://arxiv.org/pdf/2601.01568v2"
    },
    "bibtex": ""
  },
  {
    "title": "DrivingGen: A Comprehensive Benchmark for Generative Video World Models in Autonomous Driving",
    "authors": [
      "Yang Zhou",
      "Hao Shao",
      "Letian Wang",
      "Zhuofan Zong",
      "Hongsheng Li",
      "Steven L. Waslander"
    ],
    "abstract": "Video generation models, as one form of world models, have emerged as one of the most exciting frontiers in AI, promising agents the ability to imagine the future by modeling the temporal evolution of complex scenes. In autonomous driving, this vision gives rise to driving world models: generative simulators that imagine ego and agent futures, enabling scalable simulation, safe testing of corner cases, and rich synthetic data generation. Yet, despite fast-growing research activity, the field lacks a rigorous benchmark to measure progress and guide priorities. Existing evaluations remain limited: generic video metrics overlook safety-critical imaging factors; trajectory plausibility is rarely quantified; temporal and agent-level consistency is neglected; and controllability with respect to ego conditioning is ignored. Moreover, current datasets fail to cover the diversity of conditions required for real-world deployment. To address these gaps, we present DrivingGen, the first comprehensive benchmark for generative driving world models. DrivingGen combines a diverse evaluation dataset curated from both driving datasets and internet-scale video sources, spanning varied weather, time of day, geographic regions, and complex maneuvers, with a suite of new metrics that jointly assess visual realism, trajectory plausibility, temporal coherence, and controllability. Benchmarking 14 state-of-the-art models reveals clear trade-offs: general models look better but break physics, while driving-specific ones capture motion realistically but lag in visual quality. DrivingGen offers a unified evaluation framework to foster reliable, controllable, and deployable driving world models, enabling scalable simulation, planning, and data-driven decision-making.",
    "arxiv_url": "https://arxiv.org/abs/2601.01528v1",
    "pdf_url": "https://arxiv.org/pdf/2601.01528v1",
    "published_date": "2026-01-04",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "controllable",
      "dit",
      "world model",
      "trajectory",
      "video generation",
      "autonomous driving",
      "evaluation",
      "simulation",
      "benchmark",
      "physics"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.01528v1",
      "pdf": "https://arxiv.org/pdf/2601.01528v1"
    },
    "bibtex": ""
  },
  {
    "title": "DreamID-V:Bridging the Image-to-Video Gap for High-Fidelity Face Swapping via Diffusion Transformer",
    "authors": [
      "Xu Guo",
      "Fulong Ye",
      "Xinghui Li",
      "Pengqi Tu",
      "Pengze Zhang",
      "Qichao Sun",
      "Songtao Zhao",
      "Xiangwang Hou",
      "Qian He"
    ],
    "abstract": "Video Face Swapping (VFS) requires seamlessly injecting a source identity into a target video while meticulously preserving the original pose, expression, lighting, background, and dynamic information. Existing methods struggle to maintain identity similarity and attribute preservation while preserving temporal consistency. To address the challenge, we propose a comprehensive framework to seamlessly transfer the superiority of Image Face Swapping (IFS) to the video domain. We first introduce a novel data pipeline SyncID-Pipe that pre-trains an Identity-Anchored Video Synthesizer and combines it with IFS models to construct bidirectional ID quadruplets for explicit supervision. Building upon paired data, we propose the first Diffusion Transformer-based framework DreamID-V, employing a core Modality-Aware Conditioning module to discriminatively inject multi-model conditions. Meanwhile, we propose a Synthetic-to-Real Curriculum mechanism and an Identity-Coherence Reinforcement Learning strategy to enhance visual realism and identity consistency under challenging scenarios. To address the issue of limited benchmarks, we introduce IDBench-V, a comprehensive benchmark encompassing diverse scenes. Extensive experiments demonstrate DreamID-V outperforms state-of-the-art methods and further exhibits exceptional versatility, which can be seamlessly adapted to various swap-related tasks.",
    "arxiv_url": "https://arxiv.org/abs/2601.01425v1",
    "pdf_url": "https://arxiv.org/pdf/2601.01425v1",
    "published_date": "2026-01-04",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "temporal consistency",
      "dit",
      "identity",
      "diffusion transformer",
      "image-to-video",
      "benchmark"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.01425v1",
      "pdf": "https://arxiv.org/pdf/2601.01425v1"
    },
    "bibtex": ""
  },
  {
    "title": "Slot-ID: Identity-Preserving Video Generation from Reference Videos via Slot-Based Temporal Identity Encoding",
    "authors": [
      "Yixuan Lai",
      "He Wang",
      "Kun Zhou",
      "Tianjia Shao"
    ],
    "abstract": "Producing prompt-faithful videos that preserve a user-specified identity remains challenging: models need to extrapolate facial dynamics from sparse reference while balancing the tension between identity preservation and motion naturalness. Conditioning on a single image completely ignores the temporal signature, which leads to pose-locked motions, unnatural warping, and \"average\" faces when viewpoints and expressions change. To this end, we introduce an identity-conditioned variant of a diffusion-transformer video generator which uses a short reference video rather than a single portrait. Our key idea is to incorporate the dynamics in the reference. A short clip reveals subject-specific patterns, e.g., how smiles form, across poses and lighting. From this clip, a Sinkhorn-routed encoder learns compact identity tokens that capture characteristic dynamics while remaining pretrained backbone-compatible. Despite adding only lightweight conditioning, the approach consistently improves identity retention under large pose changes and expressive facial behavior, while maintaining prompt faithfulness and visual realism across diverse subjects and prompts.",
    "arxiv_url": "https://arxiv.org/abs/2601.01352v1",
    "pdf_url": "https://arxiv.org/pdf/2601.01352v1",
    "published_date": "2026-01-04",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "dynamics",
      "identity",
      "video generation",
      "dit"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.01352v1",
      "pdf": "https://arxiv.org/pdf/2601.01352v1"
    },
    "bibtex": ""
  },
  {
    "title": "Judge Model for Large-scale Multimodality Benchmarks",
    "authors": [
      "Min-Han Shih",
      "Yu-Hsin Wu",
      "Yu-Wei Chen"
    ],
    "abstract": "We propose a dedicated multimodal Judge Model designed to provide reliable, explainable evaluation across a diverse suite of tasks. Our benchmark spans text, audio, image, and video modalities, drawing from carefully sampled public datasets with fixed seeds to ensure reproducibility and minimize train test leakage. Instead of simple scoring, our framework aggregates multimodal judgments, analyzes the quality and reasoning consistency of model outputs, and generates diagnostic feedback. We evaluate several MLLMs, including Gemini 2.5, Phi 4, and Qwen 2.5, across 280 multimodal samples and compare judge model assessments with human annotators. Results show strong alignment between the Judge Model and human scores, demonstrating its potential as a scalable, interpretable evaluation pipeline for future multimodal AI research.",
    "arxiv_url": "https://arxiv.org/abs/2601.06106v1",
    "pdf_url": "https://arxiv.org/pdf/2601.06106v1",
    "published_date": "2026-01-03",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CV",
      "cs.MA"
    ],
    "github_url": "",
    "keywords": [
      "benchmark",
      "evaluation"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.06106v1",
      "pdf": "https://arxiv.org/pdf/2601.06106v1"
    },
    "bibtex": ""
  },
  {
    "title": "VEAT Quantifies Implicit Associations in Text-to-Video Generator Sora and Reveals Challenges in Bias Mitigation",
    "authors": [
      "Yongxu Sun",
      "Michael Saxon",
      "Ian Yang",
      "Anna-Maria Gueorguieva",
      "Aylin Caliskan"
    ],
    "abstract": "Text-to-Video (T2V) generators such as Sora raise concerns about whether generated content reflects societal bias. We extend embedding-association tests from words and images to video by introducing the Video Embedding Association Test (VEAT) and Single-Category VEAT (SC-VEAT). We validate these methods by reproducing the direction and magnitude of associations from widely used baselines, including Implicit Association Test (IAT) scenarios and OASIS image categories. We then quantify race (African American vs. European American) and gender (women vs. men) associations with valence (pleasant vs. unpleasant) across 17 occupations and 7 awards. Sora videos associate European Americans and women more with pleasantness (both d>0.8). Effect sizes correlate with real-world demographic distributions: percent men and White in occupations (r=0.93, r=0.83) and percent male and non-Black among award recipients (r=0.88, r=0.99). Applying explicit debiasing prompts generally reduces effect-size magnitudes, but can backfire: two Black-associated occupations (janitor, postal service) become more Black-associated after debiasing. Together, these results reveal that easily accessible T2V generators can actually amplify representational harms if not rigorously evaluated and responsibly deployed.",
    "arxiv_url": "https://arxiv.org/abs/2601.00996v1",
    "pdf_url": "https://arxiv.org/pdf/2601.00996v1",
    "published_date": "2026-01-02",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "text-to-video",
      "t2v"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.00996v1",
      "pdf": "https://arxiv.org/pdf/2601.00996v1"
    },
    "bibtex": ""
  },
  {
    "title": "PhyEduVideo: A Benchmark for Evaluating Text-to-Video Models for Physics Education",
    "authors": [
      "Megha Mariam K. M",
      "Aditya Arun",
      "Zakaria Laskar",
      "C. V. Jawahar"
    ],
    "abstract": "Generative AI models, particularly Text-to-Video (T2V) systems, offer a promising avenue for transforming science education by automating the creation of engaging and intuitive visual explanations. In this work, we take a first step toward evaluating their potential in physics education by introducing a dedicated benchmark for explanatory video generation. The benchmark is designed to assess how well T2V models can convey core physics concepts through visual illustrations. Each physics concept in our benchmark is decomposed into granular teaching points, with each point accompanied by a carefully crafted prompt intended for visual explanation of the teaching point. T2V models are evaluated on their ability to generate accurate videos in response to these prompts. Our aim is to systematically explore the feasibility of using T2V models to generate high-quality, curriculum-aligned educational content-paving the way toward scalable, accessible, and personalized learning experiences powered by AI. Our evaluation reveals that current models produce visually coherent videos with smooth motion and minimal flickering, yet their conceptual accuracy is less reliable. Performance in areas such as mechanics, fluids, and optics is encouraging, but models struggle with electromagnetism and thermodynamics, where abstract interactions are harder to depict. These findings underscore the gap between visual quality and conceptual correctness in educational video generation. We hope this benchmark helps the community close that gap and move toward T2V systems that can deliver accurate, curriculum-aligned physics content at scale. The benchmark and accompanying codebase are publicly available at https://github.com/meghamariamkm/PhyEduVideo.",
    "arxiv_url": "https://arxiv.org/abs/2601.00943v1",
    "pdf_url": "https://arxiv.org/pdf/2601.00943v1",
    "published_date": "2026-01-02",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/meghamariamkm/PhyEduVideo",
    "keywords": [
      "education",
      "text-to-video",
      "video generation",
      "evaluation",
      "concept",
      "dynamics",
      "t2v",
      "benchmark",
      "physics"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.00943v1",
      "pdf": "https://arxiv.org/pdf/2601.00943v1",
      "github": "https://github.com/meghamariamkm/PhyEduVideo"
    },
    "bibtex": ""
  },
  {
    "title": "Investigating the Viability of Employing Multi-modal Large Language Models in the Context of Audio Deepfake Detection",
    "authors": [
      "Akanksha Chuchra",
      "Shukesh Reddy",
      "Sudeepta Mishra",
      "Abhijit Das",
      "Abhinav Dhall"
    ],
    "abstract": "While Vision-Language Models (VLMs) and Multimodal Large Language Models (MLLMs) have shown strong generalisation in detecting image and video deepfakes, their use for audio deepfake detection remains largely unexplored. In this work, we aim to explore the potential of MLLMs for audio deepfake detection. Combining audio inputs with a range of text prompts as queries to find out the viability of MLLMs to learn robust representations across modalities for audio deepfake detection. Therefore, we attempt to explore text-aware and context-rich, question-answer based prompts with binary decisions. We hypothesise that such a feature-guided reasoning will help in facilitating deeper multimodal understanding and enable robust feature learning for audio deepfake detection. We evaluate the performance of two MLLMs, Qwen2-Audio-7B-Instruct and SALMONN, in two evaluation modes: (a) zero-shot and (b) fine-tuned. Our experiments demonstrate that combining audio with a multi-prompt approach could be a viable way forward for audio deepfake detection. Our experiments show that the models perform poorly without task-specific training and struggle to generalise to out-of-domain data. However, they achieve good performance on in-domain data with minimal supervision, indicating promising potential for audio deepfake detection.",
    "arxiv_url": "https://arxiv.org/abs/2601.00777v1",
    "pdf_url": "https://arxiv.org/pdf/2601.00777v1",
    "published_date": "2026-01-02",
    "categories": [
      "cs.SD",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "evaluation",
      "multi-modal"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.00777v1",
      "pdf": "https://arxiv.org/pdf/2601.00777v1"
    },
    "bibtex": ""
  },
  {
    "title": "Pixel-to-4D: Camera-Controlled Image-to-Video Generation with Dynamic 3D Gaussians",
    "authors": [
      "Melonie de Almeida",
      "Daniela Ivanova",
      "Tong Shi",
      "John H. Williamson",
      "Paul Henderson"
    ],
    "abstract": "Humans excel at forecasting the future dynamics of a scene given just a single image. Video generation models that can mimic this ability are an essential component for intelligent systems. Recent approaches have improved temporal coherence and 3D consistency in single-image-conditioned video generation. However, these methods often lack robust user controllability, such as modifying the camera path, limiting their applicability in real-world applications. Most existing camera-controlled image-to-video models struggle with accurately modeling camera motion, maintaining temporal consistency, and preserving geometric integrity. Leveraging explicit intermediate 3D representations offers a promising solution by enabling coherent video generation aligned with a given camera trajectory. Although these methods often use 3D point clouds to render scenes and introduce object motion in a later stage, this two-step process still falls short in achieving full temporal consistency, despite allowing precise control over camera movement. We propose a novel framework that constructs a 3D Gaussian scene representation and samples plausible object motion, given a single image in a single forward pass. This enables fast, camera-guided video generation without the need for iterative denoising to inject object motion into render frames. Extensive experiments on the KITTI, Waymo, RealEstate10K and DL3DV-10K datasets demonstrate that our method achieves state-of-the-art video quality and inference efficiency. The project page is available at https://melonienimasha.github.io/Pixel-to-4D-Website.",
    "arxiv_url": "https://arxiv.org/abs/2601.00678v1",
    "pdf_url": "https://arxiv.org/pdf/2601.00678v1",
    "published_date": "2026-01-02",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "temporal consistency",
      "dit",
      "trajectory",
      "video generation",
      "image-to-video",
      "dynamics",
      "denoising"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.00678v1",
      "pdf": "https://arxiv.org/pdf/2601.00678v1",
      "project": "https://melonienimasha.github.io/Pixel-to-4D-Website"
    },
    "bibtex": ""
  },
  {
    "title": "All-in-One Video Restoration under Smoothly Evolving Unknown Weather Degradations",
    "authors": [
      "Wenrui Li",
      "Hongtao Chen",
      "Yao Xiao",
      "Wangmeng Zuo",
      "Jiantao Zhou",
      "Yonghong Tian",
      "Xiaopeng Fan"
    ],
    "abstract": "All-in-one image restoration aims to recover clean images from diverse unknown degradations using a single model. But extending this task to videos faces unique challenges. Existing approaches primarily focus on frame-wise degradation variation, overlooking the temporal continuity that naturally exists in real-world degradation processes. In practice, degradation types and intensities evolve smoothly over time, and multiple degradations may coexist or transition gradually. In this paper, we introduce the Smoothly Evolving Unknown Degradations (SEUD) scenario, where both the active degradation set and degradation intensity change continuously over time. To support this scenario, we design a flexible synthesis pipeline that generates temporally coherent videos with single, compound, and evolving degradations. To address the challenges in the SEUD scenario, we propose an all-in-One Recurrent Conditional and Adaptive prompting Network (ORCANet). First, a Coarse Intensity Estimation Dehazing (CIED) module estimates haze intensity using physical priors and provides coarse dehazed features as initialization. Second, a Flow Prompt Generation (FPG) module extracts degradation features. FPG generates both static prompts that capture segment-level degradation types and dynamic prompts that adapt to frame-level intensity variations. Furthermore, a label-aware supervision mechanism improves the discriminability of static prompt representations under different degradations. Extensive experiments show that ORCANet achieves superior restoration quality, temporal consistency, and robustness over image and video-based baselines. Code is available at https://github.com/Friskknight/ORCANet-SEUD.",
    "arxiv_url": "https://arxiv.org/abs/2601.00533v2",
    "pdf_url": "https://arxiv.org/pdf/2601.00533v2",
    "published_date": "2026-01-02",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/Friskknight/ORCANet-SEUD",
    "keywords": [
      "video restoration",
      "physical",
      "temporal consistency",
      "dit"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.00533v2",
      "pdf": "https://arxiv.org/pdf/2601.00533v2",
      "github": "https://github.com/Friskknight/ORCANet-SEUD"
    },
    "bibtex": ""
  },
  {
    "title": "MotionPhysics: Learnable Motion Distillation for Text-Guided Simulation",
    "authors": [
      "Miaowei Wang",
      "Jakub Zadrony",
      "Oisin Mac Aodha",
      "Amir Vaxman"
    ],
    "abstract": "Accurately simulating existing 3D objects and a wide variety of materials often demands expert knowledge and time-consuming physical parameter tuning to achieve the desired dynamic behavior. We introduce MotionPhysics, an end-to-end differentiable framework that infers plausible physical parameters from a user-provided natural language prompt for a chosen 3D scene of interest, removing the need for guidance from ground-truth trajectories or annotated videos. Our approach first utilizes a multimodal large language model to estimate material parameter values, which are constrained to lie within plausible ranges. We further propose a learnable motion distillation loss that extracts robust motion priors from pretrained video diffusion models while minimizing appearance and geometry inductive biases to guide the simulation. We evaluate MotionPhysics across more than thirty scenarios, including real-world, human-designed, and AI-generated 3D objects, spanning a wide range of materials such as elastic solids, metals, foams, sand, and both Newtonian and non-Newtonian fluids. We demonstrate that MotionPhysics produces visually realistic dynamic simulations guided by natural language, surpassing the state of the art while automatically determining physically plausible parameters. The code and project page are available at: https://wangmiaowei.github.io/MotionPhysics.github.io/.",
    "arxiv_url": "https://arxiv.org/abs/2601.00504v1",
    "pdf_url": "https://arxiv.org/pdf/2601.00504v1",
    "published_date": "2026-01-01",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "video diffusion",
      "distillation",
      "physical",
      "diffusion model",
      "simulation",
      "physics"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.00504v1",
      "pdf": "https://arxiv.org/pdf/2601.00504v1",
      "project": "https://wangmiaowei.github.io/MotionPhysics.github.io"
    },
    "bibtex": ""
  },
  {
    "title": "NeoVerse: Enhancing 4D World Model with in-the-wild Monocular Videos",
    "authors": [
      "Yuxue Yang",
      "Lue Fan",
      "Ziqi Shi",
      "Junran Peng",
      "Feng Wang",
      "Zhaoxiang Zhang"
    ],
    "abstract": "In this paper, we propose NeoVerse, a versatile 4D world model that is capable of 4D reconstruction, novel-trajectory video generation, and rich downstream applications. We first identify a common limitation of scalability in current 4D world modeling methods, caused either by expensive and specialized multi-view 4D data or by cumbersome training pre-processing. In contrast, our NeoVerse is built upon a core philosophy that makes the full pipeline scalable to diverse in-the-wild monocular videos. Specifically, NeoVerse features pose-free feed-forward 4D reconstruction, online monocular degradation pattern simulation, and other well-aligned techniques. These designs empower NeoVerse with versatility and generalization to various domains. Meanwhile, NeoVerse achieves state-of-the-art performance in standard reconstruction and generation benchmarks. Our project page is available at https://neoverse-4d.github.io",
    "arxiv_url": "https://arxiv.org/abs/2601.00393v1",
    "pdf_url": "https://arxiv.org/pdf/2601.00393v1",
    "published_date": "2026-01-01",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "world model",
      "trajectory",
      "video generation",
      "simulation",
      "benchmark"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.00393v1",
      "pdf": "https://arxiv.org/pdf/2601.00393v1",
      "project": "https://neoverse-4d.github.io"
    },
    "bibtex": ""
  },
  {
    "title": "SpaceTimePilot: Generative Rendering of Dynamic Scenes Across Space and Time",
    "authors": [
      "Zhening Huang",
      "Hyeonho Jeong",
      "Xuelin Chen",
      "Yulia Gryaditskaya",
      "Tuanfeng Y. Wang",
      "Joan Lasenby",
      "Chun-Hao Huang"
    ],
    "abstract": "We present SpaceTimePilot, a video diffusion model that disentangles space and time for controllable generative rendering. Given a monocular video, SpaceTimePilot can independently alter the camera viewpoint and the motion sequence within the generative process, re-rendering the scene for continuous and arbitrary exploration across space and time. To achieve this, we introduce an effective animation time-embedding mechanism in the diffusion process, allowing explicit control of the output video's motion sequence with respect to that of the source video. As no datasets provide paired videos of the same dynamic scene with continuous temporal variations, we propose a simple yet effective temporal-warping training scheme that repurposes existing multi-view datasets to mimic temporal differences. This strategy effectively supervises the model to learn temporal control and achieve robust space-time disentanglement. To further enhance the precision of dual control, we introduce two additional components: an improved camera-conditioning mechanism that allows altering the camera from the first frame, and CamxTime, the first synthetic space-and-time full-coverage rendering dataset that provides fully free space-time video trajectories within a scene. Joint training on the temporal-warping scheme and the CamxTime dataset yields more precise temporal control. We evaluate SpaceTimePilot on both real-world and synthetic data, demonstrating clear space-time disentanglement and strong results compared to prior work. Project page: https://zheninghuang.github.io/Space-Time-Pilot/ Code: https://github.com/ZheningHuang/spacetimepilot",
    "arxiv_url": "https://arxiv.org/abs/2512.25075v1",
    "pdf_url": "https://arxiv.org/pdf/2512.25075v1",
    "published_date": "2025-12-31",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "github_url": "https://github.com/ZheningHuang/spacetimepilot",
    "keywords": [
      "diffusion model",
      "controllable",
      "video diffusion",
      "dit"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.25075v1",
      "pdf": "https://arxiv.org/pdf/2512.25075v1",
      "project": "https://zheninghuang.github.io/Space-Time-Pilot",
      "github": "https://github.com/ZheningHuang/spacetimepilot"
    },
    "bibtex": ""
  },
  {
    "title": "TeleWorld: Towards Dynamic Multimodal Synthesis with a 4D World Model",
    "authors": [
      "Yabo Chen",
      "Yuanzhi Liang",
      "Jiepeng Wang",
      "Tingxi Chen",
      "Junfei Cheng",
      "Zixiao Gu",
      "Yuyang Huang",
      "Zicheng Jiang",
      "Wei Li",
      "Tian Li",
      "Weichen Li",
      "Zuoxin Li",
      "Guangce Liu",
      "Jialun Liu",
      "Junqi Liu",
      "Haoyuan Wang",
      "Qizhen Weng",
      "Xuan'er Wu",
      "Xunzhi Xiang",
      "Xiaoyan Yang",
      "Xin Zhang",
      "Shiwen Zhang",
      "Junyu Zhou",
      "Chengcheng Zhou",
      "Haibin Huang",
      "Chi Zhang",
      "Xuelong Li"
    ],
    "abstract": "World models aim to endow AI systems with the ability to represent, generate, and interact with dynamic environments in a coherent and temporally consistent manner. While recent video generation models have demonstrated impressive visual quality, they remain limited in real-time interaction, long-horizon consistency, and persistent memory of dynamic scenes, hindering their evolution into practical world models. In this report, we present TeleWorld, a real-time multimodal 4D world modeling framework that unifies video generation, dynamic scene reconstruction, and long-term world memory within a closed-loop system. TeleWorld introduces a novel generation-reconstruction-guidance paradigm, where generated video streams are continuously reconstructed into a dynamic 4D spatio-temporal representation, which in turn guides subsequent generation to maintain spatial, temporal, and physical consistency. To support long-horizon generation with low latency, we employ an autoregressive diffusion-based video model enhanced with Macro-from-Micro Planning (MMPL)--a hierarchical planning method that reduces error accumulation from frame-level to segment-level-alongside efficient Distribution Matching Distillation (DMD), enabling real-time synthesis under practical computational budgets. Our approach achieves seamless integration of dynamic object modeling and static scene representation within a unified 4D framework, advancing world models toward practical, interactive, and computationally accessible systems. Extensive experiments demonstrate that TeleWorld achieves strong performance in both static and dynamic world understanding, long-term consistency, and real-time generation efficiency, positioning it as a practical step toward interactive, memory-enabled world models for multimodal generation and embodied intelligence.",
    "arxiv_url": "https://arxiv.org/abs/2601.00051v1",
    "pdf_url": "https://arxiv.org/pdf/2601.00051v1",
    "published_date": "2025-12-31",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "interactive",
      "distillation",
      "world model",
      "physical",
      "video generation",
      "autoregressive",
      "efficient"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.00051v1",
      "pdf": "https://arxiv.org/pdf/2601.00051v1"
    },
    "bibtex": ""
  },
  {
    "title": "Beyond the Last Frame: Process-aware Evaluation for Generative Video Reasoning",
    "authors": [
      "Yifan Li",
      "Yukai Gu",
      "Yingqian Min",
      "Zikang Liu",
      "Yifan Du",
      "Kun Zhou",
      "Min Yang",
      "Wayne Xin Zhao",
      "Minghui Qiu"
    ],
    "abstract": "Recent breakthroughs in video generation have demonstrated an emerging capability termed Chain-of-Frames (CoF) reasoning, where models resolve complex tasks through the generation of continuous frames. While these models show promise for Generative Video Reasoning (GVR), existing evaluation frameworks often rely on single-frame assessments, which can lead to outcome-hacking, where a model reaches a correct conclusion through an erroneous process. To address this, we propose a process-aware evaluation paradigm. We introduce VIPER, a comprehensive benchmark spanning 16 tasks across temporal, structural, symbolic, spatial, physics, and planning reasoning. Furthermore, we propose Process-outcome Consistency (POC@r), a new metric that utilizes VLM-as-Judge with a hierarchical rubric to evaluate both the validity of the intermediate steps and the final result. Our experiments reveal that state-of-the-art video models achieve POC@1.0 only about 20% and exhibit a significant outcome-hacking. We further explore the impact of test-time scaling and sampling robustness, highlighting a substantial gap between current video generation and true generalized visual reasoning. Our benchmark are released at https://github.com/RUCAIBox/VIPER.",
    "arxiv_url": "https://arxiv.org/abs/2512.24952v2",
    "pdf_url": "https://arxiv.org/pdf/2512.24952v2",
    "published_date": "2025-12-31",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/RUCAIBox/VIPER",
    "keywords": [
      "dit",
      "video generation",
      "evaluation",
      "benchmark",
      "physics"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.24952v2",
      "pdf": "https://arxiv.org/pdf/2512.24952v2",
      "github": "https://github.com/RUCAIBox/VIPER"
    },
    "bibtex": ""
  },
  {
    "title": "Dream2Flow: Bridging Video Generation and Open-World Manipulation with 3D Object Flow",
    "authors": [
      "Karthik Dharmarajan",
      "Wenlong Huang",
      "Jiajun Wu",
      "Li Fei-Fei",
      "Ruohan Zhang"
    ],
    "abstract": "Generative video modeling has emerged as a compelling tool to zero-shot reason about plausible physical interactions for open-world manipulation. Yet, it remains a challenge to translate such human-led motions into the low-level actions demanded by robotic systems. We observe that given an initial image and task instruction, these models excel at synthesizing sensible object motions. Thus, we introduce Dream2Flow, a framework that bridges video generation and robotic control through 3D object flow as an intermediate representation. Our method reconstructs 3D object motions from generated videos and formulates manipulation as object trajectory tracking. By separating the state changes from the actuators that realize those changes, Dream2Flow overcomes the embodiment gap and enables zero-shot guidance from pre-trained video models to manipulate objects of diverse categories-including rigid, articulated, deformable, and granular. Through trajectory optimization or reinforcement learning, Dream2Flow converts reconstructed 3D object flow into executable low-level commands without task-specific demonstrations. Simulation and real-world experiments highlight 3D object flow as a general and scalable interface for adapting video generation models to open-world robotic manipulation. Videos and visualizations are available at https://dream2flow.github.io/.",
    "arxiv_url": "https://arxiv.org/abs/2512.24766v1",
    "pdf_url": "https://arxiv.org/pdf/2512.24766v1",
    "published_date": "2025-12-31",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "simulation",
      "trajectory",
      "video generation",
      "physical"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.24766v1",
      "pdf": "https://arxiv.org/pdf/2512.24766v1",
      "project": "https://dream2flow.github.io"
    },
    "bibtex": ""
  },
  {
    "title": "FlowBlending: Stage-Aware Multi-Model Sampling for Fast and High-Fidelity Video Generation",
    "authors": [
      "Jibin Song",
      "Mingi Kwon",
      "Jaeseok Jeong",
      "Youngjung Uh"
    ],
    "abstract": "In this work, we show that the impact of model capacity varies across timesteps: it is crucial for the early and late stages but largely negligible during the intermediate stage. Accordingly, we propose FlowBlending, a stage-aware multi-model sampling strategy that employs a large model and a small model at capacity-sensitive stages and intermediate stages, respectively. We further introduce simple criteria to choose stage boundaries and provide a velocity-divergence analysis as an effective proxy for identifying capacity-sensitive regions. Across LTX-Video (2B/13B) and WAN 2.1 (1.3B/14B), FlowBlending achieves up to 1.65x faster inference with 57.35% fewer FLOPs, while maintaining the visual fidelity, temporal coherence, and semantic alignment of the large models. FlowBlending is also compatible with existing sampling-acceleration techniques, enabling up to 2x additional speedup. Project page is available at: https://jibin86.github.io/flowblending_project_page.",
    "arxiv_url": "https://arxiv.org/abs/2512.24724v1",
    "pdf_url": "https://arxiv.org/pdf/2512.24724v1",
    "published_date": "2025-12-31",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "acceleration",
      "video generation",
      "dit"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.24724v1",
      "pdf": "https://arxiv.org/pdf/2512.24724v1",
      "project": "https://jibin86.github.io/flowblending_project_page"
    },
    "bibtex": ""
  },
  {
    "title": "PhyGDPO: Physics-Aware Groupwise Direct Preference Optimization for Physically Consistent Text-to-Video Generation",
    "authors": [
      "Yuanhao Cai",
      "Kunpeng Li",
      "Menglin Jia",
      "Jialiang Wang",
      "Junzhe Sun",
      "Feng Liang",
      "Weifeng Chen",
      "Felix Juefei-Xu",
      "Chu Wang",
      "Ali Thabet",
      "Xiaoliang Dai",
      "Xuan Ju",
      "Alan Yuille",
      "Ji Hou"
    ],
    "abstract": "Recent advances in text-to-video (T2V) generation have achieved good visual quality, yet synthesizing videos that faithfully follow physical laws remains an open challenge. Existing methods mainly based on graphics or prompt extension struggle to generalize beyond simple simulated environments or learn implicit physical reasoning. The scarcity of training data with rich physics interactions and phenomena is also a problem. In this paper, we first introduce a Physics-Augmented video data construction Pipeline, PhyAugPipe, that leverages a vision-language model (VLM) with chain-of-thought reasoning to collect a large-scale training dataset, PhyVidGen-135K. Then we formulate a principled Physics-aware Groupwise Direct Preference Optimization, PhyGDPO, framework that builds upon the groupwise Plackett-Luce probabilistic model to capture holistic preferences beyond pairwise comparisons. In PhyGDPO, we design a Physics-Guided Rewarding (PGR) scheme that embeds VLM-based physics rewards to steer optimization toward physical consistency. We also propose a LoRA-Switch Reference (LoRA-SR) scheme that eliminates memory-heavy reference duplication for efficient training. Experiments show that our method significantly outperforms state-of-the-art open-source methods on PhyGenBench and VideoPhy2. Please check our project page at https://caiyuanhao1998.github.io/project/PhyGDPO for more video results. Our code, models, and data will be released at https://github.com/caiyuanhao1998/Open-PhyGDPO",
    "arxiv_url": "https://arxiv.org/abs/2512.24551v2",
    "pdf_url": "https://arxiv.org/pdf/2512.24551v2",
    "published_date": "2025-12-31",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/caiyuanhao1998/Open-PhyGDPO",
    "keywords": [
      "physical",
      "text-to-video",
      "video generation",
      "physics-aware",
      "t2v",
      "efficient",
      "physics"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.24551v2",
      "pdf": "https://arxiv.org/pdf/2512.24551v2",
      "project": "https://caiyuanhao1998.github.io/project/PhyGDPO",
      "github": "https://github.com/caiyuanhao1998/Open-PhyGDPO"
    },
    "bibtex": ""
  },
  {
    "title": "Taming Hallucinations: Boosting MLLMs' Video Understanding via Counterfactual Video Generation",
    "authors": [
      "Zhe Huang",
      "Hao Wen",
      "Aiming Hao",
      "Bingze Song",
      "Meiqi Wu",
      "Jiahong Wu",
      "Xiangxiang Chu",
      "Sheng Lu",
      "Haoqian Wang"
    ],
    "abstract": "Multimodal Large Language Models (MLLMs) have made remarkable progress in video understanding. However, they suffer from a critical vulnerability: an over-reliance on language priors, which can lead to visual ungrounded hallucinations, especially when processing counterfactual videos that defy common sense. This limitation, stemming from the intrinsic data imbalance between text and video, is challenging to address due to the substantial cost of collecting and annotating counterfactual data. To address this, we introduce DualityForge, a novel counterfactual data synthesis framework that employs controllable, diffusion-based video editing to transform real-world videos into counterfactual scenarios. By embedding structured contextual information into the video editing and QA generation processes, the framework automatically produces high-quality QA pairs together with original-edited video pairs for contrastive training. Based on this, we build DualityVidQA, a large-scale video dataset designed to reduce MLLM hallucinations. In addition, to fully exploit the contrastive nature of our paired data, we propose Duality-Normalized Advantage Training (DNA-Train), a two-stage SFT-RL training regime where the RL phase applies pair-wise $\\ell_1$ advantage normalization, thereby enabling a more stable and efficient policy optimization. Experiments on DualityVidQA-Test demonstrate that our method substantially reduces model hallucinations on counterfactual videos, yielding a relative improvement of 24.0% over the Qwen2.5-VL-7B baseline. Moreover, our approach achieves significant gains across both hallucination and general-purpose benchmarks, indicating strong generalization capability. We will open-source our dataset and code.",
    "arxiv_url": "https://arxiv.org/abs/2512.24271v1",
    "pdf_url": "https://arxiv.org/pdf/2512.24271v1",
    "published_date": "2025-12-30",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "controllable",
      "dit",
      "video editing",
      "video generation",
      "efficient",
      "benchmark"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.24271v1",
      "pdf": "https://arxiv.org/pdf/2512.24271v1"
    },
    "bibtex": ""
  },
  {
    "title": "Mirage: One-Step Video Diffusion for Photorealistic and Coherent Asset Editing in Driving Scenes",
    "authors": [
      "Shuyun Wang",
      "Haiyang Sun",
      "Bing Wang",
      "Hangjun Ye",
      "Xin Yu"
    ],
    "abstract": "Vision-centric autonomous driving systems rely on diverse and scalable training data to achieve robust performance. While video object editing offers a promising path for data augmentation, existing methods often struggle to maintain both high visual fidelity and temporal coherence. In this work, we propose \\textbf{Mirage}, a one-step video diffusion model for photorealistic and coherent asset editing in driving scenes. Mirage builds upon a text-to-video diffusion prior to ensure temporal consistency across frames. However, 3D causal variational autoencoders often suffer from degraded spatial fidelity due to compression, and directly passing 3D encoder features to decoder layers breaks temporal causality. To address this, we inject temporally agnostic latents from a pretrained 2D encoder into the 3D decoder to restore detail while preserving causal structures. Furthermore, because scene objects and inserted assets are optimized under different objectives, their Gaussians exhibit a distribution mismatch that leads to pose misalignment. To mitigate this, we introduce a two-stage data alignment strategy combining coarse 3D alignment and fine 2D refinement, thereby improving alignment and providing cleaner supervision. Extensive experiments demonstrate that Mirage achieves high realism and temporal consistency across diverse editing scenarios. Beyond asset editing, Mirage can also generalize to other video-to-video translation tasks, serving as a reliable baseline for future research. Our code is available at https://github.com/wm-research/mirage.",
    "arxiv_url": "https://arxiv.org/abs/2512.24227v1",
    "pdf_url": "https://arxiv.org/pdf/2512.24227v1",
    "published_date": "2025-12-30",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/wm-research/mirage",
    "keywords": [
      "temporal consistency",
      "video diffusion",
      "dit",
      "text-to-video",
      "autonomous driving",
      "video-to-video",
      "diffusion model",
      "video translation"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.24227v1",
      "pdf": "https://arxiv.org/pdf/2512.24227v1",
      "github": "https://github.com/wm-research/mirage"
    },
    "bibtex": ""
  },
  {
    "title": "RainFusion2.0: Temporal-Spatial Awareness and Hardware-Efficient Block-wise Sparse Attention",
    "authors": [
      "Aiyue Chen",
      "Yaofu Liu",
      "Junjian Huang",
      "Guang Lian",
      "Yiwu Yao",
      "Wangli Lan",
      "Jing Lin",
      "Zhixin Ma",
      "Tingting Zhou",
      "Harry Yang"
    ],
    "abstract": "In video and image generation tasks, Diffusion Transformer (DiT) models incur extremely high computational costs due to attention mechanisms, which limits their practical applications. Furthermore, with hardware advancements, a wide range of devices besides graphics processing unit (GPU), such as application-specific integrated circuit (ASIC), have been increasingly adopted for model inference. Sparse attention, which leverages the inherent sparsity of attention by skipping computations for insignificant tokens, is an effective approach to mitigate computational costs. However, existing sparse attention methods have two critical limitations: the overhead of sparse pattern prediction and the lack of hardware generality, as most of these methods are designed for GPU. To address these challenges, this study proposes RainFusion2.0, which aims to develop an online adaptive, hardware-efficient, and low-overhead sparse attention mechanism to accelerate both video and image generative models, with robust performance across diverse hardware platforms. Key technical insights include: (1) leveraging block-wise mean values as representative tokens for sparse mask prediction; (2) implementing spatiotemporal-aware token permutation; and (3) introducing a first-frame sink mechanism specifically designed for video generation scenarios. Experimental results demonstrate that RainFusion2.0 can achieve 80% sparsity while achieving an end-to-end speedup of 1.5~1.8x without compromising video quality. Moreover, RainFusion2.0 demonstrates effectiveness across various generative models and validates its generalization across diverse hardware platforms.",
    "arxiv_url": "https://arxiv.org/abs/2512.24086v1",
    "pdf_url": "https://arxiv.org/pdf/2512.24086v1",
    "published_date": "2025-12-30",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "diffusion transformer",
      "video generation",
      "dit"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.24086v1",
      "pdf": "https://arxiv.org/pdf/2512.24086v1"
    },
    "bibtex": ""
  },
  {
    "title": "PhyAVBench: A Challenging Audio Physics-Sensitivity Benchmark for Physically Grounded Text-to-Audio-Video Generation",
    "authors": [
      "Tianxin Xie",
      "Wentao Lei",
      "Guanjie Huang",
      "Pengfei Zhang",
      "Kai Jiang",
      "Chunhui Zhang",
      "Fengji Ma",
      "Haoyu He",
      "Han Zhang",
      "Jiangshan He",
      "Jinting Wang",
      "Linghan Fang",
      "Lufei Gao",
      "Orkesh Ablet",
      "Peihua Zhang",
      "Ruolin Hu",
      "Shengyu Li",
      "Weilin Lin",
      "Xiaoyang Feng",
      "Xinyue Yang",
      "Yan Rong",
      "Yanyun Wang",
      "Zihang Shao",
      "Zelin Zhao",
      "Chenxing Li",
      "Shan Yang",
      "Wenfu Wang",
      "Meng Yu",
      "Dong Yu",
      "Li Liu"
    ],
    "abstract": "Text-to-audio-video (T2AV) generation underpins a wide range of applications demanding realistic audio-visual content, including virtual reality, world modeling, gaming, and filmmaking. However, existing T2AV models remain incapable of generating physically plausible sounds, primarily due to their limited understanding of physical principles. To situate current research progress, we present PhyAVBench, a challenging audio physics-sensitivity benchmark designed to systematically evaluate the audio physics grounding capabilities of existing T2AV models. PhyAVBench comprises 1,000 groups of paired text prompts with controlled physical variables that implicitly induce sound variations, enabling a fine-grained assessment of models' sensitivity to changes in underlying acoustic conditions. We term this evaluation paradigm the Audio-Physics Sensitivity Test (APST). Unlike prior benchmarks that primarily focus on audio-video synchronization, PhyAVBench explicitly evaluates models' understanding of the physical mechanisms underlying sound generation, covering 6 major audio physics dimensions, 4 daily scenarios (music, sound effects, speech, and their mix), and 50 fine-grained test points, ranging from fundamental aspects such as sound diffraction to more complex phenomena, e.g., Helmholtz resonance. Each test point consists of multiple groups of paired prompts, where each prompt is grounded by at least 20 newly recorded or collected real-world videos, thereby minimizing the risk of data leakage during model pre-training. Both prompts and videos are iteratively refined through rigorous human-involved error correction and quality control to ensure high quality. We argue that only models with a genuine grasp of audio-related physical principles can generate physically consistent audio-visual content. We hope PhyAVBench will stimulate future progress in this critical yet largely unexplored domain.",
    "arxiv_url": "https://arxiv.org/abs/2512.23994v1",
    "pdf_url": "https://arxiv.org/pdf/2512.23994v1",
    "published_date": "2025-12-30",
    "categories": [
      "cs.SD",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "dit",
      "world model",
      "physical",
      "video generation",
      "film",
      "evaluation",
      "sound",
      "benchmark",
      "physics"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.23994v1",
      "pdf": "https://arxiv.org/pdf/2512.23994v1"
    },
    "bibtex": ""
  },
  {
    "title": "DriveExplorer: Images-Only Decoupled 4D Reconstruction with Progressive Restoration for Driving View Extrapolation",
    "authors": [
      "Yuang Jia",
      "Jinlong Wang",
      "Jiayi Zhao",
      "Chunlam Li",
      "Shunzhou Wang",
      "Wei Gao"
    ],
    "abstract": "This paper presents an effective solution for view extrapolation in autonomous driving scenarios. Recent approaches focus on generating shifted novel view images from given viewpoints using diffusion models. However, these methods heavily rely on priors such as LiDAR point clouds, 3D bounding boxes, and lane annotations, which demand expensive sensors or labor-intensive labeling, limiting applicability in real-world deployment. In this work, with only images and optional camera poses, we first estimate a global static point cloud and per-frame dynamic point clouds, fusing them into a unified representation. We then employ a deformable 4D Gaussian framework to reconstruct the scene. The initially trained 4D Gaussian model renders degraded and pseudo-images to train a video diffusion model. Subsequently, progressively shifted Gaussian renderings are iteratively refined by the diffusion model,and the enhanced results are incorporated back as training data for 4DGS. This process continues until extrapolation reaches the target viewpoints. Compared with baselines, our method produces higher-quality images at novel extrapolated viewpoints.",
    "arxiv_url": "https://arxiv.org/abs/2512.23983v1",
    "pdf_url": "https://arxiv.org/pdf/2512.23983v1",
    "published_date": "2025-12-30",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "diffusion model",
      "novel view",
      "video diffusion",
      "autonomous driving"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.23983v1",
      "pdf": "https://arxiv.org/pdf/2512.23983v1"
    },
    "bibtex": ""
  },
  {
    "title": "T2VAttack: Adversarial Attack on Text-to-Video Diffusion Models",
    "authors": [
      "Changzhen Li",
      "Yuecong Min",
      "Jie Zhang",
      "Zheng Yuan",
      "Shiguang Shan",
      "Xilin Chen"
    ],
    "abstract": "The rapid evolution of Text-to-Video (T2V) diffusion models has driven remarkable advancements in generating high-quality, temporally coherent videos from natural language descriptions. Despite these achievements, their vulnerability to adversarial attacks remains largely unexplored. In this paper, we introduce T2VAttack, a comprehensive study of adversarial attacks on T2V diffusion models from both semantic and temporal perspectives. Considering the inherently dynamic nature of video data, we propose two distinct attack objectives: a semantic objective to evaluate video-text alignment and a temporal objective to assess the temporal dynamics. To achieve an effective and efficient attack process, we propose two adversarial attack methods: (i) T2VAttack-S, which identifies semantically or temporally critical words in prompts and replaces them with synonyms via greedy search, and (ii) T2VAttack-I, which iteratively inserts optimized words with minimal perturbation to the prompt. By combining these objectives and strategies, we conduct a comprehensive evaluation on the adversarial robustness of several state-of-the-art T2V models, including ModelScope, CogVideoX, Open-Sora, and HunyuanVideo. Our experiments reveal that even minor prompt modifications, such as the substitution or insertion of a single word, can cause substantial degradation in semantic fidelity and temporal dynamics, highlighting critical vulnerabilities in current T2V diffusion models.",
    "arxiv_url": "https://arxiv.org/abs/2512.23953v1",
    "pdf_url": "https://arxiv.org/pdf/2512.23953v1",
    "published_date": "2025-12-30",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "video diffusion",
      "text-to-video",
      "evaluation",
      "diffusion model",
      "dynamics",
      "t2v",
      "efficient"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.23953v1",
      "pdf": "https://arxiv.org/pdf/2512.23953v1"
    },
    "bibtex": ""
  },
  {
    "title": "Diffusion Knows Transparency: Repurposing Video Diffusion for Transparent Object Depth and Normal Estimation",
    "authors": [
      "Shaocong Xu",
      "Songlin Wei",
      "Qizhe Wei",
      "Zheng Geng",
      "Hong Li",
      "Licheng Shen",
      "Qianpu Sun",
      "Shu Han",
      "Bin Ma",
      "Bohan Li",
      "Chongjie Ye",
      "Yuhang Zheng",
      "Nan Wang",
      "Saining Zhang",
      "Hao Zhao"
    ],
    "abstract": "Transparent objects remain notoriously hard for perception systems: refraction, reflection and transmission break the assumptions behind stereo, ToF and purely discriminative monocular depth, causing holes and temporally unstable estimates. Our key observation is that modern video diffusion models already synthesize convincing transparent phenomena, suggesting they have internalized the optical rules. We build TransPhy3D, a synthetic video corpus of transparent/reflective scenes: 11k sequences rendered with Blender/Cycles. Scenes are assembled from a curated bank of category-rich static assets and shape-rich procedural assets paired with glass/plastic/metal materials. We render RGB + depth + normals with physically based ray tracing and OptiX denoising. Starting from a large video diffusion model, we learn a video-to-video translator for depth (and normals) via lightweight LoRA adapters. During training we concatenate RGB and (noisy) depth latents in the DiT backbone and co-train on TransPhy3D and existing frame-wise synthetic datasets, yielding temporally consistent predictions for arbitrary-length input videos. The resulting model, DKT, achieves zero-shot SOTA on real and synthetic video benchmarks involving transparency: ClearPose, DREDS (CatKnown/CatNovel), and TransPhy3D-Test. It improves accuracy and temporal consistency over strong image/video baselines, and a normal variant sets the best video normal estimation results on ClearPose. A compact 1.3B version runs at ~0.17 s/frame. Integrated into a grasping stack, DKT's depth boosts success rates across translucent, reflective and diffuse surfaces, outperforming prior estimators. Together, these results support a broader claim: \"Diffusion knows transparency.\" Generative video priors can be repurposed, efficiently and label-free, into robust, temporally coherent perception for challenging real-world manipulation.",
    "arxiv_url": "https://arxiv.org/abs/2512.23705v1",
    "pdf_url": "https://arxiv.org/pdf/2512.23705v1",
    "published_date": "2025-12-29",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "temporal consistency",
      "video diffusion",
      "dit",
      "physical",
      "efficient",
      "video-to-video",
      "diffusion model",
      "denoising",
      "benchmark"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.23705v1",
      "pdf": "https://arxiv.org/pdf/2512.23705v1"
    },
    "bibtex": ""
  },
  {
    "title": "LiveTalk: Real-Time Multimodal Interactive Video Diffusion via Improved On-Policy Distillation",
    "authors": [
      "Ethan Chern",
      "Zhulin Hu",
      "Bohao Tang",
      "Jiadi Su",
      "Steffi Chern",
      "Zhijie Deng",
      "Pengfei Liu"
    ],
    "abstract": "Real-time video generation via diffusion is essential for building general-purpose multimodal interactive AI systems. However, the simultaneous denoising of all video frames with bidirectional attention via an iterative process in diffusion models prevents real-time interaction. While existing distillation methods can make the model autoregressive and reduce sampling steps to mitigate this, they focus primarily on text-to-video generation, leaving the human-AI interaction unnatural and less efficient. This paper targets real-time interactive video diffusion conditioned on a multimodal context, including text, image, and audio, to bridge the gap. Given the observation that the leading on-policy distillation approach Self Forcing encounters challenges (visual artifacts like flickering, black frames, and quality degradation) with multimodal conditioning, we investigate an improved distillation recipe with emphasis on the quality of condition inputs as well as the initialization and schedule for the on-policy optimization. On benchmarks for multimodal-conditioned (audio, image, and text) avatar video generation including HDTF, AVSpeech, and CelebV-HQ, our distilled model matches the visual quality of the full-step, bidirectional baselines of similar or larger size with 20x less inference cost and latency. Further, we integrate our model with audio language models and long-form video inference technique Anchor-Heavy Identity Sinks to build LiveTalk, a real-time multimodal interactive avatar system. System-level evaluation on our curated multi-turn interaction benchmark shows LiveTalk outperforms state-of-the-art models (Sora2, Veo3) in multi-turn video coherence and content quality, while reducing response latency from 1 to 2 minutes to real-time generation, enabling seamless human-AI multimodal interaction.",
    "arxiv_url": "https://arxiv.org/abs/2512.23576v1",
    "pdf_url": "https://arxiv.org/pdf/2512.23576v1",
    "published_date": "2025-12-29",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "interactive",
      "video diffusion",
      "distillation",
      "dit",
      "identity",
      "text-to-video",
      "video generation",
      "long-form",
      "efficient",
      "evaluation",
      "diffusion model",
      "denoising",
      "autoregressive",
      "benchmark",
      "avatar"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.23576v1",
      "pdf": "https://arxiv.org/pdf/2512.23576v1"
    },
    "bibtex": ""
  },
  {
    "title": "DriveLaW:Unifying Planning and Video Generation in a Latent Driving World",
    "authors": [
      "Tianze Xia",
      "Yongkang Li",
      "Lijun Zhou",
      "Jingfeng Yao",
      "Kaixin Xiong",
      "Haiyang Sun",
      "Bing Wang",
      "Kun Ma",
      "Guang Chen",
      "Hangjun Ye",
      "Wenyu Liu",
      "Xinggang Wang"
    ],
    "abstract": "World models have become crucial for autonomous driving, as they learn how scenarios evolve over time to address the long-tail challenges of the real world. However, current approaches relegate world models to limited roles: they operate within ostensibly unified architectures that still keep world prediction and motion planning as decoupled processes. To bridge this gap, we propose DriveLaW, a novel paradigm that unifies video generation and motion planning. By directly injecting the latent representation from its video generator into the planner, DriveLaW ensures inherent consistency between high-fidelity future generation and reliable trajectory planning. Specifically, DriveLaW consists of two core components: DriveLaW-Video, our powerful world model that generates high-fidelity forecasting with expressive latent representations, and DriveLaW-Act, a diffusion planner that generates consistent and reliable trajectories from the latent of DriveLaW-Video, with both components optimized by a three-stage progressive training strategy. The power of our unified paradigm is demonstrated by new state-of-the-art results across both tasks. DriveLaW not only advances video prediction significantly, surpassing best-performing work by 33.3% in FID and 1.8% in FVD, but also achieves a new record on the NAVSIM planning benchmark.",
    "arxiv_url": "https://arxiv.org/abs/2512.23421v2",
    "pdf_url": "https://arxiv.org/pdf/2512.23421v2",
    "published_date": "2025-12-29",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "video prediction",
      "world model",
      "trajectory",
      "video generation",
      "autonomous driving",
      "architecture",
      "benchmark"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.23421v2",
      "pdf": "https://arxiv.org/pdf/2512.23421v2"
    },
    "bibtex": ""
  },
  {
    "title": "CountGD++: Generalized Prompting for Open-World Counting",
    "authors": [
      "Niki Amini-Naieni",
      "Andrew Zisserman"
    ],
    "abstract": "The flexibility and accuracy of methods for automatically counting objects in images and videos are limited by the way the object can be specified. While existing methods allow users to describe the target object with text and visual examples, the visual examples must be manually annotated inside the image, and there is no way to specify what not to count. To address these gaps, we introduce novel capabilities that expand how the target object can be specified. Specifically, we extend the prompt to enable what not to count to be described with text and/or visual examples, introduce the concept of `pseudo-exemplars' that automate the annotation of visual examples at inference, and extend counting models to accept visual examples from both natural and synthetic external images. We also use our new counting model, CountGD++, as a vision expert agent for an LLM. Together, these contributions expand the prompt flexibility of multi-modal open-world counting and lead to significant improvements in accuracy, efficiency, and generalization across multiple datasets. Code is available at https://github.com/niki-amini-naieni/CountGDPlusPlus.",
    "arxiv_url": "https://arxiv.org/abs/2512.23351v1",
    "pdf_url": "https://arxiv.org/pdf/2512.23351v1",
    "published_date": "2025-12-29",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/niki-amini-naieni/CountGDPlusPlus",
    "keywords": [
      "multi-modal",
      "concept"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.23351v1",
      "pdf": "https://arxiv.org/pdf/2512.23351v1",
      "github": "https://github.com/niki-amini-naieni/CountGDPlusPlus"
    },
    "bibtex": ""
  },
  {
    "title": "Plug-and-Play Fidelity Optimization for Diffusion Transformer Acceleration via Cumulative Error Minimization",
    "authors": [
      "Tong Shao",
      "Yusen Fu",
      "Guoying Sun",
      "Jingde Kong",
      "Zhuotao Tian",
      "Jingyong Su"
    ],
    "abstract": "Although Diffusion Transformer (DiT) has emerged as a predominant architecture for image and video generation, its iterative denoising process results in slow inference, which hinders broader applicability and development. Caching-based methods achieve training-free acceleration, while suffering from considerable computational error. Existing methods typically incorporate error correction strategies such as pruning or prediction to mitigate it. However, their fixed caching strategy fails to adapt to the complex error variations during denoising, which limits the full potential of error correction. To tackle this challenge, we propose a novel fidelity-optimization plugin for existing error correction methods via cumulative error minimization, named CEM. CEM predefines the error to characterize the sensitivity of model to acceleration jointly influenced by timesteps and cache intervals. Guided by this prior, we formulate a dynamic programming algorithm with cumulative error approximation for strategy optimization, which achieves the caching error minimization, resulting in a substantial improvement in generation fidelity. CEM is model-agnostic and exhibits strong generalization, which is adaptable to arbitrary acceleration budgets. It can be seamlessly integrated into existing error correction frameworks and quantized models without introducing any additional computational overhead. Extensive experiments conducted on nine generation models and quantized methods across three tasks demonstrate that CEM significantly improves generation fidelity of existing acceleration models, and outperforms the original generation performance on FLUX.1-dev, PixArt-$$, StableDiffusion1.5 and Hunyuan. The code will be made publicly available.",
    "arxiv_url": "https://arxiv.org/abs/2512.23258v1",
    "pdf_url": "https://arxiv.org/pdf/2512.23258v1",
    "published_date": "2025-12-29",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "acceleration",
      "dit",
      "diffusion transformer",
      "video generation",
      "architecture",
      "denoising"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.23258v1",
      "pdf": "https://arxiv.org/pdf/2512.23258v1"
    },
    "bibtex": ""
  },
  {
    "title": "Bridging Your Imagination with Audio-Video Generation via a Unified Director",
    "authors": [
      "Jiaxu Zhang",
      "Tianshu Hu",
      "Yuan Zhang",
      "Zenan Li",
      "Linjie Luo",
      "Guosheng Lin",
      "Xin Chen"
    ],
    "abstract": "Existing AI-driven video creation systems typically treat script drafting and key-shot design as two disjoint tasks: the former relies on large language models, while the latter depends on image generation models. We argue that these two tasks should be unified within a single framework, as logical reasoning and imaginative thinking are both fundamental qualities of a film director. In this work, we propose UniMAGE, a unified director model that bridges user prompts with well-structured scripts, thereby empowering non-experts to produce long-context, multi-shot films by leveraging existing audio-video generation models. To achieve this, we employ the Mixture-of-Transformers architecture that unifies text and image generation. To further enhance narrative logic and keyframe consistency, we introduce a ``first interleaving, then disentangling'' training paradigm. Specifically, we first perform Interleaved Concept Learning, which utilizes interleaved text-image data to foster the model's deeper understanding and imaginative interpretation of scripts. We then conduct Disentangled Expert Learning, which decouples script writing from keyframe generation, enabling greater flexibility and creativity in storytelling. Extensive experiments demonstrate that UniMAGE achieves state-of-the-art performance among open-source models, generating logically coherent video scripts and visually consistent keyframe images.",
    "arxiv_url": "https://arxiv.org/abs/2512.23222v1",
    "pdf_url": "https://arxiv.org/pdf/2512.23222v1",
    "published_date": "2025-12-29",
    "categories": [
      "cs.CV",
      "cs.MM"
    ],
    "github_url": "",
    "keywords": [
      "concept",
      "architecture",
      "video generation",
      "film"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.23222v1",
      "pdf": "https://arxiv.org/pdf/2512.23222v1"
    },
    "bibtex": ""
  },
  {
    "title": "3D sans 3D Scans: Scalable Pre-training from Video-Generated Point Clouds",
    "authors": [
      "Ryousuke Yamada",
      "Kohsuke Ide",
      "Yoshihiro Fukuhara",
      "Hirokatsu Kataoka",
      "Gilles Puy",
      "Andrei Bursuc",
      "Yuki M. Asano"
    ],
    "abstract": "Despite recent progress in 3D self-supervised learning, collecting large-scale 3D scene scans remains expensive and labor-intensive. In this work, we investigate whether 3D representations can be learned from unlabeled videos recorded without any real 3D sensors. We present Laplacian-Aware Multi-level 3D Clustering with Sinkhorn-Knopp (LAM3C), a self-supervised framework that learns from video-generated point clouds from unlabeled videos. We first introduce RoomTours, a video-generated point cloud dataset constructed by collecting room-walkthrough videos from the web (e.g., real-estate tours) and generating 49,219 scenes using an off-the-shelf feed-forward reconstruction model. We also propose a noise-regularized loss that stabilizes representation learning by enforcing local geometric smoothness and ensuring feature stability under noisy point clouds. Remarkably, without using any real 3D scans, LAM3C achieves higher performance than the previous self-supervised methods on indoor semantic and instance segmentation. These results suggest that unlabeled videos represent an abundant source of data for 3D self-supervised learning.",
    "arxiv_url": "https://arxiv.org/abs/2512.23042v1",
    "pdf_url": "https://arxiv.org/pdf/2512.23042v1",
    "published_date": "2025-12-28",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.23042v1",
      "pdf": "https://arxiv.org/pdf/2512.23042v1"
    },
    "bibtex": ""
  },
  {
    "title": "ByteLoom: Weaving Geometry-Consistent Human-Object Interactions through Progressive Curriculum Learning",
    "authors": [
      "Bangya Liu",
      "Xinyu Gong",
      "Zelin Zhao",
      "Ziyang Song",
      "Yulei Lu",
      "Suhui Wu",
      "Jun Zhang",
      "Suman Banerjee",
      "Hao Zhang"
    ],
    "abstract": "Human-object interaction (HOI) video generation has garnered increasing attention due to its promising applications in digital humans, e-commerce, advertising, and robotics imitation learning. However, existing methods face two critical limitations: (1) a lack of effective mechanisms to inject multi-view information of the object into the model, leading to poor cross-view consistency, and (2) heavy reliance on fine-grained hand mesh annotations for modeling interaction occlusions. To address these challenges, we introduce ByteLoom, a Diffusion Transformer (DiT)-based framework that generates realistic HOI videos with geometrically consistent object illustration, using simplified human conditioning and 3D object inputs. We first propose an RCM-cache mechanism that leverages Relative Coordinate Maps (RCM) as a universal representation to maintain object's geometry consistency and precisely control 6-DoF object transformations in the meantime. To compensate HOI dataset scarcity and leverage existing datasets, we further design a training curriculum that enhances model capabilities in a progressive style and relaxes the demand of hand mesh. Extensive experiments demonstrate that our method faithfully preserves human identity and the object's multi-view geometry, while maintaining smooth motion and object manipulation.",
    "arxiv_url": "https://arxiv.org/abs/2512.22854v1",
    "pdf_url": "https://arxiv.org/pdf/2512.22854v1",
    "published_date": "2025-12-28",
    "categories": [
      "cs.CV",
      "cs.GR",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "dit",
      "identity",
      "diffusion transformer",
      "video generation",
      "robotics",
      "advertising",
      "style"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.22854v1",
      "pdf": "https://arxiv.org/pdf/2512.22854v1"
    },
    "bibtex": ""
  },
  {
    "title": "Autoregressive Flow Matching for Motion Prediction",
    "authors": [
      "Johnathan Xie",
      "Stefan Stojanov",
      "Cristobal Eyzaguirre",
      "Daniel L. K. Yamins",
      "Jiajun Wu"
    ],
    "abstract": "Motion prediction has been studied in different contexts with models trained on narrow distributions and applied to downstream tasks in human motion prediction and robotics. Simultaneously, recent efforts in scaling video prediction have demonstrated impressive visual realism, yet they struggle to accurately model complex motions despite massive scale. Inspired by the scaling of video generation, we develop autoregressive flow matching (ARFM), a new method for probabilistic modeling of sequential continuous data and train it on diverse video datasets to generate future point track locations over long horizons. To evaluate our model, we develop benchmarks for evaluating the ability of motion prediction models to predict human and robot motion. Our model is able to predict complex motions, and we demonstrate that conditioning robot action prediction and human motion prediction on predicted future tracks can significantly improve downstream task performance. Code and models publicly available at: https://github.com/Johnathan-Xie/arfm-motion-prediction.",
    "arxiv_url": "https://arxiv.org/abs/2512.22688v1",
    "pdf_url": "https://arxiv.org/pdf/2512.22688v1",
    "published_date": "2025-12-27",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/Johnathan-Xie/arfm-motion-prediction",
    "keywords": [
      "video prediction",
      "flow matching",
      "dit",
      "video generation",
      "robotics",
      "human motion",
      "autoregressive",
      "benchmark"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.22688v1",
      "pdf": "https://arxiv.org/pdf/2512.22688v1",
      "github": "https://github.com/Johnathan-Xie/arfm-motion-prediction"
    },
    "bibtex": ""
  },
  {
    "title": "Envision: Embodied Visual Planning via Goal-Imagery Video Diffusion",
    "authors": [
      "Yuming Gu",
      "Yizhi Wang",
      "Yining Hong",
      "Yipeng Gao",
      "Hao Jiang",
      "Angtian Wang",
      "Bo Liu",
      "Nathaniel S. Dennler",
      "Zhengfei Kuang",
      "Hao Li",
      "Gordon Wetzstein",
      "Chongyang Ma"
    ],
    "abstract": "Embodied visual planning aims to enable manipulation tasks by imagining how a scene evolves toward a desired goal and using the imagined trajectories to guide actions. Video diffusion models, through their image-to-video generation capability, provide a promising foundation for such visual imagination. However, existing approaches are largely forward predictive, generating trajectories conditioned on the initial observation without explicit goal modeling, thus often leading to spatial drift and goal misalignment. To address these challenges, we propose Envision, a diffusion-based framework that performs visual planning for embodied agents. By explicitly constraining the generation with a goal image, our method enforces physical plausibility and goal consistency throughout the generated trajectory. Specifically, Envision operates in two stages. First, a Goal Imagery Model identifies task-relevant regions, performs region-aware cross attention between the scene and the instruction, and synthesizes a coherent goal image that captures the desired outcome. Then, an Env-Goal Video Model, built upon a first-and-last-frame-conditioned video diffusion model (FL2V), interpolates between the initial observation and the goal image, producing smooth and physically plausible video trajectories that connect the start and goal states. Experiments on object manipulation and image editing benchmarks demonstrate that Envision achieves superior goal alignment, spatial consistency, and object preservation compared to baselines. The resulting visual plans can directly support downstream robotic planning and control, providing reliable guidance for embodied agents.",
    "arxiv_url": "https://arxiv.org/abs/2512.22626v1",
    "pdf_url": "https://arxiv.org/pdf/2512.22626v1",
    "published_date": "2025-12-27",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "video diffusion",
      "dit",
      "physical",
      "trajectory",
      "video generation",
      "image-to-video",
      "diffusion model",
      "benchmark"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.22626v1",
      "pdf": "https://arxiv.org/pdf/2512.22626v1"
    },
    "bibtex": ""
  },
  {
    "title": "CoAgent: Collaborative Planning and Consistency Agent for Coherent Video Generation",
    "authors": [
      "Qinglin Zeng",
      "Kaitong Cai",
      "Ruiqi Chen",
      "Qinhan Lv",
      "Keze Wang"
    ],
    "abstract": "Maintaining narrative coherence and visual consistency remains a central challenge in open-domain video generation. Existing text-to-video models often treat each shot independently, resulting in identity drift, scene inconsistency, and unstable temporal structure. We propose CoAgent, a collaborative and closed-loop framework for coherent video generation that formulates the process as a plan-synthesize-verify pipeline. Given a user prompt, style reference, and pacing constraints, a Storyboard Planner decomposes the input into structured shot-level plans with explicit entities, spatial relations, and temporal cues. A Global Context Manager maintains entity-level memory to preserve appearance and identity consistency across shots. Each shot is then generated by a Synthesis Module under the guidance of a Visual Consistency Controller, while a Verifier Agent evaluates intermediate results using vision-language reasoning and triggers selective regeneration when inconsistencies are detected. Finally, a pacing-aware editor refines temporal rhythm and transitions to match the desired narrative flow. Extensive experiments demonstrate that CoAgent significantly improves coherence, visual consistency, and narrative quality in long-form video generation.",
    "arxiv_url": "https://arxiv.org/abs/2512.22536v1",
    "pdf_url": "https://arxiv.org/pdf/2512.22536v1",
    "published_date": "2025-12-27",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "dit",
      "identity",
      "text-to-video",
      "video generation",
      "long-form",
      "style"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.22536v1",
      "pdf": "https://arxiv.org/pdf/2512.22536v1"
    },
    "bibtex": ""
  },
  {
    "title": "ProEdit: Inversion-based Editing From Prompts Done Right",
    "authors": [
      "Zhi Ouyang",
      "Dian Zheng",
      "Xiao-Ming Wu",
      "Jian-Jian Jiang",
      "Kun-Yu Lin",
      "Jingke Meng",
      "Wei-Shi Zheng"
    ],
    "abstract": "Inversion-based visual editing provides an effective and training-free way to edit an image or a video based on user instructions. Existing methods typically inject source image information during the sampling process to maintain editing consistency. However, this sampling strategy overly relies on source information, which negatively affects the edits in the target image (e.g., failing to change the subject's atributes like pose, number, or color as instructed). In this work, we propose ProEdit to address this issue both in the attention and the latent aspects. In the attention aspect, we introduce KV-mix, which mixes KV features of the source and the target in the edited region, mitigating the influence of the source image on the editing region while maintaining background consistency. In the latent aspect, we propose Latents-Shift, which perturbs the edited region of the source latent, eliminating the influence of the inverted latent on the sampling. Extensive experiments on several image and video editing benchmarks demonstrate that our method achieves SOTA performance. In addition, our design is plug-and-play, which can be seamlessly integrated into existing inversion and editing methods, such as RF-Solver, FireFlow and UniEdit.",
    "arxiv_url": "https://arxiv.org/abs/2512.22118v1",
    "pdf_url": "https://arxiv.org/pdf/2512.22118v1",
    "published_date": "2025-12-26",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "benchmark",
      "video editing",
      "dit"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.22118v1",
      "pdf": "https://arxiv.org/pdf/2512.22118v1"
    },
    "bibtex": ""
  },
  {
    "title": "Yume-1.5: A Text-Controlled Interactive World Generation Model",
    "authors": [
      "Xiaofeng Mao",
      "Zhen Li",
      "Chuanhao Li",
      "Xiaojie Xu",
      "Kaining Ying",
      "Tong He",
      "Jiangmiao Pang",
      "Yu Qiao",
      "Kaipeng Zhang"
    ],
    "abstract": "Recent approaches have demonstrated the promise of using diffusion models to generate interactive and explorable worlds. However, most of these methods face critical challenges such as excessively large parameter sizes, reliance on lengthy inference steps, and rapidly growing historical context, which severely limit real-time performance and lack text-controlled generation capabilities. To address these challenges, we propose \\method, a novel framework designed to generate realistic, interactive, and continuous worlds from a single image or text prompt. \\method achieves this through a carefully designed framework that supports keyboard-based exploration of the generated worlds. The framework comprises three core components: (1) a long-video generation framework integrating unified context compression with linear attention; (2) a real-time streaming acceleration strategy powered by bidirectional attention distillation and an enhanced text embedding scheme; (3) a text-controlled method for generating world events. We have provided the codebase in the supplementary material.",
    "arxiv_url": "https://arxiv.org/abs/2512.22096v1",
    "pdf_url": "https://arxiv.org/pdf/2512.22096v1",
    "published_date": "2025-12-26",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "interactive",
      "acceleration",
      "distillation",
      "streaming",
      "video generation",
      "diffusion model"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.22096v1",
      "pdf": "https://arxiv.org/pdf/2512.22096v1"
    },
    "bibtex": ""
  },
  {
    "title": "StreamAvatar: Streaming Diffusion Models for Real-Time Interactive Human Avatars",
    "authors": [
      "Zhiyao Sun",
      "Ziqiao Peng",
      "Yifeng Ma",
      "Yi Chen",
      "Zhengguang Zhou",
      "Zixiang Zhou",
      "Guozhen Zhang",
      "Youliang Zhang",
      "Yuan Zhou",
      "Qinglin Lu",
      "Yong-Jin Liu"
    ],
    "abstract": "Real-time, streaming interactive avatars represent a critical yet challenging goal in digital human research. Although diffusion-based human avatar generation methods achieve remarkable success, their non-causal architecture and high computational costs make them unsuitable for streaming. Moreover, existing interactive approaches are typically limited to head-and-shoulder region, limiting their ability to produce gestures and body motions. To address these challenges, we propose a two-stage autoregressive adaptation and acceleration framework that applies autoregressive distillation and adversarial refinement to adapt a high-fidelity human video diffusion model for real-time, interactive streaming. To ensure long-term stability and consistency, we introduce three key components: a Reference Sink, a Reference-Anchored Positional Re-encoding (RAPR) strategy, and a Consistency-Aware Discriminator. Building on this framework, we develop a one-shot, interactive, human avatar model capable of generating both natural talking and listening behaviors with coherent gestures. Extensive experiments demonstrate that our method achieves state-of-the-art performance, surpassing existing approaches in generation quality, real-time efficiency, and interaction naturalness. Project page: https://streamavatar.github.io .",
    "arxiv_url": "https://arxiv.org/abs/2512.22065v1",
    "pdf_url": "https://arxiv.org/pdf/2512.22065v1",
    "published_date": "2025-12-26",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.HC"
    ],
    "github_url": "",
    "keywords": [
      "interactive",
      "acceleration",
      "video diffusion",
      "distillation",
      "streaming",
      "diffusion model",
      "architecture",
      "autoregressive",
      "avatar",
      "gesture",
      "body motion"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.22065v1",
      "pdf": "https://arxiv.org/pdf/2512.22065v1",
      "project": "https://streamavatar.github.io"
    },
    "bibtex": ""
  },
  {
    "title": "Data relativistic uncertainty framework for low-illumination anime scenery image enhancement",
    "authors": [
      "Yiquan Gao",
      "John See"
    ],
    "abstract": "By contrast with the prevailing works of low-light enhancement in natural images and videos, this study copes with the low-illumination quality degradation in anime scenery images to bridge the domain gap. For such an underexplored enhancement task, we first curate images from various sources and construct an unpaired anime scenery dataset with diverse environments and illumination conditions to address the data scarcity. To exploit the power of uncertainty information inherent with the diverse illumination conditions, we propose a Data Relativistic Uncertainty (DRU) framework, motivated by the idea from Relativistic GAN. By analogy with the wave-particle duality of light, our framework interpretably defines and quantifies the illumination uncertainty of dark/bright samples, which is leveraged to dynamically adjust the objective functions to recalibrate the model learning under data uncertainty. Extensive experiments demonstrate the effectiveness of DRU framework by training several versions of EnlightenGANs, yielding superior perceptual and aesthetic qualities beyond the state-of-the-art methods that are incapable of learning from data uncertainty perspective. We hope our framework can expose a novel paradigm of data-centric learning for potential visual and language domains. Code is available.",
    "arxiv_url": "https://arxiv.org/abs/2512.21944v2",
    "pdf_url": "https://arxiv.org/pdf/2512.21944v2",
    "published_date": "2025-12-26",
    "categories": [
      "cs.CV",
      "cs.LG",
      "cs.MM"
    ],
    "github_url": "",
    "keywords": [
      "dit"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.21944v2",
      "pdf": "https://arxiv.org/pdf/2512.21944v2"
    },
    "bibtex": ""
  },
  {
    "title": "MoFu: Scale-Aware Modulation and Fourier Fusion for Multi-Subject Video Generation",
    "authors": [
      "Run Ling",
      "Ke Cao",
      "Jian Lu",
      "Ao Ma",
      "Haowei Liu",
      "Runze He",
      "Changwei Wang",
      "Rongtao Xu",
      "Yihua Shao",
      "Zhanjie Zhang",
      "Peng Wu",
      "Guibing Guo",
      "Wei Feng",
      "Zheng Zhang",
      "Jingjing Lv",
      "Junjie Shen",
      "Ching Law",
      "Xingwei Wang"
    ],
    "abstract": "Multi-subject video generation aims to synthesize videos from textual prompts and multiple reference images, ensuring that each subject preserves natural scale and visual fidelity. However, current methods face two challenges: scale inconsistency, where variations in subject size lead to unnatural generation, and permutation sensitivity, where the order of reference inputs causes subject distortion. In this paper, we propose MoFu, a unified framework that tackles both challenges. For scale inconsistency, we introduce Scale-Aware Modulation (SMO), an LLM-guided module that extracts implicit scale cues from the prompt and modulates features to ensure consistent subject sizes. To address permutation sensitivity, we present a simple yet effective Fourier Fusion strategy that processes the frequency information of reference features via the Fast Fourier Transform to produce a unified representation. Besides, we design a Scale-Permutation Stability Loss to jointly encourage scale-consistent and permutation-invariant generation. To further evaluate these challenges, we establish a dedicated benchmark with controlled variations in subject scale and reference permutation. Extensive experiments demonstrate that MoFu significantly outperforms existing methods in preserving natural scale, subject fidelity, and overall visual quality.",
    "arxiv_url": "https://arxiv.org/abs/2512.22310v1",
    "pdf_url": "https://arxiv.org/pdf/2512.22310v1",
    "published_date": "2025-12-26",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "benchmark",
      "video generation"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.22310v1",
      "pdf": "https://arxiv.org/pdf/2512.22310v1"
    },
    "bibtex": ""
  },
  {
    "title": "High-Fidelity and Long-Duration Human Image Animation with Diffusion Transformer",
    "authors": [
      "Shen Zheng",
      "Jiaran Cai",
      "Yuansheng Guan",
      "Shenneng Huang",
      "Xingpei Ma",
      "Junjie Cao",
      "Hanfeng Zhao",
      "Qiang Zhang",
      "Shunsi Zhang",
      "Xiao-Ping Zhang"
    ],
    "abstract": "Recent progress in diffusion models has significantly advanced the field of human image animation. While existing methods can generate temporally consistent results for short or regular motions, significant challenges remain, particularly in generating long-duration videos. Furthermore, the synthesis of fine-grained facial and hand details remains under-explored, limiting the applicability of current approaches in real-world, high-quality applications. To address these limitations, we propose a diffusion transformer (DiT)-based framework which focuses on generating high-fidelity and long-duration human animation videos. First, we design a set of hybrid implicit guidance signals and a sharpness guidance factor, enabling our framework to additionally incorporate detailed facial and hand features as guidance. Next, we incorporate the time-aware position shift fusion module, modify the input format within the DiT backbone, and refer to this mechanism as the Position Shift Adaptive Module, which enables video generation of arbitrary length. Finally, we introduce a novel data augmentation strategy and a skeleton alignment model to reduce the impact of human shape variations across different identities. Experimental results demonstrate that our method outperforms existing state-of-the-art approaches, achieving superior performance in both high-fidelity and long-duration human image animation.",
    "arxiv_url": "https://arxiv.org/abs/2512.21905v1",
    "pdf_url": "https://arxiv.org/pdf/2512.21905v1",
    "published_date": "2025-12-26",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "dit",
      "diffusion transformer",
      "video generation",
      "diffusion model",
      "human animation",
      "image animation"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.21905v1",
      "pdf": "https://arxiv.org/pdf/2512.21905v1"
    },
    "bibtex": ""
  },
  {
    "title": "Inference-based GAN Video Generation",
    "authors": [
      "Jingbo Yang",
      "Adrian G. Bors"
    ],
    "abstract": "Video generation has seen remarkable progress thanks to advancements in generative deep learning. However, generating long sequences remains a significant challenge. Generated videos should not only display coherent and continuous movement but also meaningful movement in successions of scenes. Models such as GANs, VAEs, and Diffusion Networks have been used for generating short video sequences, typically up to 16 frames. In this paper, we first propose a new type of video generator by enabling adversarial-based unconditional video generators with a variational encoder, akin to a VAE-GAN hybrid structure. The proposed model, as in other video deep learning-based processing frameworks, incorporates two processing branches, one for content and another for movement. However, existing models struggle with the temporal scaling of the generated videos. Classical approaches often result in degraded video quality when attempting to increase the generated video length, especially for significantly long sequences. To overcome this limitation, our research study extends the initially proposed VAE-GAN video generation model by employing a novel, memory-efficient approach to generate long videos composed of hundreds or thousands of frames ensuring their temporal continuity, consistency and dynamics. Our approach leverages a Markov chain framework with a recall mechanism, where each state represents a short-length VAE-GAN video generator. This setup enables the sequential connection of generated video sub-sequences, maintaining temporal dependencies and resulting in meaningful long video sequences.",
    "arxiv_url": "https://arxiv.org/abs/2512.21776v2",
    "pdf_url": "https://arxiv.org/pdf/2512.21776v2",
    "published_date": "2025-12-25",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "dit",
      "video generation",
      "dynamics",
      "long video",
      "efficient"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.21776v2",
      "pdf": "https://arxiv.org/pdf/2512.21776v2"
    },
    "bibtex": ""
  },
  {
    "title": "Knot Forcing: Taming Autoregressive Video Diffusion Models for Real-time Infinite Interactive Portrait Animation",
    "authors": [
      "Steven Xiao",
      "Xindi Zhang",
      "Dechao Meng",
      "Qi Wang",
      "Peng Zhang",
      "Bang Zhang"
    ],
    "abstract": "Real-time portrait animation is essential for interactive applications such as virtual assistants and live avatars, requiring high visual fidelity, temporal coherence, ultra-low latency, and responsive control from dynamic inputs like reference images and driving signals. While diffusion-based models achieve strong quality, their non-causal nature hinders streaming deployment. Causal autoregressive video generation approaches enable efficient frame-by-frame generation but suffer from error accumulation, motion discontinuities at chunk boundaries, and degraded long-term consistency. In this work, we present a novel streaming framework named Knot Forcing for real-time portrait animation that addresses these challenges through three key designs: (1) a chunk-wise generation strategy with global identity preservation via cached KV states of the reference image and local temporal modeling using sliding window attention; (2) a temporal knot module that overlaps adjacent chunks and propagates spatio-temporal cues via image-to-video conditioning to smooth inter-chunk motion transitions; and (3) A \"running ahead\" mechanism that dynamically updates the reference frame's temporal coordinate during inference, keeping its semantic context ahead of the current rollout frame to support long-term coherence. Knot Forcing enables high-fidelity, temporally consistent, and interactive portrait animation over infinite sequences, achieving real-time performance with strong visual stability on consumer-grade GPUs.",
    "arxiv_url": "https://arxiv.org/abs/2512.21734v2",
    "pdf_url": "https://arxiv.org/pdf/2512.21734v2",
    "published_date": "2025-12-25",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "interactive",
      "video diffusion",
      "streaming",
      "dit",
      "identity",
      "video generation",
      "image-to-video",
      "diffusion model",
      "autoregressive",
      "efficient",
      "avatar"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.21734v2",
      "pdf": "https://arxiv.org/pdf/2512.21734v2"
    },
    "bibtex": ""
  },
  {
    "title": "AstraNav-World: World Model for Foresight Control and Consistency",
    "authors": [
      "Junjun Hu",
      "Jintao Chen",
      "Haochen Bai",
      "Minghua Luo",
      "Shichao Xie",
      "Ziyi Chen",
      "Fei Liu",
      "Zedong Chu",
      "Xinda Xue",
      "Botao Ren",
      "Xiaolong Wu",
      "Mu Xu",
      "Shanghang Zhang"
    ],
    "abstract": "Embodied navigation in open, dynamic environments demands accurate foresight of how the world will evolve and how actions will unfold over time. We propose AstraNav-World, an end-to-end world model that jointly reasons about future visual states and action sequences within a unified probabilistic framework. Our framework integrates a diffusion-based video generator with a vision-language policy, enabling synchronized rollouts where predicted scenes and planned actions are updated simultaneously. Training optimizes two complementary objectives: generating action-conditioned multi-step visual predictions and deriving trajectories conditioned on those predicted visuals. This bidirectional constraint makes visual predictions executable and keeps decisions grounded in physically consistent, task-relevant futures, mitigating cumulative errors common in decoupled \"envision-then-plan\" pipelines. Experiments across diverse embodied navigation benchmarks show improved trajectory accuracy and higher success rates. Ablations confirm the necessity of tight vision-action coupling and unified training, with either branch removal degrading both prediction quality and policy reliability. In real-world testing, AstraNav-World demonstrated exceptional zero-shot capabilities, adapting to previously unseen scenarios without any real-world fine-tuning. These results suggest that AstraNav-World captures transferable spatial understanding and planning-relevant navigation dynamics, rather than merely overfitting to simulation-specific data distribution. Overall, by unifying foresight vision and control within a single generative model, we move closer to reliable, interpretable, and general-purpose embodied agents that operate robustly in open-ended real-world settings.",
    "arxiv_url": "https://arxiv.org/abs/2512.21714v1",
    "pdf_url": "https://arxiv.org/pdf/2512.21714v1",
    "published_date": "2025-12-25",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "dit",
      "world model",
      "physical",
      "trajectory",
      "dynamics",
      "simulation",
      "benchmark",
      "action-conditioned"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.21714v1",
      "pdf": "https://arxiv.org/pdf/2512.21714v1"
    },
    "bibtex": ""
  },
  {
    "title": "SVBench: Evaluation of Video Generation Models on Social Reasoning",
    "authors": [
      "Wenshuo Peng",
      "Gongxuan Wang",
      "Tianmeng Yang",
      "Chuanhao Li",
      "Xiaojie Xu",
      "Hui He",
      "Kaipeng Zhang"
    ],
    "abstract": "Recent text-to-video generation models exhibit remarkable progress in visual realism, motion fidelity, and text-video alignment, yet they remain fundamentally limited in their ability to generate socially coherent behavior. Unlike humans, who effortlessly infer intentions, beliefs, emotions, and social norms from brief visual cues, current models tend to render literal scenes without capturing the underlying causal or psychological logic. To systematically evaluate this gap, we introduce the first benchmark for social reasoning in video generation. Grounded in findings from developmental and social psychology, our benchmark organizes thirty classic social cognition paradigms into seven core dimensions, including mental-state inference, goal-directed action, joint attention, social coordination, prosocial behavior, social norms, and multi-agent strategy. To operationalize these paradigms, we develop a fully training-free agent-based pipeline that (i) distills the reasoning mechanism of each experiment, (ii) synthesizes diverse video-ready scenarios, (iii) enforces conceptual neutrality and difficulty control through cue-based critique, and (iv) evaluates generated videos using a high-capacity VLM judge across five interpretable dimensions of social reasoning. Using this framework, we conduct the first large-scale study across seven state-of-the-art video generation systems. Our results reveal substantial performance gaps: while modern models excel in surface-level plausibility, they systematically fail in intention recognition, belief reasoning, joint attention, and prosocial inference.",
    "arxiv_url": "https://arxiv.org/abs/2512.21507v3",
    "pdf_url": "https://arxiv.org/pdf/2512.21507v3",
    "published_date": "2025-12-25",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "text-to-video",
      "video generation",
      "evaluation",
      "concept",
      "benchmark"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.21507v3",
      "pdf": "https://arxiv.org/pdf/2512.21507v3"
    },
    "bibtex": ""
  },
  {
    "title": "GeCo: A Differentiable Geometric Consistency Metric for Video Generation",
    "authors": [
      "Leslie Gu",
      "Junhwa Hur",
      "Charles Herrmann",
      "Fangneng Zhan",
      "Todd Zickler",
      "Deqing Sun",
      "Hanspeter Pfister"
    ],
    "abstract": "We introduce GeCo, a geometry-grounded metric for jointly detecting geometric deformation and occlusion-inconsistency artifacts in static scenes. By fusing residual motion and depth priors, GeCo produces interpretable, dense consistency maps that reveal these artifacts. We use GeCo to systematically benchmark recent video generation models, uncovering common failure modes, and further employ it as a training-free guidance loss to reduce deformation artifacts during video generation.",
    "arxiv_url": "https://arxiv.org/abs/2512.22274v1",
    "pdf_url": "https://arxiv.org/pdf/2512.22274v1",
    "published_date": "2025-12-25",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "benchmark",
      "video generation"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.22274v1",
      "pdf": "https://arxiv.org/pdf/2512.22274v1"
    },
    "bibtex": ""
  },
  {
    "title": "HiStream: Efficient High-Resolution Video Generation via Redundancy-Eliminated Streaming",
    "authors": [
      "Haonan Qiu",
      "Shikun Liu",
      "Zijian Zhou",
      "Zhaochong An",
      "Weiming Ren",
      "Zhiheng Liu",
      "Jonas Schult",
      "Sen He",
      "Shoufa Chen",
      "Yuren Cong",
      "Tao Xiang",
      "Ziwei Liu",
      "Juan-Manuel Perez-Rua"
    ],
    "abstract": "High-resolution video generation, while crucial for digital media and film, is computationally bottlenecked by the quadratic complexity of diffusion models, making practical inference infeasible. To address this, we introduce HiStream, an efficient autoregressive framework that systematically reduces redundancy across three axes: i) Spatial Compression: denoising at low resolution before refining at high resolution with cached features; ii) Temporal Compression: a chunk-by-chunk strategy with a fixed-size anchor cache, ensuring stable inference speed; and iii) Timestep Compression: applying fewer denoising steps to subsequent, cache-conditioned chunks. On 1080p benchmarks, our primary HiStream model (i+ii) achieves state-of-the-art visual quality while demonstrating up to 76.2x faster denoising compared to the Wan2.1 baseline and negligible quality loss. Our faster variant, HiStream+, applies all three optimizations (i+ii+iii), achieving a 107.5x acceleration over the baseline, offering a compelling trade-off between speed and quality, thereby making high-resolution video generation both practical and scalable.",
    "arxiv_url": "https://arxiv.org/abs/2512.21338v2",
    "pdf_url": "https://arxiv.org/pdf/2512.21338v2",
    "published_date": "2025-12-24",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "acceleration",
      "streaming",
      "dit",
      "video generation",
      "efficient",
      "film",
      "diffusion model",
      "denoising",
      "autoregressive",
      "benchmark"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.21338v2",
      "pdf": "https://arxiv.org/pdf/2512.21338v2"
    },
    "bibtex": ""
  },
  {
    "title": "ACD: Direct Conditional Control for Video Diffusion Models via Attention Supervision",
    "authors": [
      "Weiqi Li",
      "Zehao Zhang",
      "Liang Lin",
      "Guangrun Wang"
    ],
    "abstract": "Controllability is a fundamental requirement in video synthesis, where accurate alignment with conditioning signals is essential. Existing classifier-free guidance methods typically achieve conditioning indirectly by modeling the joint distribution of data and conditions, which often results in limited controllability over the specified conditions. Classifier-based guidance enforces conditions through an external classifier, but the model may exploit this mechanism to raise the classifier score without genuinely satisfying the intended condition, resulting in adversarial artifacts and limited effective controllability. In this paper, we propose Attention-Conditional Diffusion (ACD), a novel framework for direct conditional control in video diffusion models via attention supervision. By aligning the model's attention maps with external control signals, ACD achieves better controllability. To support this, we introduce a sparse 3D-aware object layout as an efficient conditioning signal, along with a dedicated Layout ControlNet and an automated annotation pipeline for scalable layout integration. Extensive experiments on benchmark video generation datasets demonstrate that ACD delivers superior alignment with conditioning inputs while preserving temporal coherence and visual fidelity, establishing an effective paradigm for conditional video synthesis.",
    "arxiv_url": "https://arxiv.org/abs/2512.21268v1",
    "pdf_url": "https://arxiv.org/pdf/2512.21268v1",
    "published_date": "2025-12-24",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "layout",
      "video diffusion",
      "3d-aware",
      "dit",
      "video generation",
      "efficient",
      "video synthesis",
      "diffusion model",
      "benchmark"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.21268v1",
      "pdf": "https://arxiv.org/pdf/2512.21268v1"
    },
    "bibtex": ""
  },
  {
    "title": "DreaMontage: Arbitrary Frame-Guided One-Shot Video Generation",
    "authors": [
      "Jiawei Liu",
      "Junqiao Li",
      "Jiangfan Deng",
      "Gen Li",
      "Siyu Zhou",
      "Zetao Fang",
      "Shanshan Lao",
      "Zengde Deng",
      "Jianing Zhu",
      "Tingting Ma",
      "Jiayi Li",
      "Yunqiu Wang",
      "Qian He",
      "Xinglong Wu"
    ],
    "abstract": "The \"one-shot\" technique represents a distinct and sophisticated aesthetic in filmmaking. However, its practical realization is often hindered by prohibitive costs and complex real-world constraints. Although emerging video generation models offer a virtual alternative, existing approaches typically rely on naive clip concatenation, which frequently fails to maintain visual smoothness and temporal coherence. In this paper, we introduce DreaMontage, a comprehensive framework designed for arbitrary frame-guided generation, capable of synthesizing seamless, expressive, and long-duration one-shot videos from diverse user-provided inputs. To achieve this, we address the challenge through three primary dimensions. (i) We integrate a lightweight intermediate-conditioning mechanism into the DiT architecture. By employing an Adaptive Tuning strategy that effectively leverages base training data, we unlock robust arbitrary-frame control capabilities. (ii) To enhance visual fidelity and cinematic expressiveness, we curate a high-quality dataset and implement a Visual Expression SFT stage. In addressing critical issues such as subject motion rationality and transition smoothness, we apply a Tailored DPO scheme, which significantly improves the success rate and usability of the generated content. (iii) To facilitate the production of extended sequences, we design a Segment-wise Auto-Regressive (SAR) inference strategy that operates in a memory-efficient manner. Extensive experiments demonstrate that our approach achieves visually striking and seamlessly coherent one-shot effects while maintaining computational efficiency, empowering users to transform fragmented visual materials into vivid, cohesive one-shot cinematic experiences.",
    "arxiv_url": "https://arxiv.org/abs/2512.21252v2",
    "pdf_url": "https://arxiv.org/pdf/2512.21252v2",
    "published_date": "2025-12-24",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "dit",
      "video generation",
      "film",
      "architecture",
      "efficient"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.21252v2",
      "pdf": "https://arxiv.org/pdf/2512.21252v2"
    },
    "bibtex": ""
  },
  {
    "title": "T2AV-Compass: Towards Unified Evaluation for Text-to-Audio-Video Generation",
    "authors": [
      "Zhe Cao",
      "Tao Wang",
      "Jiaming Wang",
      "Yanghai Wang",
      "Yuanxing Zhang",
      "Jialu Chen",
      "Miao Deng",
      "Jiahao Wang",
      "Yubin Guo",
      "Chenxi Liao",
      "Yize Zhang",
      "Zhaoxiang Zhang",
      "Jiaheng Liu"
    ],
    "abstract": "Text-to-Audio-Video (T2AV) generation aims to synthesize temporally coherent video and semantically synchronized audio from natural language, yet its evaluation remains fragmented, often relying on unimodal metrics or narrowly scoped benchmarks that fail to capture cross-modal alignment, instruction following, and perceptual realism under complex prompts. To address this limitation, we present T2AV-Compass, a unified benchmark for comprehensive evaluation of T2AV systems, consisting of 500 diverse and complex prompts constructed via a taxonomy-driven pipeline to ensure semantic richness and physical plausibility. Besides, T2AV-Compass introduces a dual-level evaluation framework that integrates objective signal-level metrics for video quality, audio quality, and cross-modal alignment with a subjective MLLM-as-a-Judge protocol for instruction following and realism assessment. Extensive evaluation of 11 representative T2AVsystems reveals that even the strongest models fall substantially short of human-level realism and cross-modal consistency, with persistent failures in audio realism, fine-grained synchronization, instruction following, etc. These results indicate significant improvement room for future models and highlight the value of T2AV-Compass as a challenging and diagnostic testbed for advancing text-to-audio-video generation.",
    "arxiv_url": "https://arxiv.org/abs/2512.21094v1",
    "pdf_url": "https://arxiv.org/pdf/2512.21094v1",
    "published_date": "2025-12-24",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "benchmark",
      "evaluation",
      "physical",
      "video generation"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.21094v1",
      "pdf": "https://arxiv.org/pdf/2512.21094v1"
    },
    "bibtex": ""
  },
  {
    "title": "Learning from Next-Frame Prediction: Autoregressive Video Modeling Encodes Effective Representations",
    "authors": [
      "Jinghan Li",
      "Yang Jin",
      "Hao Jiang",
      "Yadong Mu",
      "Yang Song",
      "Kun Xu"
    ],
    "abstract": "Recent advances in pretraining general foundation models have significantly improved performance across diverse downstream tasks. While autoregressive (AR) generative models like GPT have revolutionized NLP, most visual generative pretraining methods still rely on BERT-style masked modeling, which often disregards the temporal information essential for video analysis. The few existing autoregressive visual pretraining methods suffer from issues such as inaccurate semantic localization and poor generation quality, leading to poor semantics. In this work, we propose NExT-Vid, a novel autoregressive visual generative pretraining framework that utilizes masked next-frame prediction to jointly model images and videos. NExT-Vid introduces a context-isolated autoregressive predictor to decouple semantic representation from target decoding, and a conditioned flow-matching decoder to enhance generation quality and diversity. Through context-isolated flow-matching pretraining, our approach achieves strong representations. Extensive experiments on large-scale pretrained models demonstrate that our proposed method consistently outperforms previous generative pretraining methods for visual representation learning via attentive probing in downstream classification.",
    "arxiv_url": "https://arxiv.org/abs/2512.21004v1",
    "pdf_url": "https://arxiv.org/pdf/2512.21004v1",
    "published_date": "2025-12-24",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "style",
      "autoregressive",
      "dit"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.21004v1",
      "pdf": "https://arxiv.org/pdf/2512.21004v1"
    },
    "bibtex": ""
  },
  {
    "title": "SemanticGen: Video Generation in Semantic Space",
    "authors": [
      "Jianhong Bai",
      "Xiaoshi Wu",
      "Xintao Wang",
      "Xiao Fu",
      "Yuanxing Zhang",
      "Qinghe Wang",
      "Xiaoyu Shi",
      "Menghan Xia",
      "Zuozhu Liu",
      "Haoji Hu",
      "Pengfei Wan",
      "Kun Gai"
    ],
    "abstract": "State-of-the-art video generative models typically learn the distribution of video latents in the VAE space and map them to pixels using a VAE decoder. While this approach can generate high-quality videos, it suffers from slow convergence and is computationally expensive when generating long videos. In this paper, we introduce SemanticGen, a novel solution to address these limitations by generating videos in the semantic space. Our main insight is that, due to the inherent redundancy in videos, the generation process should begin in a compact, high-level semantic space for global planning, followed by the addition of high-frequency details, rather than directly modeling a vast set of low-level video tokens using bi-directional attention. SemanticGen adopts a two-stage generation process. In the first stage, a diffusion model generates compact semantic video features, which define the global layout of the video. In the second stage, another diffusion model generates VAE latents conditioned on these semantic features to produce the final output. We observe that generation in the semantic space leads to faster convergence compared to the VAE latent space. Our method is also effective and computationally efficient when extended to long video generation. Extensive experiments demonstrate that SemanticGen produces high-quality videos and outperforms state-of-the-art approaches and strong baselines.",
    "arxiv_url": "https://arxiv.org/abs/2512.20619v3",
    "pdf_url": "https://arxiv.org/pdf/2512.20619v3",
    "published_date": "2025-12-23",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "layout",
      "dit",
      "video generation",
      "diffusion model",
      "long video",
      "efficient"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.20619v3",
      "pdf": "https://arxiv.org/pdf/2512.20619v3"
    },
    "bibtex": ""
  },
  {
    "title": "Repurposing Video Diffusion Transformers for Robust Point Tracking",
    "authors": [
      "Soowon Son",
      "Honggyu An",
      "Chaehyun Kim",
      "Hyunah Ko",
      "Jisu Nam",
      "Dahyun Chung",
      "Siyoon Jin",
      "Jung Yi",
      "Jaewon Min",
      "Junhwa Hur",
      "Seungryong Kim"
    ],
    "abstract": "Point tracking aims to localize corresponding points across video frames, serving as a fundamental task for 4D reconstruction, robotics, and video editing. Existing methods commonly rely on shallow convolutional backbones such as ResNet that process frames independently, lacking temporal coherence and producing unreliable matching costs under challenging conditions. Through systematic analysis, we find that video Diffusion Transformers (DiTs), pre-trained on large-scale real-world videos with spatio-temporal attention, inherently exhibit strong point tracking capability and robustly handle dynamic motions and frequent occlusions. We propose DiTracker, which adapts video DiTs through: (1) query-key attention matching, (2) lightweight LoRA tuning, and (3) cost fusion with a ResNet backbone. Despite training with 8 times smaller batch size, DiTracker achieves state-of-the-art performance on challenging ITTO benchmark and matches or outperforms state-of-the-art models on TAP-Vid benchmarks. Our work validates video DiT features as an effective and efficient foundation for point tracking.",
    "arxiv_url": "https://arxiv.org/abs/2512.20606v1",
    "pdf_url": "https://arxiv.org/pdf/2512.20606v1",
    "published_date": "2025-12-23",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "video diffusion",
      "dit",
      "video editing",
      "diffusion transformer",
      "robotics",
      "efficient",
      "benchmark"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.20606v1",
      "pdf": "https://arxiv.org/pdf/2512.20606v1"
    },
    "bibtex": ""
  },
  {
    "title": "FlashVLM: Text-Guided Visual Token Selection for Large Multimodal Models",
    "authors": [
      "Kaitong Cai",
      "Jusheng Zhang",
      "Jing Yang",
      "Yijia Fan",
      "Pengtao Xie",
      "Jian Wang",
      "Keze Wang"
    ],
    "abstract": "Large vision-language models (VLMs) typically process hundreds or thousands of visual tokens per image or video frame, incurring quadratic attention cost and substantial redundancy. Existing token reduction methods often ignore the textual query or rely on deep attention maps, whose instability under aggressive pruning leads to degraded semantic alignment.   We propose FlashVLM, a text guided visual token selection framework that dynamically adapts visual inputs to the query. Instead of relying on noisy attention weights, FlashVLM computes an explicit cross modal similarity between projected image tokens and normalized text embeddings in the language model space. This extrinsic relevance is fused with intrinsic visual saliency using log domain weighting and temperature controlled sharpening. In addition, a diversity preserving partition retains a minimal yet representative set of background tokens to maintain global context.   Under identical token budgets and evaluation protocols, FlashVLM achieves beyond lossless compression, slightly surpassing the unpruned baseline while pruning up to 77.8 percent of visual tokens on LLaVA 1.5, and maintaining 92.8 percent accuracy even under 94.4 percent compression. Extensive experiments on 14 image and video benchmarks demonstrate that FlashVLM delivers state of the art efficiency performance trade offs while maintaining strong robustness and generalization across mainstream VLMs.",
    "arxiv_url": "https://arxiv.org/abs/2512.20561v1",
    "pdf_url": "https://arxiv.org/pdf/2512.20561v1",
    "published_date": "2025-12-23",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "benchmark",
      "evaluation",
      "dit"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.20561v1",
      "pdf": "https://arxiv.org/pdf/2512.20561v1"
    },
    "bibtex": ""
  },
  {
    "title": "Advancing Multimodal Teacher Sentiment Analysis:The Large-Scale T-MED Dataset & The Effective AAM-TSA Model",
    "authors": [
      "Zhiyi Duan",
      "Xiangren Wang",
      "Hongyu Yuan",
      "Qianli Xing"
    ],
    "abstract": "Teachers' emotional states are critical in educational scenarios, profoundly impacting teaching efficacy, student engagement, and learning achievements. However, existing studies often fail to accurately capture teachers' emotions due to the performative nature and overlook the critical impact of instructional information on emotional expression. In this paper, we systematically investigate teacher sentiment analysis by building both the dataset and the model accordingly. We construct the first large-scale teacher multimodal sentiment analysis dataset, T-MED. To ensure labeling accuracy and efficiency, we employ a human-machine collaborative labeling process. The T-MED dataset includes 14,938 instances of teacher emotional data from 250 real classrooms across 11 subjects ranging from K-12 to higher education, integrating multimodal text, audio, video, and instructional information. Furthermore, we propose a novel asymmetric attention-based multimodal teacher sentiment analysis model, AAM-TSA. AAM-TSA introduces an asymmetric attention mechanism and hierarchical gating unit to enable differentiated cross-modal feature fusion and precise emotional classification. Experimental results demonstrate that AAM-TSA significantly outperforms existing state-of-the-art methods in terms of accuracy and interpretability on the T-MED dataset.",
    "arxiv_url": "https://arxiv.org/abs/2512.20548v2",
    "pdf_url": "https://arxiv.org/pdf/2512.20548v2",
    "published_date": "2025-12-23",
    "categories": [
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "education"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.20548v2",
      "pdf": "https://arxiv.org/pdf/2512.20548v2"
    },
    "bibtex": ""
  },
  {
    "title": "The devil is in the details: Enhancing Video Virtual Try-On via Keyframe-Driven Details Injection",
    "authors": [
      "Qingdong He",
      "Xueqin Chen",
      "Yanjie Pan",
      "Peng Tang",
      "Pengcheng Xu",
      "Zhenye Gan",
      "Chengjie Wang",
      "Xiaobin Hu",
      "Jiangning Zhang",
      "Yabiao Wang"
    ],
    "abstract": "Although diffusion transformer (DiT)-based video virtual try-on (VVT) has made significant progress in synthesizing realistic videos, existing methods still struggle to capture fine-grained garment dynamics and preserve background integrity across video frames. They also incur high computational costs due to additional interaction modules introduced into DiTs, while the limited scale and quality of existing public datasets also restrict model generalization and effective training. To address these challenges, we propose a novel framework, KeyTailor, along with a large-scale, high-definition dataset, ViT-HD. The core idea of KeyTailor is a keyframe-driven details injection strategy, motivated by the fact that keyframes inherently contain both foreground dynamics and background consistency. Specifically, KeyTailor adopts an instruction-guided keyframe sampling strategy to filter informative frames from the input video. Subsequently,two tailored keyframe-driven modules, the garment details enhancement module and the collaborative background optimization module, are employed to distill garment dynamics into garment-related latents and to optimize the integrity of background latents, both guided by keyframes.These enriched details are then injected into standard DiT blocks together with pose, mask, and noise latents, enabling efficient and realistic try-on video synthesis. This design ensures consistency without explicitly modifying the DiT architecture, while simultaneously avoiding additional complexity. In addition, our dataset ViT-HD comprises 15, 070 high-quality video samples at a resolution of 810*1080, covering diverse garments. Extensive experiments demonstrate that KeyTailor outperforms state-of-the-art baselines in terms of garment fidelity and background integrity across both dynamic and static scenarios.",
    "arxiv_url": "https://arxiv.org/abs/2512.20340v1",
    "pdf_url": "https://arxiv.org/pdf/2512.20340v1",
    "published_date": "2025-12-23",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "dit",
      "virtual try-on",
      "diffusion transformer",
      "video synthesis",
      "architecture",
      "dynamics",
      "efficient"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.20340v1",
      "pdf": "https://arxiv.org/pdf/2512.20340v1"
    },
    "bibtex": ""
  },
  {
    "title": "Learning Skills from Action-Free Videos",
    "authors": [
      "Hung-Chieh Fang",
      "Kuo-Han Hung",
      "Chu-Rong Chen",
      "Po-Jung Chou",
      "Chun-Kai Yang",
      "Po-Chen Ko",
      "Yu-Chiang Wang",
      "Yueh-Hua Wu",
      "Min-Hung Chen",
      "Shao-Hua Sun"
    ],
    "abstract": "Learning from videos offers a promising path toward generalist robots by providing rich visual and temporal priors beyond what real robot datasets contain. While existing video generative models produce impressive visual predictions, they are difficult to translate into low-level actions. Conversely, latent-action models better align videos with actions, but they typically operate at the single-step level and lack high-level planning capabilities. We bridge this gap by introducing Skill Abstraction from Optical Flow (SOF), a framework that learns latent skills from large collections of action-free videos. Our key idea is to learn a latent skill space through an intermediate representation based on optical flow that captures motion information aligned with both video dynamics and robot actions. By learning skills in this flow-based latent space, SOF enables high-level planning over video-derived skills and allows for easier translation of these skills into actions. Experiments show that our approach consistently improves performance in both multitask and long-horizon settings, demonstrating the ability to acquire and compose skills directly from raw visual data.",
    "arxiv_url": "https://arxiv.org/abs/2512.20052v1",
    "pdf_url": "https://arxiv.org/pdf/2512.20052v1",
    "published_date": "2025-12-23",
    "categories": [
      "cs.AI",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "dynamics"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.20052v1",
      "pdf": "https://arxiv.org/pdf/2512.20052v1"
    },
    "bibtex": ""
  },
  {
    "title": "Few-Shot-Based Modular Image-to-Video Adapter for Diffusion Models",
    "authors": [
      "Zhenhao Li",
      "Shaohan Yi",
      "Zheng Liu",
      "Leonartinus Gao",
      "Minh Ngoc Le",
      "Ambrose Ling",
      "Zhuoran Wang",
      "Md Amirul Islam",
      "Zhixiang Chi",
      "Yuanhao Yu"
    ],
    "abstract": "Diffusion models (DMs) have recently achieved impressive photorealism in image and video generation. However, their application to image animation remains limited, even when trained on large-scale datasets. Two primary challenges contribute to this: the high dimensionality of video signals leads to a scarcity of training data, causing DMs to favor memorization over prompt compliance when generating motion; moreover, DMs struggle to generalize to novel motion patterns not present in the training set, and fine-tuning them to learn such patterns, especially using limited training data, is still under-explored. To address these limitations, we propose Modular Image-to-Video Adapter (MIVA), a lightweight sub-network attachable to a pre-trained DM, each designed to capture a single motion pattern and scalable via parallelization. MIVAs can be efficiently trained on approximately ten samples using a single consumer-grade GPU. At inference time, users can specify motion by selecting one or multiple MIVAs, eliminating the need for prompt engineering. Extensive experiments demonstrate that MIVA enables more precise motion control while maintaining, or even surpassing, the generation quality of models trained on significantly larger datasets.",
    "arxiv_url": "https://arxiv.org/abs/2512.20000v2",
    "pdf_url": "https://arxiv.org/pdf/2512.20000v2",
    "published_date": "2025-12-23",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "video generation",
      "image-to-video",
      "efficient",
      "diffusion model",
      "motion control",
      "image animation"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.20000v2",
      "pdf": "https://arxiv.org/pdf/2512.20000v2"
    },
    "bibtex": ""
  },
  {
    "title": "How Much 3D Do Video Foundation Models Encode?",
    "authors": [
      "Zixuan Huang",
      "Xiang Li",
      "Zhaoyang Lv",
      "James M. Rehg"
    ],
    "abstract": "Videos are continuous 2D projections of 3D worlds. After training on large video data, will global 3D understanding naturally emerge? We study this by quantifying the 3D understanding of existing Video Foundation Models (VidFMs) pretrained on vast video data. We propose the first model-agnostic framework that measures the 3D awareness of various VidFMs by estimating multiple 3D properties from their features via shallow read-outs. Our study presents meaningful findings regarding the 3D awareness of VidFMs on multiple axes. In particular, we show that state-of-the-art video generation models exhibit a strong understanding of 3D objects and scenes, despite not being trained on any 3D data. Such understanding can even surpass that of large expert models specifically trained for 3D tasks. Our findings, together with the 3D benchmarking of major VidFMs, provide valuable observations for building scalable 3D models.",
    "arxiv_url": "https://arxiv.org/abs/2512.19949v1",
    "pdf_url": "https://arxiv.org/pdf/2512.19949v1",
    "published_date": "2025-12-23",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "benchmark",
      "video generation"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.19949v1",
      "pdf": "https://arxiv.org/pdf/2512.19949v1"
    },
    "bibtex": ""
  },
  {
    "title": "Learning to Refocus with Video Diffusion Models",
    "authors": [
      "SaiKiran Tedla",
      "Zhoutong Zhang",
      "Xuaner Zhang",
      "Shumian Xin"
    ],
    "abstract": "Focus is a cornerstone of photography, yet autofocus systems often fail to capture the intended subject, and users frequently wish to adjust focus after capture. We introduce a novel method for realistic post-capture refocusing using video diffusion models. From a single defocused image, our approach generates a perceptually accurate focal stack, represented as a video sequence, enabling interactive refocusing and unlocking a range of downstream applications. We release a large-scale focal stack dataset acquired under diverse real-world smartphone conditions to support this work and future research. Our method consistently outperforms existing approaches in both perceptual quality and robustness across challenging scenarios, paving the way for more advanced focus-editing capabilities in everyday photography. Code and data are available at www.learn2refocus.github.io",
    "arxiv_url": "https://arxiv.org/abs/2512.19823v3",
    "pdf_url": "https://arxiv.org/pdf/2512.19823v3",
    "published_date": "2025-12-22",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "diffusion model",
      "interactive",
      "video diffusion",
      "dit"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.19823v3",
      "pdf": "https://arxiv.org/pdf/2512.19823v3"
    },
    "bibtex": ""
  },
  {
    "title": "Generating the Past, Present and Future from a Motion-Blurred Image",
    "authors": [
      "SaiKiran Tedla",
      "Kelly Zhu",
      "Trevor Canham",
      "Felix Taubner",
      "Michael S. Brown",
      "Kiriakos N. Kutulakos",
      "David B. Lindell"
    ],
    "abstract": "We seek to answer the question: what can a motion-blurred image reveal about a scene's past, present, and future? Although motion blur obscures image details and degrades visual quality, it also encodes information about scene and camera motion during an exposure. Previous techniques leverage this information to estimate a sharp image from an input blurry one, or to predict a sequence of video frames showing what might have occurred at the moment of image capture. However, they rely on handcrafted priors or network architectures to resolve ambiguities in this inverse problem, and do not incorporate image and video priors on large-scale datasets. As such, existing methods struggle to reproduce complex scene dynamics and do not attempt to recover what occurred before or after an image was taken. Here, we introduce a new technique that repurposes a pre-trained video diffusion model trained on internet-scale datasets to recover videos revealing complex scene dynamics during the moment of capture and what might have occurred immediately into the past or future. Our approach is robust and versatile; it outperforms previous methods for this task, generalizes to challenging in-the-wild images, and supports downstream tasks such as recovering camera trajectories, object motion, and dynamic 3D scene structure. Code and data are available at https://blur2vid.github.io",
    "arxiv_url": "https://arxiv.org/abs/2512.19817v1",
    "pdf_url": "https://arxiv.org/pdf/2512.19817v1",
    "published_date": "2025-12-22",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "diffusion model",
      "video diffusion",
      "architecture",
      "dynamics"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.19817v1",
      "pdf": "https://arxiv.org/pdf/2512.19817v1",
      "project": "https://blur2vid.github.io"
    },
    "bibtex": ""
  },
  {
    "title": "Pushing the Frontier of Audiovisual Perception with Large-Scale Multimodal Correspondence Learning",
    "authors": [
      "Apoorv Vyas",
      "Heng-Jui Chang",
      "Cheng-Fu Yang",
      "Po-Yao Huang",
      "Luya Gao",
      "Julius Richter",
      "Sanyuan Chen",
      "Matt Le",
      "Piotr Dollr",
      "Christoph Feichtenhofer",
      "Ann Lee",
      "Wei-Ning Hsu"
    ],
    "abstract": "We introduce Perception Encoder Audiovisual, PE-AV, a new family of encoders for audio and video understanding trained with scaled contrastive learning. Built on PE, PE-AV makes several key contributions to extend representations to audio, and natively support joint embeddings across audio-video, audio-text, and video-text modalities. PE-AV's unified cross-modal embeddings enable novel tasks such as speech retrieval, and set a new state of the art across standard audio and video benchmarks. We unlock this by building a strong audiovisual data engine that synthesizes high-quality captions for O(100M) audio-video pairs, enabling large-scale supervision consistent across modalities. Our audio data includes speech, music, and general sound effects-avoiding single-domain limitations common in prior work. We exploit ten pairwise contrastive objectives, showing that scaling cross-modality and caption-type pairs strengthens alignment and improves zero-shot performance. We further develop PE-A-Frame by fine-tuning PE-AV with frame-level contrastive objectives, enabling fine-grained audio-frame-to-text alignment for tasks such as sound event detection.",
    "arxiv_url": "https://arxiv.org/abs/2512.19687v1",
    "pdf_url": "https://arxiv.org/pdf/2512.19687v1",
    "published_date": "2025-12-22",
    "categories": [
      "cs.SD",
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "benchmark",
      "sound"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.19687v1",
      "pdf": "https://arxiv.org/pdf/2512.19687v1"
    },
    "bibtex": ""
  },
  {
    "title": "WorldWarp: Propagating 3D Geometry with Asynchronous Video Diffusion",
    "authors": [
      "Hanyang Kong",
      "Xingyi Yang",
      "Xiaoxu Zheng",
      "Xinchao Wang"
    ],
    "abstract": "Generating long-range, geometrically consistent video presents a fundamental dilemma: while consistency demands strict adherence to 3D geometry in pixel space, state-of-the-art generative models operate most effectively in a camera-conditioned latent space. This disconnect causes current methods to struggle with occluded areas and complex camera trajectories. To bridge this gap, we propose WorldWarp, a framework that couples a 3D structural anchor with a 2D generative refiner. To establish geometric grounding, WorldWarp maintains an online 3D geometric cache built via Gaussian Splatting (3DGS). By explicitly warping historical content into novel views, this cache acts as a structural scaffold, ensuring each new frame respects prior geometry. However, static warping inevitably leaves holes and artifacts due to occlusions. We address this using a Spatio-Temporal Diffusion (ST-Diff) model designed for a \"fill-and-revise\" objective. Our key innovation is a spatio-temporal varying noise schedule: blank regions receive full noise to trigger generation, while warped regions receive partial noise to enable refinement. By dynamically updating the 3D cache at every step, WorldWarp maintains consistency across video chunks. Consequently, it achieves state-of-the-art fidelity by ensuring that 3D logic guides structure while diffusion logic perfects texture. Project page: \\href{https://hyokong.github.io/worldwarp-page/}{https://hyokong.github.io/worldwarp-page/}.",
    "arxiv_url": "https://arxiv.org/abs/2512.19678v1",
    "pdf_url": "https://arxiv.org/pdf/2512.19678v1",
    "published_date": "2025-12-22",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "novel view",
      "video diffusion",
      "dit"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.19678v1",
      "pdf": "https://arxiv.org/pdf/2512.19678v1",
      "project": "https://hyokong.github.io/worldwarp-page"
    },
    "bibtex": ""
  },
  {
    "title": "Over++: Generative Video Compositing for Layer Interaction Effects",
    "authors": [
      "Luchao Qi",
      "Jiaye Wu",
      "Jun Myeong Choi",
      "Cary Phillips",
      "Roni Sengupta",
      "Dan B Goldman"
    ],
    "abstract": "In professional video compositing workflows, artists must manually create environmental interactions-such as shadows, reflections, dust, and splashes-between foreground subjects and background layers. Existing video generative models struggle to preserve the input video while adding such effects, and current video inpainting methods either require costly per-frame masks or yield implausible results. We introduce augmented compositing, a new task that synthesizes realistic, semi-transparent environmental effects conditioned on text prompts and input video layers, while preserving the original scene. To address this task, we present Over++, a video effect generation framework that makes no assumptions about camera pose, scene stationarity, or depth supervision. We construct a paired effect dataset tailored for this task and introduce an unpaired augmentation strategy that preserves text-driven editability. Our method also supports optional mask control and keyframe guidance without requiring dense annotations. Despite training on limited data, Over++ produces diverse and realistic environmental effects and outperforms existing baselines in both effect generation and scene preservation.",
    "arxiv_url": "https://arxiv.org/abs/2512.19661v1",
    "pdf_url": "https://arxiv.org/pdf/2512.19661v1",
    "published_date": "2025-12-22",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "video inpainting",
      "dit"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.19661v1",
      "pdf": "https://arxiv.org/pdf/2512.19661v1"
    },
    "bibtex": ""
  },
  {
    "title": "StoryMem: Multi-shot Long Video Storytelling with Memory",
    "authors": [
      "Kaiwen Zhang",
      "Liming Jiang",
      "Angtian Wang",
      "Jacob Zhiyuan Fang",
      "Tiancheng Zhi",
      "Qing Yan",
      "Hao Kang",
      "Xin Lu",
      "Xingang Pan"
    ],
    "abstract": "Visual storytelling requires generating multi-shot videos with cinematic quality and long-range consistency. Inspired by human memory, we propose StoryMem, a paradigm that reformulates long-form video storytelling as iterative shot synthesis conditioned on explicit visual memory, transforming pre-trained single-shot video diffusion models into multi-shot storytellers. This is achieved by a novel Memory-to-Video (M2V) design, which maintains a compact and dynamically updated memory bank of keyframes from historical generated shots. The stored memory is then injected into single-shot video diffusion models via latent concatenation and negative RoPE shifts with only LoRA fine-tuning. A semantic keyframe selection strategy, together with aesthetic preference filtering, further ensures informative and stable memory throughout generation. Moreover, the proposed framework naturally accommodates smooth shot transitions and customized story generation applications. To facilitate evaluation, we introduce ST-Bench, a diverse benchmark for multi-shot video storytelling. Extensive experiments demonstrate that StoryMem achieves superior cross-shot consistency over previous methods while preserving high aesthetic quality and prompt adherence, marking a significant step toward coherent minute-long video storytelling.",
    "arxiv_url": "https://arxiv.org/abs/2512.19539v1",
    "pdf_url": "https://arxiv.org/pdf/2512.19539v1",
    "published_date": "2025-12-22",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "video diffusion",
      "dit",
      "long-form",
      "evaluation",
      "diffusion model",
      "long video",
      "benchmark"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.19539v1",
      "pdf": "https://arxiv.org/pdf/2512.19539v1"
    },
    "bibtex": ""
  },
  {
    "title": "Real2Edit2Real: Generating Robotic Demonstrations via a 3D Control Interface",
    "authors": [
      "Yujie Zhao",
      "Hongwei Fan",
      "Di Chen",
      "Shengcong Chen",
      "Liliang Chen",
      "Xiaoqi Li",
      "Guanghui Ren",
      "Hao Dong"
    ],
    "abstract": "Recent progress in robot learning has been driven by large-scale datasets and powerful visuomotor policy architectures, yet policy robustness remains limited by the substantial cost of collecting diverse demonstrations, particularly for spatial generalization in manipulation tasks. To reduce repetitive data collection, we present Real2Edit2Real, a framework that generates new demonstrations by bridging 3D editability with 2D visual data through a 3D control interface. Our approach first reconstructs scene geometry from multi-view RGB observations with a metric-scale 3D reconstruction model. Based on the reconstructed geometry, we perform depth-reliable 3D editing on point clouds to generate new manipulation trajectories while geometrically correcting the robot poses to recover physically consistent depth, which serves as a reliable condition for synthesizing new demonstrations. Finally, we propose a multi-conditional video generation model guided by depth as the primary control signal, together with action, edge, and ray maps, to synthesize spatially augmented multi-view manipulation videos. Experiments on four real-world manipulation tasks demonstrate that policies trained on data generated from only 1-5 source demonstrations can match or outperform those trained on 50 real-world demonstrations, improving data efficiency by up to 10-50x. Moreover, experimental results on height and texture editing demonstrate the framework's flexibility and extensibility, indicating its potential to serve as a unified data generation framework.",
    "arxiv_url": "https://arxiv.org/abs/2512.19402v1",
    "pdf_url": "https://arxiv.org/pdf/2512.19402v1",
    "published_date": "2025-12-22",
    "categories": [
      "cs.RO",
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "architecture",
      "physical",
      "video generation",
      "dit"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.19402v1",
      "pdf": "https://arxiv.org/pdf/2512.19402v1"
    },
    "bibtex": ""
  },
  {
    "title": "WaTeRFlow: Watermark Temporal Robustness via Flow Consistency",
    "authors": [
      "Utae Jeong",
      "Sumin In",
      "Hyunju Ryu",
      "Jaewan Choi",
      "Feng Yang",
      "Jongheon Jeong",
      "Seungryong Kim",
      "Sangpil Kim"
    ],
    "abstract": "Image watermarking supports authenticity and provenance, yet many schemes are still easy to bypass with various distortions and powerful generative edits. Deep learning-based watermarking has improved robustness to diffusion-based image editing, but a gap remains when a watermarked image is converted to video by image-to-video (I2V), in which per-frame watermark detection weakens. I2V has quickly advanced from short, jittery clips to multi-second, temporally coherent scenes, and it now serves not only content creation but also world-modeling and simulation workflows, making cross-modal watermark recovery crucial. We present WaTeRFlow, a framework tailored for robustness under I2V. It consists of (i) FUSE (Flow-guided Unified Synthesis Engine), which exposes the encoder-decoder to realistic distortions via instruction-driven edits and a fast video diffusion proxy during training, (ii) optical-flow warping with a Temporal Consistency Loss (TCL) that stabilizes per-frame predictions, and (iii) a semantic preservation loss that maintains the conditioning signal. Experiments across representative I2V models show accurate watermark recovery from frames, with higher first-frame and per-frame bit accuracy and resilience when various distortions are applied before or after video generation.",
    "arxiv_url": "https://arxiv.org/abs/2512.19048v1",
    "pdf_url": "https://arxiv.org/pdf/2512.19048v1",
    "published_date": "2025-12-22",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "temporal consistency",
      "video diffusion",
      "dit",
      "video generation",
      "image-to-video",
      "i2v",
      "simulation"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.19048v1",
      "pdf": "https://arxiv.org/pdf/2512.19048v1"
    },
    "bibtex": ""
  },
  {
    "title": "CETCAM: Camera-Controllable Video Generation via Consistent and Extensible Tokenization",
    "authors": [
      "Zelin Zhao",
      "Xinyu Gong",
      "Bangya Liu",
      "Ziyang Song",
      "Jun Zhang",
      "Suhui Wu",
      "Yongxin Chen",
      "Hao Zhang"
    ],
    "abstract": "Achieving precise camera control in video generation remains challenging, as existing methods often rely on camera pose annotations that are difficult to scale to large and dynamic datasets and are frequently inconsistent with depth estimation, leading to train-test discrepancies. We introduce CETCAM, a camera-controllable video generation framework that eliminates the need for camera annotations through a consistent and extensible tokenization scheme. CETCAM leverages recent advances in geometry foundation models, such as VGGT, to estimate depth and camera parameters and converts them into unified, geometry-aware tokens. These tokens are seamlessly integrated into a pretrained video diffusion backbone via lightweight context blocks. Trained in two progressive stages, CETCAM first learns robust camera controllability from diverse raw video data and then refines fine-grained visual quality using curated high-fidelity datasets. Extensive experiments across multiple benchmarks demonstrate state-of-the-art geometric consistency, temporal stability, and visual realism. Moreover, CETCAM exhibits strong adaptability to additional control modalities, including inpainting and layout control, highlighting its flexibility beyond camera control. The project page is available at https://sjtuytc.github.io/CETCam_project_page.github.io/.",
    "arxiv_url": "https://arxiv.org/abs/2512.19020v1",
    "pdf_url": "https://arxiv.org/pdf/2512.19020v1",
    "published_date": "2025-12-22",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "layout",
      "controllable",
      "video diffusion",
      "dit",
      "video generation",
      "benchmark",
      "camera control"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.19020v1",
      "pdf": "https://arxiv.org/pdf/2512.19020v1",
      "project": "https://sjtuytc.github.io/CETCam_project_page.github.io"
    },
    "bibtex": ""
  },
  {
    "title": "EchoMotion: Unified Human Video and Motion Generation via Dual-Modality Diffusion Transformer",
    "authors": [
      "Yuxiao Yang",
      "Hualian Sheng",
      "Sijia Cai",
      "Jing Lin",
      "Jiahao Wang",
      "Bing Deng",
      "Junzhe Lu",
      "Haoqian Wang",
      "Jieping Ye"
    ],
    "abstract": "Video generation models have advanced significantly, yet they still struggle to synthesize complex human movements due to the high degrees of freedom in human articulation. This limitation stems from the intrinsic constraints of pixel-only training objectives, which inherently bias models toward appearance fidelity at the expense of learning underlying kinematic principles. To address this, we introduce EchoMotion, a framework designed to model the joint distribution of appearance and human motion, thereby improving the quality of complex human action video generation. EchoMotion extends the DiT (Diffusion Transformer) framework with a dual-branch architecture that jointly processes tokens concatenated from different modalities. Furthermore, we propose MVS-RoPE (Motion-Video Syncronized RoPE), which offers unified 3D positional encoding for both video and motion tokens. By providing a synchronized coordinate system for the dual-modal latent sequence, MVS-RoPE establishes an inductive bias that fosters temporal alignment between the two modalities. We also propose a Motion-Video Two-Stage Training Strategy. This strategy enables the model to perform both the joint generation of complex human action videos and their corresponding motion sequences, as well as versatile cross-modal conditional generation tasks. To facilitate the training of a model with these capabilities, we construct HuMoVe, a large-scale dataset of approximately 80,000 high-quality, human-centric video-motion pairs. Our findings reveal that explicitly representing human motion is complementary to appearance, significantly boosting the coherence and plausibility of human-centric video generation.",
    "arxiv_url": "https://arxiv.org/abs/2512.18814v1",
    "pdf_url": "https://arxiv.org/pdf/2512.18814v1",
    "published_date": "2025-12-21",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "dit",
      "diffusion transformer",
      "video generation",
      "architecture",
      "human motion"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.18814v1",
      "pdf": "https://arxiv.org/pdf/2512.18814v1"
    },
    "bibtex": ""
  },
  {
    "title": "In-Context Audio Control of Video Diffusion Transformers",
    "authors": [
      "Wenze Liu",
      "Weicai Ye",
      "Minghong Cai",
      "Quande Liu",
      "Xintao Wang",
      "Xiangyu Yue"
    ],
    "abstract": "Recent advancements in video generation have seen a shift towards unified, transformer-based foundation models that can handle multiple conditional inputs in-context. However, these models have primarily focused on modalities like text, images, and depth maps, while strictly time-synchronous signals like audio have been underexplored. This paper introduces In-Context Audio Control of video diffusion transformers (ICAC), a framework that investigates the integration of audio signals for speech-driven video generation within a unified full-attention architecture, akin to FullDiT. We systematically explore three distinct mechanisms for injecting audio conditions: standard cross-attention, 2D self-attention, and unified 3D self-attention. Our findings reveal that while 3D attention offers the highest potential for capturing spatio-temporal audio-visual correlations, it presents significant training challenges. To overcome this, we propose a Masked 3D Attention mechanism that constrains the attention pattern to enforce temporal alignment, enabling stable training and superior performance. Our experiments demonstrate that this approach achieves strong lip synchronization and video quality, conditioned on an audio stream and reference images.",
    "arxiv_url": "https://arxiv.org/abs/2512.18772v1",
    "pdf_url": "https://arxiv.org/pdf/2512.18772v1",
    "published_date": "2025-12-21",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "video diffusion",
      "speech-driven",
      "dit",
      "diffusion transformer",
      "video generation",
      "architecture"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.18772v1",
      "pdf": "https://arxiv.org/pdf/2512.18772v1"
    },
    "bibtex": ""
  },
  {
    "title": "IPCV: Information-Preserving Compression for MLLM Visual Encoders",
    "authors": [
      "Yuan Chen",
      "Zichen Wen",
      "Yuzhou Wu",
      "Xuyang Liu",
      "Shuang Chen",
      "Junpeng Ma",
      "Weijia Li",
      "Conghui He",
      "Linfeng Zhang"
    ],
    "abstract": "Multimodal Large Language Models (MLLMs) deliver strong vision-language performance but at high computational cost, driven by numerous visual tokens processed by the Vision Transformer (ViT) encoder. Existing token pruning strategies are inadequate: LLM-stage token pruning overlooks the ViT's overhead, while conventional ViT token pruning, without language guidance, risks discarding textually critical visual cues and introduces feature distortions amplified by the ViT's bidirectional attention. To meet these challenges, we propose IPCV, a training-free, information-preserving compression framework for MLLM visual encoders. IPCV enables aggressive token pruning inside the ViT via Neighbor-Guided Reconstruction (NGR) that temporarily reconstructs pruned tokens to participate in attention with minimal overhead, then fully restores them before passing to the LLM. Besides, we introduce Attention Stabilization (AS) to further alleviate the negative influence from token pruning by approximating the K/V of pruned tokens. It can be directly applied to previous LLM-side token pruning methods to enhance their performance. Extensive experiments show that IPCV substantially reduces end-to-end computation and outperforms state-of-the-art training-free token compression methods across diverse image and video benchmarks. Our code is available at https://github.com/Perkzi/IPCV.",
    "arxiv_url": "https://arxiv.org/abs/2512.18747v1",
    "pdf_url": "https://arxiv.org/pdf/2512.18747v1",
    "published_date": "2025-12-21",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "https://github.com/Perkzi/IPCV",
    "keywords": [
      "benchmark"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.18747v1",
      "pdf": "https://arxiv.org/pdf/2512.18747v1",
      "github": "https://github.com/Perkzi/IPCV"
    },
    "bibtex": ""
  },
  {
    "title": "Memorize-and-Generate: Towards Long-Term Consistency in Real-Time Video Generation",
    "authors": [
      "Tianrui Zhu",
      "Shiyi Zhang",
      "Zhirui Sun",
      "Jingqi Tian",
      "Yansong Tang"
    ],
    "abstract": "Frame-level autoregressive (frame-AR) models have achieved significant progress, enabling real-time video generation comparable to bidirectional diffusion models and serving as a foundation for interactive world models and game engines. However, current approaches in long video generation typically rely on window attention, which naively discards historical context outside the window, leading to catastrophic forgetting and scene inconsistency; conversely, retaining full history incurs prohibitive memory costs. To address this trade-off, we propose Memorize-and-Generate (MAG), a framework that decouples memory compression and frame generation into distinct tasks. Specifically, we train a memory model to compress historical information into a compact KV cache, and a separate generator model to synthesize subsequent frames utilizing this compressed representation. Furthermore, we introduce MAG-Bench to strictly evaluate historical memory retention. Extensive experiments demonstrate that MAG achieves superior historical scene consistency while maintaining competitive performance on standard video generation benchmarks.",
    "arxiv_url": "https://arxiv.org/abs/2512.18741v2",
    "pdf_url": "https://arxiv.org/pdf/2512.18741v2",
    "published_date": "2025-12-21",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "interactive",
      "world model",
      "video generation",
      "diffusion model",
      "long video",
      "autoregressive",
      "benchmark"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.18741v2",
      "pdf": "https://arxiv.org/pdf/2512.18741v2"
    },
    "bibtex": ""
  },
  {
    "title": "PTTA: A Pure Text-to-Animation Framework for High-Quality Creation",
    "authors": [
      "Ruiqi Chen",
      "Kaitong Cai",
      "Yijia Fan",
      "Keze Wang"
    ],
    "abstract": "Traditional animation production involves complex pipelines and significant manual labor cost. While recent video generation models such as Sora, Kling, and CogVideoX achieve impressive results on natural video synthesis, they exhibit notable limitations when applied to animation generation. Recent efforts, such as AniSora, demonstrate promising performance by fine-tuning image-to-video models for animation styles, yet analogous exploration in the text-to-video setting remains limited.   In this work, we present PTTA, a pure text-to-animation framework for high-quality animation creation. We first construct a small-scale but high-quality paired dataset of animation videos and textual descriptions. Building upon the pretrained text-to-video model HunyuanVideo, we perform fine-tuning to adapt it to animation-style generation. Extensive visual evaluations across multiple dimensions show that the proposed approach consistently outperforms comparable baselines in animation video synthesis.",
    "arxiv_url": "https://arxiv.org/abs/2512.18614v1",
    "pdf_url": "https://arxiv.org/pdf/2512.18614v1",
    "published_date": "2025-12-21",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "dit",
      "text-to-video",
      "video generation",
      "image-to-video",
      "evaluation",
      "video synthesis",
      "style"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.18614v1",
      "pdf": "https://arxiv.org/pdf/2512.18614v1"
    },
    "bibtex": ""
  },
  {
    "title": "Exploration vs. Fixation: Scaffolding Divergent and Convergent Thinking for Human-AI Co-Creation with Generative Models",
    "authors": [
      "Chao Wen",
      "Tung Phung",
      "Pronita Mehrotra",
      "Sumit Gulwani",
      "Tomohiro Nagashima",
      "Adish Singla"
    ],
    "abstract": "Generative AI has begun to democratize creative work, enabling novices to produce complex artifacts such as code, images, and videos. However, in practice, existing interaction paradigms often fail to support divergent exploration: users tend to converge too quickly on early ``good enough'' results and struggle to move beyond them, leading to premature convergence and design fixation that constrains their creative potential. To address this, we propose a structured, process-oriented human-AI co-creation paradigm including divergent and convergent thinking stages, grounded in Wallas's model of creativity. To avoid design fixation, our paradigm scaffolds both high-level exploration of conceptual ideas in the early divergent thinking phase and low-level exploration of variations in the later convergent thinking phrase. We instantiate this paradigm in HAIExplore, an image co-creation system that (i) scaffolds divergent thinking through a dedicated brainstorming stage for exploring high-level ideas in a conceptual space, and (ii) scaffolds convergent refinement through an interface that externalizes users' refinement intentions as interpretable parameters and options, making the refinement process more controllable and easier to explore. We report on a within-subjects study comparing HAIExplore with a widely used linear chat interface (ChatGPT) for creative image generation. Our findings show that explicitly scaffolding the creative process into brainstorming and refinement stages can mitigate design fixation, improve perceived controllability and alignment with users' intentions, and better support the non-linear nature of creative work. We conclude with design implications for future creativity support tools and human-AI co-creation workflows.",
    "arxiv_url": "https://arxiv.org/abs/2512.18388v1",
    "pdf_url": "https://arxiv.org/pdf/2512.18388v1",
    "published_date": "2025-12-20",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "concept",
      "creative",
      "controllable"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.18388v1",
      "pdf": "https://arxiv.org/pdf/2512.18388v1"
    },
    "bibtex": ""
  },
  {
    "title": "MatSpray: Fusing 2D Material World Knowledge on 3D Geometry",
    "authors": [
      "Philipp Langsteiner",
      "Jan-Niklas Dihlmann",
      "Hendrik P. A. Lensch"
    ],
    "abstract": "Manual modeling of material parameters and 3D geometry is a time consuming yet essential task in the gaming and film industries. While recent advances in 3D reconstruction have enabled accurate approximations of scene geometry and appearance, these methods often fall short in relighting scenarios due to the lack of precise, spatially varying material parameters. At the same time, diffusion models operating on 2D images have shown strong performance in predicting physically based rendering (PBR) properties such as albedo, roughness, and metallicity. However, transferring these 2D material maps onto reconstructed 3D geometry remains a significant challenge. We propose a framework for fusing 2D material data into 3D geometry using a combination of novel learning-based and projection-based approaches. We begin by reconstructing scene geometry via Gaussian Splatting. From the input images, a diffusion model generates 2D maps for albedo, roughness, and metallic parameters. Any existing diffusion model that can convert images or videos to PBR materials can be applied. The predictions are further integrated into the 3D representation either by optimizing an image-based loss or by directly projecting the material parameters onto the Gaussians using Gaussian ray tracing. To enhance fine-scale accuracy and multi-view consistency, we further introduce a light-weight neural refinement step (Neural Merger), which takes ray-traced material features as input and produces detailed adjustments. Our results demonstrate that the proposed methods outperform existing techniques in both quantitative metrics and perceived visual realism. This enables more accurate, relightable, and photorealistic renderings from reconstructed scenes, significantly improving the realism and efficiency of asset creation workflows in content production pipelines.",
    "arxiv_url": "https://arxiv.org/abs/2512.18314v1",
    "pdf_url": "https://arxiv.org/pdf/2512.18314v1",
    "published_date": "2025-12-20",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "diffusion model",
      "physical",
      "film"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.18314v1",
      "pdf": "https://arxiv.org/pdf/2512.18314v1"
    },
    "bibtex": ""
  },
  {
    "title": "MACE-Dance: Motion-Appearance Cascaded Experts for Music-Driven Dance Video Generation",
    "authors": [
      "Kaixing Yang",
      "Jiashu Zhu",
      "Xulong Tang",
      "Ziqiao Peng",
      "Xiangyue Zhang",
      "Puwei Wang",
      "Jiahong Wu",
      "Xiangxiang Chu",
      "Hongyan Liu",
      "Jun He"
    ],
    "abstract": "With the rise of online dance-video platforms and rapid advances in AI-generated content (AIGC), music-driven dance generation has emerged as a compelling research direction. Despite substantial progress in related domains such as music-driven 3D dance generation, pose-driven image animation, and audio-driven talking-head synthesis, existing methods cannot be directly adapted to this task. Moreover, the limited studies in this area still struggle to jointly achieve high-quality visual appearance and realistic human motion. Accordingly, we present MACE-Dance, a music-driven dance video generation framework with cascaded Mixture-of-Experts (MoE). The Motion Expert performs music-to-3D motion generation while enforcing kinematic plausibility and artistic expressiveness, whereas the Appearance Expert carries out motion- and reference-conditioned video synthesis, preserving visual identity with spatiotemporal coherence. Specifically, the Motion Expert adopts a diffusion model with a BiMamba-Transformer hybrid architecture and a Guidance-Free Training (GFT) strategy, achieving state-of-the-art (SOTA) performance in 3D dance generation. The Appearance Expert employs a decoupled kinematic-aesthetic fine-tuning strategy, achieving state-of-the-art (SOTA) performance in pose-driven image animation. To better benchmark this task, we curate a large-scale and diverse dataset and design a motion-appearance evaluation protocol. Based on this protocol, MACE-Dance also achieves state-of-the-art performance. Project page: https://macedance.github.io/",
    "arxiv_url": "https://arxiv.org/abs/2512.18181v2",
    "pdf_url": "https://arxiv.org/pdf/2512.18181v2",
    "published_date": "2025-12-20",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "dit",
      "identity",
      "dance generation",
      "video generation",
      "evaluation",
      "video synthesis",
      "diffusion model",
      "human motion",
      "architecture",
      "benchmark",
      "audio-driven",
      "image animation"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.18181v2",
      "pdf": "https://arxiv.org/pdf/2512.18181v2",
      "project": "https://macedance.github.io"
    },
    "bibtex": ""
  },
  {
    "title": "Dexterous World Models",
    "authors": [
      "Byungjun Kim",
      "Taeksoo Kim",
      "Junyoung Lee",
      "Hanbyul Joo"
    ],
    "abstract": "Recent progress in 3D reconstruction has made it easy to create realistic digital twins from everyday environments. However, current digital twins remain largely static and are limited to navigation and view synthesis without embodied interactivity. To bridge this gap, we introduce Dexterous World Model (DWM), a scene-action-conditioned video diffusion framework that models how dexterous human actions induce dynamic changes in static 3D scenes.   Given a static 3D scene rendering and an egocentric hand motion sequence, DWM generates temporally coherent videos depicting plausible human-scene interactions. Our approach conditions video generation on (1) static scene renderings following a specified camera trajectory to ensure spatial consistency, and (2) egocentric hand mesh renderings that encode both geometry and motion cues to model action-conditioned dynamics directly. To train DWM, we construct a hybrid interaction video dataset. Synthetic egocentric interactions provide fully aligned supervision for joint locomotion and manipulation learning, while fixed-camera real-world videos contribute diverse and realistic object dynamics.   Experiments demonstrate that DWM enables realistic and physically plausible interactions, such as grasping, opening, and moving objects, while maintaining camera and scene consistency. This framework represents a first step toward video diffusion-based interactive digital twins and enables embodied simulation from egocentric actions.",
    "arxiv_url": "https://arxiv.org/abs/2512.17907v1",
    "pdf_url": "https://arxiv.org/pdf/2512.17907v1",
    "published_date": "2025-12-19",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "interactive",
      "video diffusion",
      "dit",
      "world model",
      "physical",
      "trajectory",
      "video generation",
      "dynamics",
      "simulation",
      "action-conditioned"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.17907v1",
      "pdf": "https://arxiv.org/pdf/2512.17907v1"
    },
    "bibtex": ""
  },
  {
    "title": "Vidarc: Embodied Video Diffusion Model for Closed-loop Control",
    "authors": [
      "Yao Feng",
      "Chendong Xiang",
      "Xinyi Mao",
      "Hengkai Tan",
      "Zuyue Zhang",
      "Shuhe Huang",
      "Kaiwen Zheng",
      "Haitian Liu",
      "Hang Su",
      "Jun Zhu"
    ],
    "abstract": "Robotic arm manipulation in data-scarce settings is a highly challenging task due to the complex embodiment dynamics and diverse contexts. Recent video-based approaches have shown great promise in capturing and transferring the temporal and physical interactions by pre-training on Internet-scale video data. However, such methods are often not optimized for the embodiment-specific closed-loop control, typically suffering from high latency and insufficient grounding. In this paper, we present Vidarc (Video Diffusion for Action Reasoning and Closed-loop Control), a novel autoregressive embodied video diffusion approach augmented by a masked inverse dynamics model. By grounding video predictions with action-relevant masks and incorporating real-time feedback through cached autoregressive generation, Vidarc achieves fast, accurate closed-loop control. Pre-trained on one million cross-embodiment episodes, Vidarc surpasses state-of-the-art baselines, achieving at least a 15% higher success rate in real-world deployment and a 91% reduction in latency. We also highlight its robust generalization and error correction capabilities across previously unseen robotic platforms.",
    "arxiv_url": "https://arxiv.org/abs/2512.17661v1",
    "pdf_url": "https://arxiv.org/pdf/2512.17661v1",
    "published_date": "2025-12-19",
    "categories": [
      "cs.RO",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "video diffusion",
      "video prediction",
      "physical",
      "diffusion model",
      "dynamics",
      "autoregressive"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.17661v1",
      "pdf": "https://arxiv.org/pdf/2512.17661v1"
    },
    "bibtex": ""
  },
  {
    "title": "Region-Constraint In-Context Generation for Instructional Video Editing",
    "authors": [
      "Zhongwei Zhang",
      "Fuchen Long",
      "Wei Li",
      "Zhaofan Qiu",
      "Wu Liu",
      "Ting Yao",
      "Tao Mei"
    ],
    "abstract": "The In-context generation paradigm recently has demonstrated strong power in instructional image editing with both data efficiency and synthesis quality. Nevertheless, shaping such in-context learning for instruction-based video editing is not trivial. Without specifying editing regions, the results can suffer from the problem of inaccurate editing regions and the token interference between editing and non-editing areas during denoising. To address these, we present ReCo, a new instructional video editing paradigm that novelly delves into constraint modeling between editing and non-editing regions during in-context generation. Technically, ReCo width-wise concatenates source and target video for joint denoising. To calibrate video diffusion learning, ReCo capitalizes on two regularization terms, i.e., latent and attention regularization, conducting on one-step backward denoised latents and attention maps, respectively. The former increases the latent discrepancy of the editing region between source and target videos while reducing that of non-editing areas, emphasizing the modification on editing area and alleviating outside unexpected content generation. The latter suppresses the attention of tokens in the editing region to the tokens in counterpart of the source video, thereby mitigating their interference during novel object generation in target video. Furthermore, we propose a large-scale, high-quality video editing dataset, i.e., ReCo-Data, comprising 500K instruction-video pairs to benefit model training. Extensive experiments conducted on four major instruction-based video editing tasks demonstrate the superiority of our proposal.",
    "arxiv_url": "https://arxiv.org/abs/2512.17650v1",
    "pdf_url": "https://arxiv.org/pdf/2512.17650v1",
    "published_date": "2025-12-19",
    "categories": [
      "cs.CV",
      "cs.MM"
    ],
    "github_url": "",
    "keywords": [
      "video editing",
      "denoising",
      "video diffusion",
      "dit"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.17650v1",
      "pdf": "https://arxiv.org/pdf/2512.17650v1"
    },
    "bibtex": ""
  },
  {
    "title": "InsertAnywhere: Bridging 4D Scene Geometry and Diffusion Models for Realistic Video Object Insertion",
    "authors": [
      "Hoiyeong Jin",
      "Hyojin Jang",
      "Jeongho Kim",
      "Junha Hyung",
      "Kinam Kim",
      "Dongjin Kim",
      "Huijin Choi",
      "Hyeonji Kim",
      "Jaegul Choo"
    ],
    "abstract": "Recent advances in diffusion-based video generation have opened new possibilities for controllable video editing, yet realistic video object insertion (VOI) remains challenging due to limited 4D scene understanding and inadequate handling of occlusion and lighting effects. We present InsertAnywhere, a new VOI framework that achieves geometrically consistent object placement and appearance-faithful video synthesis. Our method begins with a 4D aware mask generation module that reconstructs the scene geometry and propagates user specified object placement across frames while maintaining temporal coherence and occlusion consistency. Building upon this spatial foundation, we extend a diffusion based video generation model to jointly synthesize the inserted object and its surrounding local variations such as illumination and shading. To enable supervised training, we introduce ROSE++, an illumination aware synthetic dataset constructed by transforming the ROSE object removal dataset into triplets of object removed video, object present video, and a VLM generated reference image. Through extensive experiments, we demonstrate that our framework produces geometrically plausible and visually coherent object insertions across diverse real world scenarios, significantly outperforming existing research and commercial models.",
    "arxiv_url": "https://arxiv.org/abs/2512.17504v1",
    "pdf_url": "https://arxiv.org/pdf/2512.17504v1",
    "published_date": "2025-12-19",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "controllable",
      "dit",
      "video editing",
      "video generation",
      "video synthesis",
      "diffusion model"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.17504v1",
      "pdf": "https://arxiv.org/pdf/2512.17504v1"
    },
    "bibtex": ""
  },
  {
    "title": "LangDriveCTRL: Natural Language Controllable Driving Scene Editing with Multi-modal Agents",
    "authors": [
      "Yun He",
      "Francesco Pittaluga",
      "Ziyu Jiang",
      "Matthias Zwicker",
      "Manmohan Chandraker",
      "Zaid Tasneem"
    ],
    "abstract": "LangDriveCTRL is a natural-language-controllable framework for editing real-world driving videos to synthesize diverse traffic scenarios. It leverages explicit 3D scene decomposition to represent driving videos as a scene graph, containing static background and dynamic objects. To enable fine-grained editing and realism, it incorporates an agentic pipeline in which an Orchestrator transforms user instructions into execution graphs that coordinate specialized agents and tools. Specifically, an Object Grounding Agent establishes correspondence between free-form text descriptions and target object nodes in the scene graph; a Behavior Editing Agent generates multi-object trajectories from language instructions; and a Behavior Reviewer Agent iteratively reviews and refines the generated trajectories. The edited scene graph is rendered and then refined using a video diffusion tool to address artifacts introduced by object insertion and significant view changes. LangDriveCTRL supports both object node editing (removal, insertion and replacement) and multi-object behavior editing from a single natural-language instruction. Quantitatively, it achieves nearly $2\\times$ higher instruction alignment than the previous SoTA, with superior structural preservation, photorealism, and traffic realism. Project page is available at: https://yunhe24.github.io/langdrivectrl/.",
    "arxiv_url": "https://arxiv.org/abs/2512.17445v1",
    "pdf_url": "https://arxiv.org/pdf/2512.17445v1",
    "published_date": "2025-12-19",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "multi-modal",
      "controllable",
      "video diffusion",
      "dit"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.17445v1",
      "pdf": "https://arxiv.org/pdf/2512.17445v1",
      "project": "https://yunhe24.github.io/langdrivectrl"
    },
    "bibtex": ""
  },
  {
    "title": "DESSERT: Diffusion-based Event-driven Single-frame Synthesis via Residual Training",
    "authors": [
      "Jiyun Kong",
      "Jun-Hyuk Kim",
      "Jong-Seok Lee"
    ],
    "abstract": "Video frame prediction extrapolates future frames from previous frames, but suffers from prediction errors in dynamic scenes due to the lack of information about the next frame. Event cameras address this limitation by capturing per-pixel brightness changes asynchronously with high temporal resolution. Prior research on event-based video frame prediction has leveraged motion information from event data, often by predicting event-based optical flow and reconstructing frames via pixel warping. However, such approaches introduce holes and blurring when pixel displacement is inaccurate. To overcome this limitation, we propose DESSERT, a diffusion-based event-driven single-frame synthesis framework via residual training. Leveraging a pre-trained Stable Diffusion model, our method is trained on inter-frame residuals to ensure temporal consistency. The training pipeline consists of two stages: (1) an Event-to-Residual Alignment Variational Autoencoder (ER-VAE) that aligns the event frame between anchor and target frames with the corresponding residual, and (2) a diffusion model that denoises the residual latent conditioned on event data. Furthermore, we introduce Diverse-Length Temporal (DLT) augmentation, which improves robustness by training on frame segments of varying temporal lengths. Experimental results demonstrate that our method outperforms existing event-based reconstruction, image-based video frame prediction, event-based video frame prediction, and one-sided event-based video frame interpolation methods, producing sharper and more temporally consistent frame synthesis.",
    "arxiv_url": "https://arxiv.org/abs/2512.17323v1",
    "pdf_url": "https://arxiv.org/pdf/2512.17323v1",
    "published_date": "2025-12-19",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "frame interpolation",
      "diffusion model",
      "temporal consistency",
      "dit"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.17323v1",
      "pdf": "https://arxiv.org/pdf/2512.17323v1"
    },
    "bibtex": ""
  },
  {
    "title": "Mitty: Diffusion-based Human-to-Robot Video Generation",
    "authors": [
      "Yiren Song",
      "Cheng Liu",
      "Weijia Mao",
      "Mike Zheng Shou"
    ],
    "abstract": "Learning directly from human demonstration videos is a key milestone toward scalable and generalizable robot learning. Yet existing methods rely on intermediate representations such as keypoints or trajectories, introducing information loss and cumulative errors that harm temporal and visual consistency. We present Mitty, a Diffusion Transformer that enables video In-Context Learning for end-to-end Human2Robot video generation. Built on a pretrained video diffusion model, Mitty leverages strong visual-temporal priors to translate human demonstrations into robot-execution videos without action labels or intermediate abstractions. Demonstration videos are compressed into condition tokens and fused with robot denoising tokens through bidirectional attention during diffusion. To mitigate paired-data scarcity, we also develop an automatic synthesis pipeline that produces high-quality human-robot pairs from large egocentric datasets. Experiments on Human2Robot and EPIC-Kitchens show that Mitty delivers state-of-the-art results, strong generalization to unseen environments, and new insights for scalable robot learning from human observations.",
    "arxiv_url": "https://arxiv.org/abs/2512.17253v1",
    "pdf_url": "https://arxiv.org/pdf/2512.17253v1",
    "published_date": "2025-12-19",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "video diffusion",
      "dit",
      "diffusion transformer",
      "video generation",
      "diffusion model",
      "denoising"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.17253v1",
      "pdf": "https://arxiv.org/pdf/2512.17253v1"
    },
    "bibtex": ""
  },
  {
    "title": "PhysFire-WM: A Physics-Informed World Model for Emulating Fire Spread Dynamics",
    "authors": [
      "Nan Zhou",
      "Huandong Wang",
      "Jiahao Li",
      "Yang Li",
      "Xiao-Ping Zhang",
      "Yong Li",
      "Xinlei Chen"
    ],
    "abstract": "Fine-grained fire prediction plays a crucial role in emergency response. Infrared images and fire masks provide complementary thermal and boundary information, yet current methods are predominantly limited to binary mask modeling with inherent signal sparsity, failing to capture the complex dynamics of fire. While world models show promise in video generation, their physical inconsistencies pose significant challenges for fire forecasting. This paper introduces PhysFire-WM, a Physics-informed World Model for emulating Fire spread dynamics. Our approach internalizes combustion dynamics by encoding structured priors from a Physical Simulator to rectify physical discrepancies, coupled with a Cross-task Collaborative Training strategy (CC-Train) that alleviates the issue of limited information in mask-based modeling. Through parameter sharing and gradient coordination, CC-Train effectively integrates thermal radiation dynamics and spatial boundary delineation, enhancing both physical realism and geometric accuracy. Extensive experiments on a fine-grained multimodal fire dataset demonstrate the superior accuracy of PhysFire-WM in fire spread prediction. Validation underscores the importance of physical priors and cross-task collaboration, providing new insights for applying physics-informed world models to disaster prediction.",
    "arxiv_url": "https://arxiv.org/abs/2512.17152v1",
    "pdf_url": "https://arxiv.org/pdf/2512.17152v1",
    "published_date": "2025-12-19",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "world model",
      "physical",
      "video generation",
      "dynamics",
      "physics"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.17152v1",
      "pdf": "https://arxiv.org/pdf/2512.17152v1"
    },
    "bibtex": ""
  },
  {
    "title": "Characterizing Motion Encoding in Video Diffusion Timesteps",
    "authors": [
      "Vatsal Baherwani",
      "Yixuan Ren",
      "Abhinav Shrivastava"
    ],
    "abstract": "Text-to-video diffusion models synthesize temporal motion and spatial appearance through iterative denoising, yet how motion is encoded across timesteps remains poorly understood. Practitioners often exploit the empirical heuristic that early timesteps mainly shape motion and layout while later ones refine appearance, but this behavior has not been systematically characterized. In this work, we proxy motion encoding in video diffusion timesteps by the trade-off between appearance editing and motion preservation induced when injecting new conditions over specified timestep ranges, and characterize this proxy through a large-scale quantitative study. This protocol allows us to factor motion from appearance by quantitatively mapping how they compete along the denoising trajectory. Across diverse architectures, we consistently identify an early, motion-dominant regime and a later, appearance-dominant regime, yielding an operational motion-appearance boundary in timestep space. Building on this characterization, we simplify current one-shot motion customization paradigm by restricting training and inference to the motion-dominant regime, achieving strong motion transfer without auxiliary debiasing modules or specialized objectives. Our analysis turns a widely used heuristic into a spatiotemporal disentanglement principle, and our timestep-constrained recipe can serve as ready integration into existing motion transfer and editing methods.",
    "arxiv_url": "https://arxiv.org/abs/2512.22175v1",
    "pdf_url": "https://arxiv.org/pdf/2512.22175v1",
    "published_date": "2025-12-18",
    "categories": [
      "cs.CV",
      "cs.AI",
      "eess.IV"
    ],
    "github_url": "",
    "keywords": [
      "layout",
      "video diffusion",
      "dit",
      "trajectory",
      "text-to-video",
      "diffusion model",
      "denoising",
      "architecture",
      "customization"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.22175v1",
      "pdf": "https://arxiv.org/pdf/2512.22175v1"
    },
    "bibtex": ""
  },
  {
    "title": "Infinite-Homography as Robust Conditioning for Camera-Controlled Video Generation",
    "authors": [
      "Min-Jung Kim",
      "Jeongho Kim",
      "Hoiyeong Jin",
      "Junha Hyung",
      "Jaegul Choo"
    ],
    "abstract": "Recent progress in video diffusion models has spurred growing interest in camera-controlled novel-view video generation for dynamic scenes, aiming to provide creators with cinematic camera control capabilities in post-production. A key challenge in camera-controlled video generation is ensuring fidelity to the specified camera pose, while maintaining view consistency and reasoning about occluded geometry from limited observations. To address this, existing methods either train trajectory-conditioned video generation model on trajectory-video pair dataset, or estimate depth from the input video to reproject it along a target trajectory and generate the unprojected regions. Nevertheless, existing methods struggle to generate camera-pose-faithful, high-quality videos for two main reasons: (1) reprojection-based approaches are highly susceptible to errors caused by inaccurate depth estimation; and (2) the limited diversity of camera trajectories in existing datasets restricts learned models. To address these limitations, we present InfCam, a depth-free, camera-controlled video-to-video generation framework with high pose fidelity. The framework integrates two key components: (1) infinite homography warping, which encodes 3D camera rotations directly within the 2D latent space of a video diffusion model. Conditioning on this noise-free rotational information, the residual parallax term is predicted through end-to-end training to achieve high camera-pose fidelity; and (2) a data augmentation pipeline that transforms existing synthetic multiview datasets into sequences with diverse trajectories and focal lengths. Experimental results demonstrate that InfCam outperforms baseline methods in camera-pose accuracy and visual fidelity, generalizing well from synthetic to real-world data. Link to our project page:https://emjay73.github.io/InfCam/",
    "arxiv_url": "https://arxiv.org/abs/2512.17040v1",
    "pdf_url": "https://arxiv.org/pdf/2512.17040v1",
    "published_date": "2025-12-18",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "video diffusion",
      "dit",
      "trajectory",
      "video generation",
      "video-to-video",
      "diffusion model",
      "camera control"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.17040v1",
      "pdf": "https://arxiv.org/pdf/2512.17040v1",
      "project": "https://emjay73.github.io/InfCam"
    },
    "bibtex": ""
  },
  {
    "title": "The World is Your Canvas: Painting Promptable Events with Reference Images, Trajectories, and Text",
    "authors": [
      "Hanlin Wang",
      "Hao Ouyang",
      "Qiuyu Wang",
      "Yue Yu",
      "Yihao Meng",
      "Wen Wang",
      "Ka Leong Cheng",
      "Shuailei Ma",
      "Qingyan Bai",
      "Yixuan Li",
      "Cheng Chen",
      "Yanhong Zeng",
      "Xing Zhu",
      "Yujun Shen",
      "Qifeng Chen"
    ],
    "abstract": "We present WorldCanvas, a framework for promptable world events that enables rich, user-directed simulation by combining text, trajectories, and reference images. Unlike text-only approaches and existing trajectory-controlled image-to-video methods, our multimodal approach combines trajectories -- encoding motion, timing, and visibility -- with natural language for semantic intent and reference images for visual grounding of object identity, enabling the generation of coherent, controllable events that include multi-agent interactions, object entry/exit, reference-guided appearance and counterintuitive events. The resulting videos demonstrate not only temporal coherence but also emergent consistency, preserving object identity and scene despite temporary disappearance. By supporting expressive world events generation, WorldCanvas advances world models from passive predictors to interactive, user-shaped simulators. Our project page is available at: https://worldcanvas.github.io/.",
    "arxiv_url": "https://arxiv.org/abs/2512.16924v1",
    "pdf_url": "https://arxiv.org/pdf/2512.16924v1",
    "published_date": "2025-12-18",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "interactive",
      "controllable",
      "world model",
      "identity",
      "trajectory",
      "image-to-video",
      "simulation"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.16924v1",
      "pdf": "https://arxiv.org/pdf/2512.16924v1",
      "project": "https://worldcanvas.github.io"
    },
    "bibtex": ""
  },
  {
    "title": "EasyV2V: A High-quality Instruction-based Video Editing Framework",
    "authors": [
      "Jinjie Mai",
      "Chaoyang Wang",
      "Guocheng Gordon Qian",
      "Willi Menapace",
      "Sergey Tulyakov",
      "Bernard Ghanem",
      "Peter Wonka",
      "Ashkan Mirzaei"
    ],
    "abstract": "While image editing has advanced rapidly, video editing remains less explored, facing challenges in consistency, control, and generalization. We study the design space of data, architecture, and control, and introduce \\emph{EasyV2V}, a simple and effective framework for instruction-based video editing. On the data side, we compose existing experts with fast inverses to build diverse video pairs, lift image edit pairs into videos via single-frame supervision and pseudo pairs with shared affine motion, mine dense-captioned clips for video pairs, and add transition supervision to teach how edits unfold. On the model side, we observe that pretrained text-to-video models possess editing capability, motivating a simplified design. Simple sequence concatenation for conditioning with light LoRA fine-tuning suffices to train a strong model. For control, we unify spatiotemporal control via a single mask mechanism and support optional reference images. Overall, EasyV2V works with flexible inputs, e.g., video+text, video+mask+text, video+mask+reference+text, and achieves state-of-the-art video editing results, surpassing concurrent and commercial systems. Project page: https://snap-research.github.io/easyv2v/",
    "arxiv_url": "https://arxiv.org/abs/2512.16920v1",
    "pdf_url": "https://arxiv.org/pdf/2512.16920v1",
    "published_date": "2025-12-18",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "video editing",
      "text-to-video",
      "architecture",
      "dit"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.16920v1",
      "pdf": "https://arxiv.org/pdf/2512.16920v1",
      "project": "https://snap-research.github.io/easyv2v"
    },
    "bibtex": ""
  },
  {
    "title": "AdaTooler-V: Adaptive Tool-Use for Images and Videos",
    "authors": [
      "Chaoyang Wang",
      "Kaituo Feng",
      "Dongyang Chen",
      "Zhongyu Wang",
      "Zhixun Li",
      "Sicheng Gao",
      "Meng Meng",
      "Xu Zhou",
      "Manyuan Zhang",
      "Yuzhang Shang",
      "Xiangyu Yue"
    ],
    "abstract": "Recent advances have shown that multimodal large language models (MLLMs) benefit from multimodal interleaved chain-of-thought (CoT) with vision tool interactions. However, existing open-source models often exhibit blind tool-use reasoning patterns, invoking vision tools even when they are unnecessary, which significantly increases inference overhead and degrades model performance. To this end, we propose AdaTooler-V, an MLLM that performs adaptive tool-use by determining whether a visual problem truly requires tools. First, we introduce AT-GRPO, a reinforcement learning algorithm that adaptively adjusts reward scales based on the Tool Benefit Score of each sample, encouraging the model to invoke tools only when they provide genuine improvements. Moreover, we construct two datasets to support training: AdaTooler-V-CoT-100k for SFT cold start and AdaTooler-V-300k for RL with verifiable rewards across single-image, multi-image, and video data. Experiments across twelve benchmarks demonstrate the strong reasoning capability of AdaTooler-V, outperforming existing methods in diverse visual reasoning tasks. Notably, AdaTooler-V-7B achieves an accuracy of 89.8\\% on the high-resolution benchmark V*, surpassing the commercial proprietary model GPT-4o and Gemini 1.5 Pro. All code, models, and data are released.",
    "arxiv_url": "https://arxiv.org/abs/2512.16918v2",
    "pdf_url": "https://arxiv.org/pdf/2512.16918v2",
    "published_date": "2025-12-18",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "benchmark"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.16918v2",
      "pdf": "https://arxiv.org/pdf/2512.16918v2"
    },
    "bibtex": ""
  },
  {
    "title": "Animate Any Character in Any World",
    "authors": [
      "Yitong Wang",
      "Fangyun Wei",
      "Hongyang Zhang",
      "Bo Dai",
      "Yan Lu"
    ],
    "abstract": "Recent advances in world models have greatly enhanced interactive environment simulation. Existing methods mainly fall into two categories: (1) static world generation models, which construct 3D environments without active agents, and (2) controllable-entity models, which allow a single entity to perform limited actions in an otherwise uncontrollable environment. In this work, we introduce AniX, leveraging the realism and structural grounding of static world generation while extending controllable-entity models to support user-specified characters capable of performing open-ended actions. Users can provide a 3DGS scene and a character, then direct the character through natural language to perform diverse behaviors from basic locomotion to object-centric interactions while freely exploring the environment. AniX synthesizes temporally coherent video clips that preserve visual fidelity with the provided scene and character, formulated as a conditional autoregressive video generation problem. Built upon a pre-trained video generator, our training strategy significantly enhances motion dynamics while maintaining generalization across actions and characters. Our evaluation covers a broad range of aspects, including visual quality, character consistency, action controllability, and long-horizon coherence.",
    "arxiv_url": "https://arxiv.org/abs/2512.17796v1",
    "pdf_url": "https://arxiv.org/pdf/2512.17796v1",
    "published_date": "2025-12-18",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "interactive",
      "controllable",
      "dit",
      "world model",
      "video generation",
      "evaluation",
      "dynamics",
      "simulation",
      "autoregressive"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.17796v1",
      "pdf": "https://arxiv.org/pdf/2512.17796v1"
    },
    "bibtex": ""
  },
  {
    "title": "SceneDiff: A Benchmark and Method for Multiview Object Change Detection",
    "authors": [
      "Yuqun Wu",
      "Chih-hao Lin",
      "Henry Che",
      "Aditi Tiwari",
      "Chuhang Zou",
      "Shenlong Wang",
      "Derek Hoiem"
    ],
    "abstract": "We investigate the problem of identifying objects that have been added, removed, or moved between a pair of captures (images or videos) of the same scene at different times. Detecting such changes is important for many applications, such as robotic tidying or construction progress and safety monitoring. A major challenge is that varying viewpoints can cause objects to falsely appear changed. We introduce SceneDiff Benchmark, the first multiview change detection benchmark with object instance annotations, comprising 350 diverse video pairs with thousands of changed objects. We also introduce the SceneDiff method, a new training-free approach for multiview object change detection that leverages pretrained 3D, segmentation, and image encoding models to robustly predict across multiple benchmarks. Our method aligns the captures in 3D, extracts object regions, and compares spatial and semantic region features to detect changes. Experiments on multi-view and two-view benchmarks demonstrate that our method outperforms existing approaches by large margins (94% and 37.4% relative AP improvements). The benchmark and code will be publicly released.",
    "arxiv_url": "https://arxiv.org/abs/2512.16908v1",
    "pdf_url": "https://arxiv.org/pdf/2512.16908v1",
    "published_date": "2025-12-18",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "benchmark"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.16908v1",
      "pdf": "https://arxiv.org/pdf/2512.16908v1"
    },
    "bibtex": ""
  },
  {
    "title": "FlashPortrait: 6x Faster Infinite Portrait Animation with Adaptive Latent Prediction",
    "authors": [
      "Shuyuan Tu",
      "Yueming Pan",
      "Yinming Huang",
      "Xintong Han",
      "Zhen Xing",
      "Qi Dai",
      "Kai Qiu",
      "Chong Luo",
      "Zuxuan Wu"
    ],
    "abstract": "Current diffusion-based acceleration methods for long-portrait animation struggle to ensure identity (ID) consistency. This paper presents FlashPortrait, an end-to-end video diffusion transformer capable of synthesizing ID-preserving, infinite-length videos while achieving up to 6x acceleration in inference speed. In particular, FlashPortrait begins by computing the identity-agnostic facial expression features with an off-the-shelf extractor. It then introduces a Normalized Facial Expression Block to align facial features with diffusion latents by normalizing them with their respective means and variances, thereby improving identity stability in facial modeling. During inference, FlashPortrait adopts a dynamic sliding-window scheme with weighted blending in overlapping areas, ensuring smooth transitions and ID consistency in long animations. In each context window, based on the latent variation rate at particular timesteps and the derivative magnitude ratio among diffusion layers, FlashPortrait utilizes higher-order latent derivatives at the current timestep to directly predict latents at future timesteps, thereby skipping several denoising steps and achieving 6x speed acceleration. Experiments on benchmarks show the effectiveness of FlashPortrait both qualitatively and quantitatively.",
    "arxiv_url": "https://arxiv.org/abs/2512.16900v1",
    "pdf_url": "https://arxiv.org/pdf/2512.16900v1",
    "published_date": "2025-12-18",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "acceleration",
      "video diffusion",
      "identity",
      "diffusion transformer",
      "denoising",
      "benchmark"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.16900v1",
      "pdf": "https://arxiv.org/pdf/2512.16900v1"
    },
    "bibtex": ""
  },
  {
    "title": "Instant Expressive Gaussian Head Avatar via 3D-Aware Expression Distillation",
    "authors": [
      "Kaiwen Jiang",
      "Xueting Li",
      "Seonwook Park",
      "Ravi Ramamoorthi",
      "Shalini De Mello",
      "Koki Nagano"
    ],
    "abstract": "Portrait animation has witnessed tremendous quality improvements thanks to recent advances in video diffusion models. However, these 2D methods often compromise 3D consistency and speed, limiting their applicability in real-world scenarios, such as digital twins or telepresence. In contrast, 3D-aware facial animation feedforward methods -- built upon explicit 3D representations, such as neural radiance fields or Gaussian splatting -- ensure 3D consistency and achieve faster inference speed, but come with inferior expression details. In this paper, we aim to combine their strengths by distilling knowledge from a 2D diffusion-based method into a feed-forward encoder, which instantly converts an in-the-wild single image into a 3D-consistent, fast yet expressive animatable representation. Our animation representation is decoupled from the face's 3D representation and learns motion implicitly from data, eliminating the dependency on pre-defined parametric models that often constrain animation capabilities. Unlike previous computationally intensive global fusion mechanisms (e.g., multiple attention layers) for fusing 3D structural and animation information, our design employs an efficient lightweight local fusion strategy to achieve high animation expressivity. As a result, our method runs at 107.31 FPS for animation and pose control while achieving comparable animation quality to the state-of-the-art, surpassing alternative designs that trade speed for quality or vice versa. Project website is https://research.nvidia.com/labs/amri/projects/instant4d",
    "arxiv_url": "https://arxiv.org/abs/2512.16893v1",
    "pdf_url": "https://arxiv.org/pdf/2512.16893v1",
    "published_date": "2025-12-18",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "distillation",
      "video diffusion",
      "3d-aware",
      "diffusion model",
      "efficient",
      "avatar"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.16893v1",
      "pdf": "https://arxiv.org/pdf/2512.16893v1",
      "project": "https://research.nvidia.com/labs/amri/projects/instant4d"
    },
    "bibtex": ""
  },
  {
    "title": "Pixel Seal: Adversarial-only training for invisible image and video watermarking",
    "authors": [
      "Tom Souek",
      "Pierre Fernandez",
      "Hady Elsahar",
      "Sylvestre-Alvise Rebuffi",
      "Valeriu Lacatusu",
      "Tuan Tran",
      "Tom Sander",
      "Alexandre Mourachko"
    ],
    "abstract": "Invisible watermarking is essential for tracing the provenance of digital content. However, training state-of-the-art models remains notoriously difficult, with current approaches often struggling to balance robustness against true imperceptibility. This work introduces Pixel Seal, which sets a new state-of-the-art for image and video watermarking. We first identify three fundamental issues of existing methods: (i) the reliance on proxy perceptual losses such as MSE and LPIPS that fail to mimic human perception and result in visible watermark artifacts; (ii) the optimization instability caused by conflicting objectives, which necessitates exhaustive hyperparameter tuning; and (iii) reduced robustness and imperceptibility of watermarks when scaling models to high-resolution images and videos. To overcome these issues, we first propose an adversarial-only training paradigm that eliminates unreliable pixel-wise imperceptibility losses. Second, we introduce a three-stage training schedule that stabilizes convergence by decoupling robustness and imperceptibility. Third, we address the resolution gap via high-resolution adaptation, employing JND-based attenuation and training-time inference simulation to eliminate upscaling artifacts. We thoroughly evaluate the robustness and imperceptibility of Pixel Seal on different image types and across a wide range of transformations, and show clear improvements over the state-of-the-art. We finally demonstrate that the model efficiently adapts to video via temporal watermark pooling, positioning Pixel Seal as a practical and scalable solution for reliable provenance in real-world image and video settings.",
    "arxiv_url": "https://arxiv.org/abs/2512.16874v1",
    "pdf_url": "https://arxiv.org/pdf/2512.16874v1",
    "published_date": "2025-12-18",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CR",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "simulation",
      "upscaling"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.16874v1",
      "pdf": "https://arxiv.org/pdf/2512.16874v1"
    },
    "bibtex": ""
  },
  {
    "title": "Kling-Omni Technical Report",
    "authors": [
      "Kling Team",
      "Jialu Chen",
      "Yuanzheng Ci",
      "Xiangyu Du",
      "Zipeng Feng",
      "Kun Gai",
      "Sainan Guo",
      "Feng Han",
      "Jingbin He",
      "Kang He",
      "Xiao Hu",
      "Xiaohua Hu",
      "Boyuan Jiang",
      "Fangyuan Kong",
      "Hang Li",
      "Jie Li",
      "Qingyu Li",
      "Shen Li",
      "Xiaohan Li",
      "Yan Li",
      "Jiajun Liang",
      "Borui Liao",
      "Yiqiao Liao",
      "Weihong Lin",
      "Quande Liu",
      "Xiaokun Liu",
      "Yilun Liu",
      "Yuliang Liu",
      "Shun Lu",
      "Hangyu Mao",
      "Yunyao Mao",
      "Haodong Ouyang",
      "Wenyu Qin",
      "Wanqi Shi",
      "Xiaoyu Shi",
      "Lianghao Su",
      "Haozhi Sun",
      "Peiqin Sun",
      "Pengfei Wan",
      "Chao Wang",
      "Chenyu Wang",
      "Meng Wang",
      "Qiulin Wang",
      "Runqi Wang",
      "Xintao Wang",
      "Xuebo Wang",
      "Zekun Wang",
      "Min Wei",
      "Tiancheng Wen",
      "Guohao Wu",
      "Xiaoshi Wu",
      "Zhenhua Wu",
      "Da Xie",
      "Yingtong Xiong",
      "Yulong Xu",
      "Sile Yang",
      "Zikang Yang",
      "Weicai Ye",
      "Ziyang Yuan",
      "Shenglong Zhang",
      "Shuaiyu Zhang",
      "Yuanxing Zhang",
      "Yufan Zhang",
      "Wenzheng Zhao",
      "Ruiliang Zhou",
      "Yan Zhou",
      "Guosheng Zhu",
      "Yongjie Zhu"
    ],
    "abstract": "We present Kling-Omni, a generalist generative framework designed to synthesize high-fidelity videos directly from multimodal visual language inputs. Adopting an end-to-end perspective, Kling-Omni bridges the functional separation among diverse video generation, editing, and intelligent reasoning tasks, integrating them into a holistic system. Unlike disjointed pipeline approaches, Kling-Omni supports a diverse range of user inputs, including text instructions, reference images, and video contexts, processing them into a unified multimodal representation to deliver cinematic-quality and highly-intelligent video content creation. To support these capabilities, we constructed a comprehensive data system that serves as the foundation for multimodal video creation. The framework is further empowered by efficient large-scale pre-training strategies and infrastructure optimizations for inference. Comprehensive evaluations reveal that Kling-Omni demonstrates exceptional capabilities in in-context generation, reasoning-based editing, and multimodal instruction following. Moving beyond a content creation tool, we believe Kling-Omni is a pivotal advancement toward multimodal world simulators capable of perceiving, reasoning, generating and interacting with the dynamic and complex worlds.",
    "arxiv_url": "https://arxiv.org/abs/2512.16776v1",
    "pdf_url": "https://arxiv.org/pdf/2512.16776v1",
    "published_date": "2025-12-18",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "dit",
      "video generation",
      "evaluation",
      "world simulator",
      "efficient"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.16776v1",
      "pdf": "https://arxiv.org/pdf/2512.16776v1"
    },
    "bibtex": ""
  },
  {
    "title": "Hazedefy: A Lightweight Real-Time Image and Video Dehazing Pipeline for Practical Deployment",
    "authors": [
      "Ayush Bhavsar"
    ],
    "abstract": "This paper introduces Hazedefy, a lightweight and application-focused dehazing pipeline intended for real-time video and live camera feed enhancement. Hazedefy prioritizes computational simplicity and practical deployability on consumer-grade hardware, building upon the Dark Channel Prior (DCP) concept and the atmospheric scattering model. Key elements include gamma-adaptive reconstruction, a fast transmission approximation with lower bounds for numerical stability, a stabilized atmospheric light estimator based on fractional top-pixel averaging, and an optional color balance stage. The pipeline is suitable for mobile and embedded applications, as experimental demonstrations on real-world images and videos show improved visibility and contrast without requiring GPU acceleration.",
    "arxiv_url": "https://arxiv.org/abs/2512.16609v1",
    "pdf_url": "https://arxiv.org/pdf/2512.16609v1",
    "published_date": "2025-12-18",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "concept",
      "acceleration"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.16609v1",
      "pdf": "https://arxiv.org/pdf/2512.16609v1"
    },
    "bibtex": ""
  },
  {
    "title": "Factorized Video Generation: Decoupling Scene Construction and Temporal Synthesis in Text-to-Video Diffusion Models",
    "authors": [
      "Mariam Hassan",
      "Bastien Van Delft",
      "Wuyang Li",
      "Alexandre Alahi"
    ],
    "abstract": "State-of-the-art Text-to-Video (T2V) diffusion models can generate visually impressive results, yet they still frequently fail to compose complex scenes or follow logical temporal instructions. In this paper, we argue that many errors, including apparent motion failures, originate from the model's inability to construct a semantically correct or logically consistent initial frame. We introduce Factorized Video Generation (FVG), a pipeline that decouples these tasks by decomposing the Text-to-Video generation into three specialized stages: (1) Reasoning, where a Large Language Model (LLM) rewrites the video prompt to describe only the initial scene, resolving temporal ambiguities; (2) Composition, where a Text-to-Image (T2I) model synthesizes a high-quality, compositionally-correct anchor frame from this new prompt; and (3) Temporal Synthesis, where a video model, finetuned to understand this anchor, focuses its entire capacity on animating the scene and following the prompt. Our decomposed approach sets a new state-of-the-art on the T2V CompBench benchmark and significantly improves all tested models on VBench2. Furthermore, we show that visual anchoring allows us to cut the number of sampling steps by 70% without any loss in performance, leading to a substantial speed-up in sampling. Factorized Video Generation offers a simple yet practical path toward more efficient, robust, and controllable video synthesis",
    "arxiv_url": "https://arxiv.org/abs/2512.16371v1",
    "pdf_url": "https://arxiv.org/pdf/2512.16371v1",
    "published_date": "2025-12-18",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "video diffusion",
      "controllable",
      "text-to-video",
      "video generation",
      "efficient",
      "video synthesis",
      "diffusion model",
      "t2v",
      "benchmark"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.16371v1",
      "pdf": "https://arxiv.org/pdf/2512.16371v1"
    },
    "bibtex": ""
  },
  {
    "title": "TurboDiffusion: Accelerating Video Diffusion Models by 100-200 Times",
    "authors": [
      "Jintao Zhang",
      "Kaiwen Zheng",
      "Kai Jiang",
      "Haoxu Wang",
      "Ion Stoica",
      "Joseph E. Gonzalez",
      "Jianfei Chen",
      "Jun Zhu"
    ],
    "abstract": "We introduce TurboDiffusion, a video generation acceleration framework that can speed up end-to-end diffusion generation by 100-200x while maintaining video quality. TurboDiffusion mainly relies on several components for acceleration: (1) Attention acceleration: TurboDiffusion uses low-bit SageAttention and trainable Sparse-Linear Attention (SLA) to speed up attention computation. (2) Step distillation: TurboDiffusion adopts rCM for efficient step distillation. (3) W8A8 quantization: TurboDiffusion quantizes model parameters and activations to 8 bits to accelerate linear layers and compress the model. In addition, TurboDiffusion incorporates several other engineering optimizations.   We conduct experiments on the Wan2.2-I2V-14B-720P, Wan2.1-T2V-1.3B-480P, Wan2.1-T2V-14B-720P, and Wan2.1-T2V-14B-480P models. Experimental results show that TurboDiffusion achieves 100-200x speedup for video generation even on a single RTX 5090 GPU, while maintaining comparable video quality. The GitHub repository, which includes model checkpoints and easy-to-use code, is available at https://github.com/thu-ml/TurboDiffusion.",
    "arxiv_url": "https://arxiv.org/abs/2512.16093v1",
    "pdf_url": "https://arxiv.org/pdf/2512.16093v1",
    "published_date": "2025-12-18",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "github_url": "https://github.com/thu-ml/TurboDiffusion",
    "keywords": [
      "acceleration",
      "video diffusion",
      "distillation",
      "dit",
      "video generation",
      "i2v",
      "diffusion model",
      "t2v",
      "efficient"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.16093v1",
      "pdf": "https://arxiv.org/pdf/2512.16093v1",
      "github": "https://github.com/thu-ml/TurboDiffusion"
    },
    "bibtex": ""
  },
  {
    "title": "CoVAR: Co-generation of Video and Action for Robotic Manipulation via Multi-Modal Diffusion",
    "authors": [
      "Liudi Yang",
      "Yang Bai",
      "George Eskandar",
      "Fengyi Shen",
      "Mohammad Altillawi",
      "Dong Chen",
      "Ziyuan Liu",
      "Abhinav Valada"
    ],
    "abstract": "We present a method to generate video-action pairs that follow text instructions, starting from an initial image observation and the robot's joint states. Our approach automatically provides action labels for video diffusion models, overcoming the common lack of action annotations and enabling their full use for robotic policy learning. Existing methods either adopt two-stage pipelines, which limit tightly coupled cross-modal information sharing, or rely on adapting a single-modal diffusion model for a joint distribution that cannot fully leverage pretrained video knowledge. To overcome these limitations, we (1) extend a pretrained video diffusion model with a parallel, dedicated action diffusion model that preserves pretrained knowledge, (2) introduce a Bridge Attention mechanism to enable effective cross-modal interaction, and (3) design an action refinement module to convert coarse actions into precise controls for low-resolution datasets. Extensive evaluations on multiple public benchmarks and real-world datasets demonstrate that our method generates higher-quality videos, more accurate actions, and significantly outperforms existing baselines, offering a scalable framework for leveraging large-scale video data for robotic learning.",
    "arxiv_url": "https://arxiv.org/abs/2512.16023v1",
    "pdf_url": "https://arxiv.org/pdf/2512.16023v1",
    "published_date": "2025-12-17",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "multi-modal",
      "video diffusion",
      "evaluation",
      "diffusion model",
      "benchmark"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.16023v1",
      "pdf": "https://arxiv.org/pdf/2512.16023v1"
    },
    "bibtex": ""
  },
  {
    "title": "Spatia: Video Generation with Updatable Spatial Memory",
    "authors": [
      "Jinjing Zhao",
      "Fangyun Wei",
      "Zhening Liu",
      "Hongyang Zhang",
      "Chang Xu",
      "Yan Lu"
    ],
    "abstract": "Existing video generation models struggle to maintain long-term spatial and temporal consistency due to the dense, high-dimensional nature of video signals. To overcome this limitation, we propose Spatia, a spatial memory-aware video generation framework that explicitly preserves a 3D scene point cloud as persistent spatial memory. Spatia iteratively generates video clips conditioned on this spatial memory and continuously updates it through visual SLAM. This dynamic-static disentanglement design enhances spatial consistency throughout the generation process while preserving the model's ability to produce realistic dynamic entities. Furthermore, Spatia enables applications such as explicit camera control and 3D-aware interactive editing, providing a geometrically grounded framework for scalable, memory-driven video generation.",
    "arxiv_url": "https://arxiv.org/abs/2512.15716v1",
    "pdf_url": "https://arxiv.org/pdf/2512.15716v1",
    "published_date": "2025-12-17",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "interactive",
      "temporal consistency",
      "3d-aware",
      "dit",
      "video generation",
      "camera control"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.15716v1",
      "pdf": "https://arxiv.org/pdf/2512.15716v1"
    },
    "bibtex": ""
  },
  {
    "title": "End-to-End Training for Autoregressive Video Diffusion via Self-Resampling",
    "authors": [
      "Yuwei Guo",
      "Ceyuan Yang",
      "Hao He",
      "Yang Zhao",
      "Meng Wei",
      "Zhenheng Yang",
      "Weilin Huang",
      "Dahua Lin"
    ],
    "abstract": "Autoregressive video diffusion models hold promise for world simulation but are vulnerable to exposure bias arising from the train-test mismatch. While recent works address this via post-training, they typically rely on a bidirectional teacher model or online discriminator. To achieve an end-to-end solution, we introduce Resampling Forcing, a teacher-free framework that enables training autoregressive video models from scratch and at scale. Central to our approach is a self-resampling scheme that simulates inference-time model errors on history frames during training. Conditioned on these degraded histories, a sparse causal mask enforces temporal causality while enabling parallel training with frame-level diffusion loss. To facilitate efficient long-horizon generation, we further introduce history routing, a parameter-free mechanism that dynamically retrieves the top-k most relevant history frames for each query. Experiments demonstrate that our approach achieves performance comparable to distillation-based baselines while exhibiting superior temporal consistency on longer videos owing to native-length training.",
    "arxiv_url": "https://arxiv.org/abs/2512.15702v1",
    "pdf_url": "https://arxiv.org/pdf/2512.15702v1",
    "published_date": "2025-12-17",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "temporal consistency",
      "video diffusion",
      "distillation",
      "dit",
      "diffusion model",
      "simulation",
      "autoregressive",
      "efficient"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.15702v1",
      "pdf": "https://arxiv.org/pdf/2512.15702v1"
    },
    "bibtex": ""
  },
  {
    "title": "Skyra: AI-Generated Video Detection via Grounded Artifact Reasoning",
    "authors": [
      "Yifei Li",
      "Wenzhao Zheng",
      "Yanran Zhang",
      "Runze Sun",
      "Yu Zheng",
      "Lei Chen",
      "Jie Zhou",
      "Jiwen Lu"
    ],
    "abstract": "The misuse of AI-driven video generation technologies has raised serious social concerns, highlighting the urgent need for reliable AI-generated video detectors. However, most existing methods are limited to binary classification and lack the necessary explanations for human interpretation. In this paper, we present Skyra, a specialized multimodal large language model (MLLM) that identifies human-perceivable visual artifacts in AI-generated videos and leverages them as grounded evidence for both detection and explanation. To support this objective, we construct ViF-CoT-4K for Supervised Fine-Tuning (SFT), which represents the first large-scale AI-generated video artifact dataset with fine-grained human annotations. We then develop a two-stage training strategy that systematically enhances our model's spatio-temporal artifact perception, explanation capability, and detection accuracy. To comprehensively evaluate Skyra, we introduce ViF-Bench, a benchmark comprising 3K high-quality samples generated by over ten state-of-the-art video generators. Extensive experiments demonstrate that Skyra surpasses existing methods across multiple benchmarks, while our evaluation yields valuable insights for advancing explainable AI-generated video detection.",
    "arxiv_url": "https://arxiv.org/abs/2512.15693v1",
    "pdf_url": "https://arxiv.org/pdf/2512.15693v1",
    "published_date": "2025-12-17",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "benchmark",
      "evaluation",
      "video generation"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.15693v1",
      "pdf": "https://arxiv.org/pdf/2512.15693v1"
    },
    "bibtex": ""
  },
  {
    "title": "Lights, Camera, Consistency: A Multistage Pipeline for Character-Stable AI Video Stories",
    "authors": [
      "Chayan Jain",
      "Rishant Sharma",
      "Archit Garg",
      "Ishan Bhanuka",
      "Pratik Narang",
      "Dhruv Kumar"
    ],
    "abstract": "Generating long, cohesive video stories with consistent characters is a significant challenge for current text-to-video AI. We introduce a method that approaches video generation in a filmmaker-like manner. Instead of creating a video in one step, our proposed pipeline first uses a large language model to generate a detailed production script. This script guides a text-to-image model in creating consistent visuals for each character, which then serve as anchors for a video generation model to synthesize each scene individually. Our baseline comparisons validate the necessity of this multi-stage decomposition; specifically, we observe that removing the visual anchoring mechanism results in a catastrophic drop in character consistency scores (from 7.99 to 0.55), confirming that visual priors are essential for identity preservation. Furthermore, we analyze cultural disparities in current models, revealing distinct biases in subject consistency and dynamic degree between Indian vs Western-themed generations.",
    "arxiv_url": "https://arxiv.org/abs/2512.16954v1",
    "pdf_url": "https://arxiv.org/pdf/2512.16954v1",
    "published_date": "2025-12-17",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "identity",
      "text-to-video",
      "video generation",
      "film"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.16954v1",
      "pdf": "https://arxiv.org/pdf/2512.16954v1"
    },
    "bibtex": ""
  },
  {
    "title": "GRAN-TED: Generating Robust, Aligned, and Nuanced Text Embedding for Diffusion Models",
    "authors": [
      "Bozhou Li",
      "Sihan Yang",
      "Yushuo Guan",
      "Ruichuan An",
      "Xinlong Chen",
      "Yang Shi",
      "Pengfei Wan",
      "Wentao Zhang",
      "Yuanxing zhang"
    ],
    "abstract": "The text encoder is a critical component of text-to-image and text-to-video diffusion models, fundamentally determining the semantic fidelity of the generated content. However, its development has been hindered by two major challenges: the lack of an efficient evaluation framework that reliably predicts downstream generation performance, and the difficulty of effectively adapting pretrained language models for visual synthesis. To address these issues, we introduce GRAN-TED, a paradigm to Generate Robust, Aligned, and Nuanced Text Embeddings for Diffusion models. Our contribution is twofold. First, we propose TED-6K, a novel text-only benchmark that enables efficient and robust assessment of an encoder's representational quality without requiring costly end-to-end model training. We demonstrate that performance on TED-6K, standardized via a lightweight, unified adapter, strongly correlates with an encoder's effectiveness in downstream generation tasks. Notably, under our experimental setup, compared with training a diffusion model from scratch, evaluating with TED-6K is about \\textbf{750$\\times$ faster}. Second, guided by this validated framework, we develop a superior text encoder using a novel two-stage training paradigm. This process involves an initial fine-tuning stage on a Multimodal Large Language Model for better visual representation, followed by a layer-wise weighting method to extract more nuanced and potent text features. Our experiments show that the resulting GRAN-TED encoder not only achieves state-of-the-art performance on TED-6K but also leads to demonstrable performance gains in text-to-image and text-to-video generation. Our TED-6K dataset and evaluation code are available at the following link: https://anonymous.4open.science/r/GRAN-TED-4FCC/.",
    "arxiv_url": "https://arxiv.org/abs/2512.15560v2",
    "pdf_url": "https://arxiv.org/pdf/2512.15560v2",
    "published_date": "2025-12-17",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "video diffusion",
      "text-to-video",
      "video generation",
      "efficient",
      "evaluation",
      "diffusion model",
      "benchmark"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.15560v2",
      "pdf": "https://arxiv.org/pdf/2512.15560v2",
      "project": "https://anonymous.4open.science/r/GRAN-TED-4FCC"
    },
    "bibtex": ""
  },
  {
    "title": "DeX-Portrait: Disentangled and Expressive Portrait Animation via Explicit and Latent Motion Representations",
    "authors": [
      "Yuxiang Shi",
      "Zhe Li",
      "Yanwen Wang",
      "Hao Zhu",
      "Xun Cao",
      "Ligang Liu"
    ],
    "abstract": "Portrait animation from a single source image and a driving video is a long-standing problem. Recent approaches tend to adopt diffusion-based image/video generation models for realistic and expressive animation. However, none of these diffusion models realizes high-fidelity disentangled control between the head pose and facial expression, hindering applications like expression-only or pose-only editing and animation. To address this, we propose DeX-Portrait, a novel approach capable of generating expressive portrait animation driven by disentangled pose and expression signals. Specifically, we represent the pose as an explicit global transformation and the expression as an implicit latent code. First, we design a powerful motion trainer to learn both pose and expression encoders for extracting precise and decomposed driving signals. Then we propose to inject the pose transformation into the diffusion model through a dual-branch conditioning mechanism, and the expression latent through cross attention. Finally, we design a progressive hybrid classifier-free guidance for more faithful identity consistency. Experiments show that our method outperforms state-of-the-art baselines on both animation quality and disentangled controllability.",
    "arxiv_url": "https://arxiv.org/abs/2512.15524v1",
    "pdf_url": "https://arxiv.org/pdf/2512.15524v1",
    "published_date": "2025-12-17",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "diffusion model",
      "video generation",
      "identity",
      "dit"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.15524v1",
      "pdf": "https://arxiv.org/pdf/2512.15524v1"
    },
    "bibtex": ""
  },
  {
    "title": "SoliReward: Mitigating Susceptibility to Reward Hacking and Annotation Noise in Video Generation Reward Models",
    "authors": [
      "Jiesong Lian",
      "Ruizhe Zhong",
      "Zixiang Zhou",
      "Xiaoyue Mi",
      "Yixue Hao",
      "Yuan Zhou",
      "Qinglin Lu",
      "Long Hu",
      "Junchi Yan"
    ],
    "abstract": "Post-training alignment of video generation models with human preferences is a critical goal. Developing effective Reward Models (RMs) for this process faces significant methodological hurdles. Current data collection paradigms, reliant on in-prompt pairwise annotations, suffer from labeling noise. Concurrently, the architectural design of VLM-based RMs, particularly their output mechanisms, remains underexplored. Furthermore, RM is susceptible to reward hacking in post-training. To mitigate these limitations, we propose SoliReward, a systematic framework for video RM training. Our framework first sources high-quality, cost-efficient data via single-item binary annotations, then constructs preference pairs using a cross-prompt pairing strategy. Architecturally, we employ a Hierarchical Progressive Query Attention mechanism to enhance feature aggregation. Finally, we introduce a modified BT loss that explicitly accommodates win-tie scenarios. This approach regularizes the RM's score distribution for positive samples, providing more nuanced preference signals to alleviate over-focus on a small number of top-scoring samples. Our approach is validated on benchmarks evaluating physical plausibility, subject deformity, and semantic alignment, demonstrating improvements in direct RM evaluation metrics and in the efficacy of post-training on video generation models. Code and benchmark are available at https://github.com/lian700/SoliReward",
    "arxiv_url": "https://arxiv.org/abs/2512.22170v2",
    "pdf_url": "https://arxiv.org/pdf/2512.22170v2",
    "published_date": "2025-12-17",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "github_url": "https://github.com/lian700/SoliReward",
    "keywords": [
      "physical",
      "video generation",
      "efficient",
      "evaluation",
      "benchmark"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.22170v2",
      "pdf": "https://arxiv.org/pdf/2512.22170v2",
      "github": "https://github.com/lian700/SoliReward"
    },
    "bibtex": ""
  },
  {
    "title": "Audio-Visual Cross-Modal Compression for Generative Face Video Coding",
    "authors": [
      "Youmin Xu",
      "Mengxi Guo",
      "Shijie Zhao",
      "Weiqi Li",
      "Junlin Li",
      "Li Zhang",
      "Jian Zhang"
    ],
    "abstract": "Generative face video coding (GFVC) is vital for modern applications like video conferencing, yet existing methods primarily focus on video motion while neglecting the significant bitrate contribution of audio. Despite the well-established correlation between audio and lip movements, this cross-modal coherence has not been systematically exploited for compression. To address this, we propose an Audio-Visual Cross-Modal Compression (AVCC) framework that jointly compresses audio and video streams. Our framework extracts motion information from video and tokenizes audio features, then aligns them through a unified audio-video diffusion process. This allows synchronized reconstruction of both modalities from a shared representation. In extremely low-rate scenarios, AVCC can even reconstruct one modality from the other. Experiments show that AVCC significantly outperforms the Versatile Video Coding (VVC) standard and state-of-the-art GFVC schemes in rate-distortion performance, paving the way for more efficient multimodal communication systems.",
    "arxiv_url": "https://arxiv.org/abs/2512.15262v1",
    "pdf_url": "https://arxiv.org/pdf/2512.15262v1",
    "published_date": "2025-12-17",
    "categories": [
      "eess.IV",
      "cs.MM"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "video diffusion"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.15262v1",
      "pdf": "https://arxiv.org/pdf/2512.15262v1"
    },
    "bibtex": ""
  },
  {
    "title": "3DProxyImg: Controllable 3D-Aware Animation Synthesis from Single Image via 2D-3D Aligned Proxy Embedding",
    "authors": [
      "Yupeng Zhu",
      "Xiongzhen Zhang",
      "Ye Chen",
      "Bingbing Ni"
    ],
    "abstract": "3D animation is central to modern visual media, yet traditional production pipelines remain labor-intensive, expertise-demanding, and computationally expensive. Recent AIGC-based approaches partially automate asset creation and rigging, but they either inherit the heavy costs of full 3D pipelines or rely on video-synthesis paradigms that sacrifice 3D controllability and interactivity. We focus on single-image 3D animation generation and argue that progress is fundamentally constrained by a trade-off between rendering quality and 3D control.   To address this limitation, we propose a lightweight 3D animation framework that decouples geometric control from appearance synthesis. The core idea is a 2D-3D aligned proxy representation that uses a coarse 3D estimate as a structural carrier, while delegating high-fidelity appearance and view synthesis to learned image-space generative priors. This proxy formulation enables 3D-aware motion control and interaction comparable to classical pipelines, without requiring accurate geometry or expensive optimization, and naturally extends to coherent background animation. Extensive experiments demonstrate that our method achieves efficient animation generation on low-power platforms and outperforms video-based 3D animation generation in identity preservation, geometric and textural consistency, and the level of precise, interactive control it offers to users.",
    "arxiv_url": "https://arxiv.org/abs/2512.15126v2",
    "pdf_url": "https://arxiv.org/pdf/2512.15126v2",
    "published_date": "2025-12-17",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "interactive",
      "controllable",
      "3d-aware",
      "dit",
      "identity",
      "efficient",
      "motion control"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.15126v2",
      "pdf": "https://arxiv.org/pdf/2512.15126v2"
    },
    "bibtex": ""
  },
  {
    "title": "TalkVerse: Democratizing Minute-Long Audio-Driven Video Generation",
    "authors": [
      "Zhenzhi Wang",
      "Jian Wang",
      "Ke Ma",
      "Dahua Lin",
      "Bing Zhou"
    ],
    "abstract": "We introduce TalkVerse, a large-scale, open corpus for single-person, audio-driven talking video generation designed to enable fair, reproducible comparison across methods. While current state-of-the-art systems rely on closed data or compute-heavy models, TalkVerse offers 2.3 million high-resolution (720p/1080p) audio-video synchronized clips totaling 6.3k hours. These are curated from over 60k hours of video via a transparent pipeline that includes scene-cut detection, aesthetic assessment, strict audio-visual synchronization checks, and comprehensive annotations including 2D skeletons and structured visual/audio-style captions. Leveraging TalkVerse, we present a reproducible 5B DiT baseline built on Wan2.2-5B. By utilizing a video VAE with a high downsampling ratio and a sliding window mechanism with motion-frame context, our model achieves minute-long generation with low drift. It delivers comparable lip-sync and visual quality to the 14B Wan-S2V model but with 10$\\times$ lower inference cost. To enhance storytelling in long videos, we integrate an MLLM director to rewrite prompts based on audio and visual cues. Furthermore, our model supports zero-shot video dubbing via controlled latent noise injection. We open-source the dataset, training recipes, and 5B checkpoints to lower barriers for research in audio-driven human video generation. Project Page: https://zhenzhiwang.github.io/talkverse/",
    "arxiv_url": "https://arxiv.org/abs/2512.14938v1",
    "pdf_url": "https://arxiv.org/pdf/2512.14938v1",
    "published_date": "2025-12-16",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.MM",
      "cs.SD"
    ],
    "github_url": "",
    "keywords": [
      "dit",
      "video generation",
      "long video",
      "style",
      "audio-driven"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.14938v1",
      "pdf": "https://arxiv.org/pdf/2512.14938v1",
      "project": "https://zhenzhiwang.github.io/talkverse"
    },
    "bibtex": ""
  },
  {
    "title": "MemFlow: Flowing Adaptive Memory for Consistent and Efficient Long Video Narratives",
    "authors": [
      "Sihui Ji",
      "Xi Chen",
      "Shuai Yang",
      "Xin Tao",
      "Pengfei Wan",
      "Hengshuang Zhao"
    ],
    "abstract": "The core challenge for streaming video generation is maintaining the content consistency in long context, which poses high requirement for the memory design. Most existing solutions maintain the memory by compressing historical frames with predefined strategies. However, different to-generate video chunks should refer to different historical cues, which is hard to satisfy with fixed strategies. In this work, we propose MemFlow to address this problem. Specifically, before generating the coming chunk, we dynamically update the memory bank by retrieving the most relevant historical frames with the text prompt of this chunk. This design enables narrative coherence even if new event happens or scenario switches in future frames. In addition, during generation, we only activate the most relevant tokens in the memory bank for each query in the attention layers, which effectively guarantees the generation efficiency. In this way, MemFlow achieves outstanding long-context consistency with negligible computation burden (7.9% speed reduction compared with the memory-free baseline) and keeps the compatibility with any streaming video generation model with KV cache.",
    "arxiv_url": "https://arxiv.org/abs/2512.14699v1",
    "pdf_url": "https://arxiv.org/pdf/2512.14699v1",
    "published_date": "2025-12-16",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "dit",
      "streaming",
      "video generation",
      "long video",
      "efficient"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.14699v1",
      "pdf": "https://arxiv.org/pdf/2512.14699v1"
    },
    "bibtex": ""
  },
  {
    "title": "WorldPlay: Towards Long-Term Geometric Consistency for Real-Time Interactive World Modeling",
    "authors": [
      "Wenqiang Sun",
      "Haiyu Zhang",
      "Haoyuan Wang",
      "Junta Wu",
      "Zehan Wang",
      "Zhenwei Wang",
      "Yunhong Wang",
      "Jun Zhang",
      "Tengfei Wang",
      "Chunchao Guo"
    ],
    "abstract": "This paper presents WorldPlay, a streaming video diffusion model that enables real-time, interactive world modeling with long-term geometric consistency, resolving the trade-off between speed and memory that limits current methods. WorldPlay draws power from three key innovations. 1) We use a Dual Action Representation to enable robust action control in response to the user's keyboard and mouse inputs. 2) To enforce long-term consistency, our Reconstituted Context Memory dynamically rebuilds context from past frames and uses temporal reframing to keep geometrically important but long-past frames accessible, effectively alleviating memory attenuation. 3) We also propose Context Forcing, a novel distillation method designed for memory-aware model. Aligning memory context between the teacher and student preserves the student's capacity to use long-range information, enabling real-time speeds while preventing error drift. Taken together, WorldPlay generates long-horizon streaming 720p video at 24 FPS with superior consistency, comparing favorably with existing techniques and showing strong generalization across diverse scenes. Project page and online demo can be found: https://3d-models.hunyuan.tencent.com/world/ and https://3d.hunyuan.tencent.com/sceneTo3D.",
    "arxiv_url": "https://arxiv.org/abs/2512.14614v1",
    "pdf_url": "https://arxiv.org/pdf/2512.14614v1",
    "published_date": "2025-12-16",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "interactive",
      "video diffusion",
      "distillation",
      "streaming",
      "world model",
      "diffusion model"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.14614v1",
      "pdf": "https://arxiv.org/pdf/2512.14614v1",
      "project": "https://3d-models.hunyuan.tencent.com/world"
    },
    "bibtex": ""
  },
  {
    "title": "S2D: Sparse-To-Dense Keymask Distillation for Unsupervised Video Instance Segmentation",
    "authors": [
      "Leon Sick",
      "Lukas Hoyer",
      "Dominik Engel",
      "Pedro Hermosilla",
      "Timo Ropinski"
    ],
    "abstract": "In recent years, the state-of-the-art in unsupervised video instance segmentation has heavily relied on synthetic video data, generated from object-centric image datasets such as ImageNet. However, video synthesis by artificially shifting and scaling image instance masks fails to accurately model realistic motion in videos, such as perspective changes, movement by parts of one or multiple instances, or camera motion. To tackle this issue, we propose an unsupervised video instance segmentation model trained exclusively on real video data. We start from unsupervised instance segmentation masks on individual video frames. However, these single-frame segmentations exhibit temporal noise and their quality varies through the video. Therefore, we establish temporal coherence by identifying high-quality keymasks in the video by leveraging deep motion priors. The sparse keymask pseudo-annotations are then used to train a segmentation model for implicit mask propagation, for which we propose a Sparse-To-Dense Distillation approach aided by a Temporal DropLoss. After training the final model on the resulting dense labelset, our approach outperforms the current state-of-the-art across various benchmarks.",
    "arxiv_url": "https://arxiv.org/abs/2512.14440v1",
    "pdf_url": "https://arxiv.org/pdf/2512.14440v1",
    "published_date": "2025-12-16",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "benchmark",
      "video synthesis",
      "distillation"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.14440v1",
      "pdf": "https://arxiv.org/pdf/2512.14440v1"
    },
    "bibtex": ""
  },
  {
    "title": "SS4D: Native 4D Generative Model via Structured Spacetime Latents",
    "authors": [
      "Zhibing Li",
      "Mengchen Zhang",
      "Tong Wu",
      "Jing Tan",
      "Jiaqi Wang",
      "Dahua Lin"
    ],
    "abstract": "We present SS4D, a native 4D generative model that synthesizes dynamic 3D objects directly from monocular video. Unlike prior approaches that construct 4D representations by optimizing over 3D or video generative models, we train a generator directly on 4D data, achieving high fidelity, temporal coherence, and structural consistency. At the core of our method is a compressed set of structured spacetime latents. Specifically, (1) To address the scarcity of 4D training data, we build on a pre-trained single-image-to-3D model, preserving strong spatial consistency. (2) Temporal consistency is enforced by introducing dedicated temporal layers that reason across frames. (3) To support efficient training and inference over long video sequences, we compress the latent sequence along the temporal axis using factorized 4D convolutions and temporal downsampling blocks. In addition, we employ a carefully designed training strategy to enhance robustness against occlusion",
    "arxiv_url": "https://arxiv.org/abs/2512.14284v1",
    "pdf_url": "https://arxiv.org/pdf/2512.14284v1",
    "published_date": "2025-12-16",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "long video",
      "temporal consistency",
      "dit"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.14284v1",
      "pdf": "https://arxiv.org/pdf/2512.14284v1"
    },
    "bibtex": ""
  },
  {
    "title": "DRAW2ACT: Turning Depth-Encoded Trajectories into Robotic Demonstration Videos",
    "authors": [
      "Yang Bai",
      "Liudi Yang",
      "George Eskandar",
      "Fengyi Shen",
      "Mohammad Altillawi",
      "Ziyuan Liu",
      "Gitta Kutyniok"
    ],
    "abstract": "Video diffusion models provide powerful real-world simulators for embodied AI but remain limited in controllability for robotic manipulation. Recent works on trajectory-conditioned video generation address this gap but often rely on 2D trajectories or single modality conditioning, which restricts their ability to produce controllable and consistent robotic demonstrations. We present DRAW2ACT, a depth-aware trajectory-conditioned video generation framework that extracts multiple orthogonal representations from the input trajectory, capturing depth, semantics, shape and motion, and injects them into the diffusion model. Moreover, we propose to jointly generate spatially aligned RGB and depth videos, leveraging cross-modality attention mechanisms and depth supervision to enhance the spatio-temporal consistency. Finally, we introduce a multimodal policy model conditioned on the generated RGB and depth sequences to regress the robot's joint angles. Experiments on Bridge V2, Berkeley Autolab, and simulation benchmarks show that DRAW2ACT achieves superior visual fidelity and consistency while yielding higher manipulation success rates compared to existing baselines.",
    "arxiv_url": "https://arxiv.org/abs/2512.14217v1",
    "pdf_url": "https://arxiv.org/pdf/2512.14217v1",
    "published_date": "2025-12-16",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "temporal consistency",
      "controllable",
      "video diffusion",
      "dit",
      "trajectory",
      "video generation",
      "world simulator",
      "diffusion model",
      "simulation",
      "benchmark"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.14217v1",
      "pdf": "https://arxiv.org/pdf/2512.14217v1"
    },
    "bibtex": ""
  },
  {
    "title": "AnchorHOI: Zero-shot Generation of 4D Human-Object Interaction via Anchor-based Prior Distillation",
    "authors": [
      "Sisi Dai",
      "Kai Xu"
    ],
    "abstract": "Despite significant progress in text-driven 4D human-object interaction (HOI) generation with supervised methods, the scalability remains limited by the scarcity of large-scale 4D HOI datasets. To overcome this, recent approaches attempt zero-shot 4D HOI generation with pre-trained image diffusion models. However, interaction cues are minimally distilled during the generation process, restricting their applicability across diverse scenarios. In this paper, we propose AnchorHOI, a novel framework that thoroughly exploits hybrid priors by incorporating video diffusion models beyond image diffusion models, advancing 4D HOI generation. Nevertheless, directly optimizing high-dimensional 4D HOI with such priors remains challenging, particularly for human pose and compositional motion. To address this challenge, AnchorHOI introduces an anchor-based prior distillation strategy, which constructs interaction-aware anchors and then leverages them to guide generation in a tractable two-step process. Specifically, two tailored anchors are designed for 4D HOI generation: anchor Neural Radiance Fields (NeRFs) for expressive interaction composition, and anchor keypoints for realistic motion synthesis. Extensive experiments demonstrate that AnchorHOI outperforms previous methods with superior diversity and generalization.",
    "arxiv_url": "https://arxiv.org/abs/2512.14095v1",
    "pdf_url": "https://arxiv.org/pdf/2512.14095v1",
    "published_date": "2025-12-16",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "diffusion model",
      "video diffusion",
      "distillation"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.14095v1",
      "pdf": "https://arxiv.org/pdf/2512.14095v1"
    },
    "bibtex": ""
  },
  {
    "title": "SDAR-VL: Stable and Efficient Block-wise Diffusion for Vision-Language Understanding",
    "authors": [
      "Shuang Cheng",
      "Yuhua Jiang",
      "Zineng Zhou",
      "Dawei Liu",
      "Wang Tao",
      "Linfeng Zhang",
      "Biqing Qi",
      "Bowen Zhou"
    ],
    "abstract": "Block-wise discrete diffusion offers an attractive balance between parallel generation and causal dependency modeling, making it a promising backbone for vision-language modeling. However, its practical adoption has been limited by high training cost, slow convergence, and instability, which have so far kept it behind strong autoregressive (AR) baselines. We present \\textbf{SDAR-VL}, the first systematic application of block-wise discrete diffusion to large-scale vision-language understanding (VLU), together with an \\emph{integrated framework for efficient and stable training}. This framework unifies three components: (1) \\textbf{Asynchronous Block-wise Noise Scheduling} to diversify supervision within each batch; (2) \\textbf{Effective Mask Ratio Scaling} for unbiased loss normalization under stochastic masking; and (3) a \\textbf{Progressive Beta Noise Curriculum} that increases effective mask coverage while preserving corruption diversity. Experiments on 21 single-image, multi-image, and video benchmarks show that SDAR-VL consistently improves \\emph{training efficiency}, \\emph{convergence stability}, and \\emph{task performance} over conventional block diffusion. On this evaluation suite, SDAR-VL sets a new state of the art among diffusion-based vision-language models and, under matched settings, matches or surpasses strong AR baselines such as LLaVA-OneVision as well as the global diffusion baseline LLaDA-V, establishing block-wise diffusion as a practical backbone for VLU.",
    "arxiv_url": "https://arxiv.org/abs/2512.14068v1",
    "pdf_url": "https://arxiv.org/pdf/2512.14068v1",
    "published_date": "2025-12-16",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "benchmark",
      "evaluation",
      "efficient",
      "autoregressive"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.14068v1",
      "pdf": "https://arxiv.org/pdf/2512.14068v1"
    },
    "bibtex": ""
  },
  {
    "title": "DiffusionBrowser: Interactive Diffusion Previews via Multi-Branch Decoders",
    "authors": [
      "Susung Hong",
      "Chongjian Ge",
      "Zhifei Zhang",
      "Jui-Hsien Wang"
    ],
    "abstract": "Video diffusion models have revolutionized generative video synthesis, but they are imprecise, slow, and can be opaque during generation -- keeping users in the dark for a prolonged period. In this work, we propose DiffusionBrowser, a model-agnostic, lightweight decoder framework that allows users to interactively generate previews at any point (timestep or transformer block) during the denoising process. Our model can generate multi-modal preview representations that include RGB and scene intrinsics at more than 4$\\times$ real-time speed (less than 1 second for a 4-second video) that convey consistent appearance and motion to the final video. With the trained decoder, we show that it is possible to interactively guide the generation at intermediate noise steps via stochasticity reinjection and modal steering, unlocking a new control capability. Moreover, we systematically probe the model using the learned decoders, revealing how scene, object, and other details are composed and assembled during the otherwise black-box denoising process.",
    "arxiv_url": "https://arxiv.org/abs/2512.13690v1",
    "pdf_url": "https://arxiv.org/pdf/2512.13690v1",
    "published_date": "2025-12-15",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "multi-modal",
      "interactive",
      "video diffusion",
      "video synthesis",
      "denoising",
      "diffusion model"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.13690v1",
      "pdf": "https://arxiv.org/pdf/2512.13690v1"
    },
    "bibtex": ""
  },
  {
    "title": "LongVie 2: Multimodal Controllable Ultra-Long Video World Model",
    "authors": [
      "Jianxiong Gao",
      "Zhaoxi Chen",
      "Xian Liu",
      "Junhao Zhuang",
      "Chengming Xu",
      "Jianfeng Feng",
      "Yu Qiao",
      "Yanwei Fu",
      "Chenyang Si",
      "Ziwei Liu"
    ],
    "abstract": "Building video world models upon pretrained video generation systems represents an important yet challenging step toward general spatiotemporal intelligence. A world model should possess three essential properties: controllability, long-term visual quality, and temporal consistency. To this end, we take a progressive approach-first enhancing controllability and then extending toward long-term, high-quality generation. We present LongVie 2, an end-to-end autoregressive framework trained in three stages: (1) Multi-modal guidance, which integrates dense and sparse control signals to provide implicit world-level supervision and improve controllability; (2) Degradation-aware training on the input frame, bridging the gap between training and long-term inference to maintain high visual quality; and (3) History-context guidance, which aligns contextual information across adjacent clips to ensure temporal consistency. We further introduce LongVGenBench, a comprehensive benchmark comprising 100 high-resolution one-minute videos covering diverse real-world and synthetic environments. Extensive experiments demonstrate that LongVie 2 achieves state-of-the-art performance in long-range controllability, temporal coherence, and visual fidelity, and supports continuous video generation lasting up to five minutes, marking a significant step toward unified video world modeling.",
    "arxiv_url": "https://arxiv.org/abs/2512.13604v1",
    "pdf_url": "https://arxiv.org/pdf/2512.13604v1",
    "published_date": "2025-12-15",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "multi-modal",
      "temporal consistency",
      "controllable",
      "world model",
      "video generation",
      "long video",
      "autoregressive",
      "benchmark"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.13604v1",
      "pdf": "https://arxiv.org/pdf/2512.13604v1"
    },
    "bibtex": ""
  },
  {
    "title": "Seedance 1.5 pro: A Native Audio-Visual Joint Generation Foundation Model",
    "authors": [
      "Team Seedance",
      "Heyi Chen",
      "Siyan Chen",
      "Xin Chen",
      "Yanfei Chen",
      "Ying Chen",
      "Zhuo Chen",
      "Feng Cheng",
      "Tianheng Cheng",
      "Xinqi Cheng",
      "Xuyan Chi",
      "Jian Cong",
      "Jing Cui",
      "Qinpeng Cui",
      "Qide Dong",
      "Junliang Fan",
      "Jing Fang",
      "Zetao Fang",
      "Chengjian Feng",
      "Han Feng",
      "Mingyuan Gao",
      "Yu Gao",
      "Dong Guo",
      "Qiushan Guo",
      "Boyang Hao",
      "Qingkai Hao",
      "Bibo He",
      "Qian He",
      "Tuyen Hoang",
      "Ruoqing Hu",
      "Xi Hu",
      "Weilin Huang",
      "Zhaoyang Huang",
      "Zhongyi Huang",
      "Donglei Ji",
      "Siqi Jiang",
      "Wei Jiang",
      "Yunpu Jiang",
      "Zhuo Jiang",
      "Ashley Kim",
      "Jianan Kong",
      "Zhichao Lai",
      "Shanshan Lao",
      "Yichong Leng",
      "Ai Li",
      "Feiya Li",
      "Gen Li",
      "Huixia Li",
      "JiaShi Li",
      "Liang Li",
      "Ming Li",
      "Shanshan Li",
      "Tao Li",
      "Xian Li",
      "Xiaojie Li",
      "Xiaoyang Li",
      "Xingxing Li",
      "Yameng Li",
      "Yifu Li",
      "Yiying Li",
      "Chao Liang",
      "Han Liang",
      "Jianzhong Liang",
      "Ying Liang",
      "Zhiqiang Liang",
      "Wang Liao",
      "Yalin Liao",
      "Heng Lin",
      "Kengyu Lin",
      "Shanchuan Lin",
      "Xi Lin",
      "Zhijie Lin",
      "Feng Ling",
      "Fangfang Liu",
      "Gaohong Liu",
      "Jiawei Liu",
      "Jie Liu",
      "Jihao Liu",
      "Shouda Liu",
      "Shu Liu",
      "Sichao Liu",
      "Songwei Liu",
      "Xin Liu",
      "Xue Liu",
      "Yibo Liu",
      "Zikun Liu",
      "Zuxi Liu",
      "Junlin Lyu",
      "Lecheng Lyu",
      "Qian Lyu",
      "Han Mu",
      "Xiaonan Nie",
      "Jingzhe Ning",
      "Xitong Pan",
      "Yanghua Peng",
      "Lianke Qin",
      "Xueqiong Qu",
      "Yuxi Ren",
      "Kai Shen",
      "Guang Shi",
      "Lei Shi",
      "Yan Song",
      "Yinglong Song",
      "Fan Sun",
      "Li Sun",
      "Renfei Sun",
      "Yan Sun",
      "Zeyu Sun",
      "Wenjing Tang",
      "Yaxue Tang",
      "Zirui Tao",
      "Feng Wang",
      "Furui Wang",
      "Jinran Wang",
      "Junkai Wang",
      "Ke Wang",
      "Kexin Wang",
      "Qingyi Wang",
      "Rui Wang",
      "Sen Wang",
      "Shuai Wang",
      "Tingru Wang",
      "Weichen Wang",
      "Xin Wang",
      "Yanhui Wang",
      "Yue Wang",
      "Yuping Wang",
      "Yuxuan Wang",
      "Ziyu Wang",
      "Guoqiang Wei",
      "Wanru Wei",
      "Di Wu",
      "Guohong Wu",
      "Hanjie Wu",
      "Jian Wu",
      "Jie Wu",
      "Ruolan Wu",
      "Xinglong Wu",
      "Yonghui Wu",
      "Ruiqi Xia",
      "Liang Xiang",
      "Fei Xiao",
      "XueFeng Xiao",
      "Pan Xie",
      "Shuangyi Xie",
      "Shuang Xu",
      "Jinlan Xue",
      "Shen Yan",
      "Bangbang Yang",
      "Ceyuan Yang",
      "Jiaqi Yang",
      "Runkai Yang",
      "Tao Yang",
      "Yang Yang",
      "Yihang Yang",
      "ZhiXian Yang",
      "Ziyan Yang",
      "Songting Yao",
      "Yifan Yao",
      "Zilyu Ye",
      "Bowen Yu",
      "Jian Yu",
      "Chujie Yuan",
      "Linxiao Yuan",
      "Sichun Zeng",
      "Weihong Zeng",
      "Xuejiao Zeng",
      "Yan Zeng",
      "Chuntao Zhang",
      "Heng Zhang",
      "Jingjie Zhang",
      "Kuo Zhang",
      "Liang Zhang",
      "Liying Zhang",
      "Manlin Zhang",
      "Ting Zhang",
      "Weida Zhang",
      "Xiaohe Zhang",
      "Xinyan Zhang",
      "Yan Zhang",
      "Yuan Zhang",
      "Zixiang Zhang",
      "Fengxuan Zhao",
      "Huating Zhao",
      "Yang Zhao",
      "Hao Zheng",
      "Jianbin Zheng",
      "Xiaozheng Zheng",
      "Yangyang Zheng",
      "Yijie Zheng",
      "Jiexin Zhou",
      "Jiahui Zhu",
      "Kuan Zhu",
      "Shenhan Zhu",
      "Wenjia Zhu",
      "Benhui Zou",
      "Feilong Zuo"
    ],
    "abstract": "Recent strides in video generation have paved the way for unified audio-visual generation. In this work, we present Seedance 1.5 pro, a foundational model engineered specifically for native, joint audio-video generation. Leveraging a dual-branch Diffusion Transformer architecture, the model integrates a cross-modal joint module with a specialized multi-stage data pipeline, achieving exceptional audio-visual synchronization and superior generation quality. To ensure practical utility, we implement meticulous post-training optimizations, including Supervised Fine-Tuning (SFT) on high-quality datasets and Reinforcement Learning from Human Feedback (RLHF) with multi-dimensional reward models. Furthermore, we introduce an acceleration framework that boosts inference speed by over 10X. Seedance 1.5 pro distinguishes itself through precise multilingual and dialect lip-syncing, dynamic cinematic camera control, and enhanced narrative coherence, positioning it as a robust engine for professional-grade content creation. Seedance 1.5 pro is now accessible on Volcano Engine at https://console.volcengine.com/ark/region:ark+cn-beijing/experience/vision?type=GenVideo.",
    "arxiv_url": "https://arxiv.org/abs/2512.13507v3",
    "pdf_url": "https://arxiv.org/pdf/2512.13507v3",
    "published_date": "2025-12-15",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "acceleration",
      "diffusion transformer",
      "video generation",
      "architecture",
      "camera control"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.13507v3",
      "pdf": "https://arxiv.org/pdf/2512.13507v3",
      "project": "https://console.volcengine.com/ark/region:ark+cn-beijing/experience/vision?type=GenVideo"
    },
    "bibtex": ""
  },
  {
    "title": "Transform Trained Transformer: Accelerating Naive 4K Video Generation Over 10$\\times$",
    "authors": [
      "Jiangning Zhang",
      "Junwei Zhu",
      "Teng Hu",
      "Yabiao Wang",
      "Donghao Luo",
      "Weijian Cao",
      "Zhenye Gan",
      "Xiaobin Hu",
      "Zhucun Xue",
      "Chengjie Wang"
    ],
    "abstract": "Native 4K (2160$\\times$3840) video generation remains a critical challenge due to the quadratic computational explosion of full-attention as spatiotemporal resolution increases, making it difficult for models to strike a balance between efficiency and quality. This paper proposes a novel Transformer retrofit strategy termed $\\textbf{T3}$ ($\\textbf{T}$ransform $\\textbf{T}$rained $\\textbf{T}$ransformer) that, without altering the core architecture of full-attention pretrained models, significantly reduces compute requirements by optimizing their forward logic. Specifically, $\\textbf{T3-Video}$ introduces a multi-scale weight-sharing window attention mechanism and, via hierarchical blocking together with an axis-preserving full-attention design, can effect an \"attention pattern\" transformation of a pretrained model using only modest compute and data. Results on 4K-VBench show that $\\textbf{T3-Video}$ substantially outperforms existing approaches: while delivering performance improvements (+4.29$\\uparrow$ VQA and +0.08$\\uparrow$ VTC), it accelerates native 4K video generation by more than 10$\\times$. Project page at https://zhangzjn.github.io/projects/T3-Video",
    "arxiv_url": "https://arxiv.org/abs/2512.13492v1",
    "pdf_url": "https://arxiv.org/pdf/2512.13492v1",
    "published_date": "2025-12-15",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "architecture",
      "video generation"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.13492v1",
      "pdf": "https://arxiv.org/pdf/2512.13492v1",
      "project": "https://zhangzjn.github.io/projects/T3-Video"
    },
    "bibtex": ""
  },
  {
    "title": "PoseAnything: Universal Pose-guided Video Generation with Part-aware Temporal Coherence",
    "authors": [
      "Ruiyan Wang",
      "Teng Hu",
      "Kaihui Huang",
      "Zihan Su",
      "Ran Yi",
      "Lizhuang Ma"
    ],
    "abstract": "Pose-guided video generation refers to controlling the motion of subjects in generated video through a sequence of poses. It enables precise control over subject motion and has important applications in animation. However, current pose-guided video generation methods are limited to accepting only human poses as input, thus generalizing poorly to pose of other subjects. To address this issue, we propose PoseAnything, the first universal pose-guided video generation framework capable of handling both human and non-human characters, supporting arbitrary skeletal inputs. To enhance consistency preservation during motion, we introduce Part-aware Temporal Coherence Module, which divides the subject into different parts, establishes part correspondences, and computes cross-attention between corresponding parts across frames to achieve fine-grained part-level consistency. Additionally, we propose Subject and Camera Motion Decoupled CFG, a novel guidance strategy that, for the first time, enables independent camera movement control in pose-guided video generation, by separately injecting subject and camera motion control information into the positive and negative anchors of CFG. Furthermore, we present XPose, a high-quality public dataset containing 50,000 non-human pose-video pairs, along with an automated pipeline for annotation and filtering. Extensive experiments demonstrate that Pose-Anything significantly outperforms state-of-the-art methods in both effectiveness and generalization.",
    "arxiv_url": "https://arxiv.org/abs/2512.13465v1",
    "pdf_url": "https://arxiv.org/pdf/2512.13465v1",
    "published_date": "2025-12-15",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "motion control",
      "pose-guided",
      "video generation",
      "dit"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.13465v1",
      "pdf": "https://arxiv.org/pdf/2512.13465v1"
    },
    "bibtex": ""
  },
  {
    "title": "Beyond the Visible: Disocclusion-Aware Editing via Proxy Dynamic Graphs",
    "authors": [
      "Anran Qi",
      "Changjian Li",
      "Adrien Bousseau",
      "Niloy J. Mitra"
    ],
    "abstract": "We address image-to-video generation with explicit user control over the final frame's disoccluded regions. Current image-to-video pipelines produce plausible motion but struggle to generate predictable, articulated motions while enforcing user-specified content in newly revealed areas. Our key idea is to separate motion specification from appearance synthesis: we introduce a lightweight, user-editable Proxy Dynamic Graph (PDG) that deterministically yet approximately drives part motion, while a frozen diffusion prior is used to synthesize plausible appearance that follows that motion. In our training-free pipeline, the user loosely annotates and reposes a PDG, from which we compute a dense motion flow to leverage diffusion as a motion-guided shader. We then let the user edit appearance in the disoccluded areas of the image, and exploit the visibility information encoded by the PDG to perform a latent-space composite that reconciles motion with user intent in these areas. This design yields controllable articulation and user control over disocclusions without fine-tuning. We demonstrate clear advantages against state-of-the-art alternatives towards images turned into short videos of articulated objects, furniture, vehicles, and deformables. Our method mixes generative control, in the form of loose pose and structure, with predictable controls, in the form of appearance specification in the final frame in the disoccluded regions, unlocking a new image-to-video workflow. Code will be released on acceptance. Project page: https://anranqi.github.io/beyond-visible.github.io/",
    "arxiv_url": "https://arxiv.org/abs/2512.13392v2",
    "pdf_url": "https://arxiv.org/pdf/2512.13392v2",
    "published_date": "2025-12-15",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "controllable",
      "video generation",
      "image-to-video",
      "dit"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.13392v2",
      "pdf": "https://arxiv.org/pdf/2512.13392v2",
      "project": "https://anranqi.github.io/beyond-visible.github.io"
    },
    "bibtex": ""
  },
  {
    "title": "KlingAvatar 2.0 Technical Report",
    "authors": [
      "Kling Team",
      "Jialu Chen",
      "Yikang Ding",
      "Zhixue Fang",
      "Kun Gai",
      "Yuan Gao",
      "Kang He",
      "Jingyun Hua",
      "Boyuan Jiang",
      "Mingming Lao",
      "Xiaohan Li",
      "Hui Liu",
      "Jiwen Liu",
      "Xiaoqiang Liu",
      "Yuan Liu",
      "Shun Lu",
      "Yongsen Mao",
      "Yingchao Shao",
      "Huafeng Shi",
      "Xiaoyu Shi",
      "Peiqin Sun",
      "Songlin Tang",
      "Pengfei Wan",
      "Chao Wang",
      "Xuebo Wang",
      "Haoxian Zhang",
      "Yuanxing Zhang",
      "Yan Zhou"
    ],
    "abstract": "Avatar video generation models have achieved remarkable progress in recent years. However, prior work exhibits limited efficiency in generating long-duration high-resolution videos, suffering from temporal drifting, quality degradation, and weak prompt following as video length increases. To address these challenges, we propose KlingAvatar 2.0, a spatio-temporal cascade framework that performs upscaling in both spatial resolution and temporal dimension. The framework first generates low-resolution blueprint video keyframes that capture global semantics and motion, and then refines them into high-resolution, temporally coherent sub-clips using a first-last frame strategy, while retaining smooth temporal transitions in long-form videos. To enhance cross-modal instruction fusion and alignment in extended videos, we introduce a Co-Reasoning Director composed of three modality-specific large language model (LLM) experts. These experts reason about modality priorities and infer underlying user intent, converting inputs into detailed storylines through multi-turn dialogue. A Negative Director further refines negative prompts to improve instruction alignment. Building on these components, we extend the framework to support ID-specific multi-character control. Extensive experiments demonstrate that our model effectively addresses the challenges of efficient, multimodally aligned long-form high-resolution video generation, delivering enhanced visual clarity, realistic lip-teeth rendering with accurate lip synchronization, strong identity preservation, and coherent multimodal instruction following.",
    "arxiv_url": "https://arxiv.org/abs/2512.13313v1",
    "pdf_url": "https://arxiv.org/pdf/2512.13313v1",
    "published_date": "2025-12-15",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "identity",
      "long-form",
      "video generation",
      "upscaling",
      "efficient",
      "avatar"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.13313v1",
      "pdf": "https://arxiv.org/pdf/2512.13313v1"
    },
    "bibtex": ""
  },
  {
    "title": "LINA: Learning INterventions Adaptively for Physical Alignment and Generalization in Diffusion Models",
    "authors": [
      "Shu Yu",
      "Chaochao Lu"
    ],
    "abstract": "Diffusion models (DMs) have achieved remarkable success in image and video generation. However, they still struggle with (1) physical alignment and (2) out-of-distribution (OOD) instruction following. We argue that these issues stem from the models' failure to learn causal directions and to disentangle causal factors for novel recombination. We introduce the Causal Scene Graph (CSG) and the Physical Alignment Probe (PAP) dataset to enable diagnostic interventions. This analysis yields three key insights. First, DMs struggle with multi-hop reasoning for elements not explicitly determined in the prompt. Second, the prompt embedding contains disentangled representations for texture and physics. Third, visual causal structure is disproportionately established during the initial, computationally limited denoising steps. Based on these findings, we introduce LINA (Learning INterventions Adaptively), a novel framework that learns to predict prompt-specific interventions, which employs (1) targeted guidance in the prompt and visual latent spaces, and (2) a reallocated, causality-aware denoising schedule. Our approach enforces both physical alignment and OOD instruction following in image and video DMs, achieving state-of-the-art performance on challenging causal generation tasks and the Winoground dataset. Our project page is at https://opencausalab.github.io/LINA.",
    "arxiv_url": "https://arxiv.org/abs/2512.13290v1",
    "pdf_url": "https://arxiv.org/pdf/2512.13290v1",
    "published_date": "2025-12-15",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "physical",
      "video generation",
      "diffusion model",
      "denoising",
      "physics"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.13290v1",
      "pdf": "https://arxiv.org/pdf/2512.13290v1",
      "project": "https://opencausalab.github.io/LINA"
    },
    "bibtex": ""
  },
  {
    "title": "Video Reality Test: Can AI-Generated ASMR Videos fool VLMs and Humans?",
    "authors": [
      "Jiaqi Wang",
      "Weijia Wu",
      "Yi Zhan",
      "Rui Zhao",
      "Ming Hu",
      "James Cheng",
      "Wei Liu",
      "Philip Torr",
      "Kevin Qinghong Lin"
    ],
    "abstract": "Recent advances in video generation have produced vivid content that are often indistinguishable from real videos, making AI-generated video detection an emerging societal challenge. Prior AIGC detection benchmarks mostly evaluate video without audio, target broad narrative domains, and focus on classification solely. Yet it remains unclear whether state-of-the-art video generation models can produce immersive, audio-paired videos that reliably deceive humans and VLMs. To this end, we introduce Video Reality Test, an ASMR-sourced video benchmark suite for testing perceptual realism under tight audio-visual coupling, featuring the following dimensions: (i) Immersive ASMR video-audio sources. Built on carefully curated real ASMR videos, the benchmark targets fine-grained action-object interactions with diversity across objects, actions, and backgrounds. (ii) Peer-Review evaluation. An adversarial creator-reviewer protocol where video generation models act as creators aiming to fool reviewers, while VLMs serve as reviewers seeking to identify fakeness. Our experimental findings show: The best creator Veo3.1-Fast even fools most VLMs: the strongest reviewer (Gemini 2.5-Pro) achieves only 56% accuracy (random 50%), far below that of human experts (81.25%). Adding audio improves real-fake discrimination, yet superficial cues such as watermarks can still significantly mislead models. These findings delineate the current boundary of video generation realism and expose limitations of VLMs in perceptual fidelity and audio-visual consistency. Our code is available at https://github.com/video-reality-test/video-reality-test.",
    "arxiv_url": "https://arxiv.org/abs/2512.13281v3",
    "pdf_url": "https://arxiv.org/pdf/2512.13281v3",
    "published_date": "2025-12-15",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/video-reality-test/video-reality-test",
    "keywords": [
      "benchmark",
      "evaluation",
      "video generation"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.13281v3",
      "pdf": "https://arxiv.org/pdf/2512.13281v3",
      "github": "https://github.com/video-reality-test/video-reality-test"
    },
    "bibtex": ""
  },
  {
    "title": "STARCaster: Spatio-Temporal AutoRegressive Video Diffusion for Identity- and View-Aware Talking Portraits",
    "authors": [
      "Foivos Paraperas Papantoniou",
      "Stathis Galanakis",
      "Rolandos Alexandros Potamias",
      "Bernhard Kainz",
      "Stefanos Zafeiriou"
    ],
    "abstract": "This paper presents STARCaster, an identity-aware spatio-temporal video diffusion model that addresses both speech-driven portrait animation and free-viewpoint talking portrait synthesis, given an identity embedding or reference image, within a unified framework. Existing 2D speech-to-video diffusion models depend heavily on reference guidance, leading to limited motion diversity. At the same time, 3D-aware animation typically relies on inversion through pre-trained tri-plane generators, which often leads to imperfect reconstructions and identity drift. We rethink reference- and geometry-based paradigms in two ways. First, we deviate from strict reference conditioning at pre-training by introducing softer identity constraints. Second, we address 3D awareness implicitly within the 2D video domain by leveraging the inherent multi-view nature of video data. STARCaster adopts a compositional approach progressing from ID-aware motion modeling, to audio-visual synchronization via lip reading-based supervision, and finally to novel view animation through temporal-to-spatial adaptation. To overcome the scarcity of 4D audio-visual data, we propose a decoupled learning approach in which view consistency and temporal coherence are trained independently. A self-forcing training scheme enables the model to learn from longer temporal contexts than those generated at inference, mitigating the overly static animations common in existing autoregressive approaches. Comprehensive evaluations demonstrate that STARCaster generalizes effectively across tasks and identities, consistently surpassing prior approaches in different benchmarks.",
    "arxiv_url": "https://arxiv.org/abs/2512.13247v1",
    "pdf_url": "https://arxiv.org/pdf/2512.13247v1",
    "published_date": "2025-12-15",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "video diffusion",
      "3d-aware",
      "dit",
      "speech-driven",
      "identity",
      "evaluation",
      "diffusion model",
      "autoregressive",
      "benchmark",
      "novel view"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.13247v1",
      "pdf": "https://arxiv.org/pdf/2512.13247v1"
    },
    "bibtex": ""
  },
  {
    "title": "Motus: A Unified Latent Action World Model",
    "authors": [
      "Hongzhe Bi",
      "Hengkai Tan",
      "Shenghao Xie",
      "Zeyuan Wang",
      "Shuhe Huang",
      "Haitian Liu",
      "Ruowen Zhao",
      "Yao Feng",
      "Chendong Xiang",
      "Yinze Rong",
      "Hongyan Zhao",
      "Hanyu Liu",
      "Zhizhong Su",
      "Lei Ma",
      "Hang Su",
      "Jun Zhu"
    ],
    "abstract": "While a general embodied agent must function as a unified system, current methods are built on isolated models for understanding, world modeling, and control. This fragmentation prevents unifying multimodal generative capabilities and hinders learning from large-scale, heterogeneous data. In this paper, we propose Motus, a unified latent action world model that leverages existing general pretrained models and rich, sharable motion information. Motus introduces a Mixture-of-Transformer (MoT) architecture to integrate three experts (i.e., understanding, video generation, and action) and adopts a UniDiffuser-style scheduler to enable flexible switching between different modeling modes (i.e., world models, vision-language-action models, inverse dynamics models, video generation models, and video-action joint prediction models). Motus further leverages the optical flow to learn latent actions and adopts a recipe with three-phase training pipeline and six-layer data pyramid, thereby extracting pixel-level \"delta action\" and enabling large-scale action pretraining. Experiments show that Motus achieves superior performance against state-of-the-art methods in both simulation (a +15% improvement over X-VLA and a +45% improvement over Pi0.5) and real-world scenarios(improved by +11~48%), demonstrating unified modeling of all functionalities and priors significantly benefits downstream robotic tasks.",
    "arxiv_url": "https://arxiv.org/abs/2512.13030v2",
    "pdf_url": "https://arxiv.org/pdf/2512.13030v2",
    "published_date": "2025-12-15",
    "categories": [
      "cs.CV",
      "cs.LG",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "world model",
      "video generation",
      "architecture",
      "simulation",
      "dynamics",
      "style"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.13030v2",
      "pdf": "https://arxiv.org/pdf/2512.13030v2"
    },
    "bibtex": ""
  },
  {
    "title": "SneakPeek: Future-Guided Instructional Streaming Video Generation",
    "authors": [
      "Cheeun Hong",
      "German Barquero",
      "Fadime Sener",
      "Markos Georgopoulos",
      "Edgar Schnfeld",
      "Stefan Popov",
      "Yuming Du",
      "Oscar Maas",
      "Albert Pumarola"
    ],
    "abstract": "Instructional video generation is an emerging task that aims to synthesize coherent demonstrations of procedural activities from textual descriptions. Such capability has broad implications for content creation, education, and human-AI interaction, yet existing video diffusion models struggle to maintain temporal consistency and controllability across long sequences of multiple action steps. We introduce a pipeline for future-driven streaming instructional video generation, dubbed SneakPeek, a diffusion-based autoregressive framework designed to generate precise, stepwise instructional videos conditioned on an initial image and structured textual prompts. Our approach introduces three key innovations to enhance consistency and controllability: (1) predictive causal adaptation, where a causal model learns to perform next-frame prediction and anticipate future keyframes; (2) future-guided self-forcing with a dual-region KV caching scheme to address the exposure bias issue at inference time; (3) multi-prompt conditioning, which provides fine-grained and procedural control over multi-step instructions. Together, these components mitigate temporal drift, preserve motion consistency, and enable interactive video generation where future prompt updates dynamically influence ongoing streaming video generation. Experimental results demonstrate that our method produces temporally coherent and semantically faithful instructional videos that accurately follow complex, multi-step task descriptions.",
    "arxiv_url": "https://arxiv.org/abs/2512.13019v1",
    "pdf_url": "https://arxiv.org/pdf/2512.13019v1",
    "published_date": "2025-12-15",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "interactive",
      "education",
      "temporal consistency",
      "video diffusion",
      "streaming",
      "dit",
      "video generation",
      "diffusion model",
      "autoregressive"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.13019v1",
      "pdf": "https://arxiv.org/pdf/2512.13019v1"
    },
    "bibtex": ""
  },
  {
    "title": "What Happens Next? Next Scene Prediction with a Unified Video Model",
    "authors": [
      "Xinjie Li",
      "Zhimin Chen",
      "Rui Zhao",
      "Florian Schiffers",
      "Zhenyu Liao",
      "Vimal Bhat"
    ],
    "abstract": "Recent unified models for joint understanding and generation have significantly advanced visual generation capabilities. However, their focus on conventional tasks like text-to-video generation has left the temporal reasoning potential of unified models largely underexplored. To address this gap, we introduce Next Scene Prediction (NSP), a new task that pushes unified video models toward temporal and causal reasoning. Unlike text-to-video generation, NSP requires predicting plausible futures from preceding context, demanding deeper understanding and reasoning. To tackle this task, we propose a unified framework combining Qwen-VL for comprehension and LTX for synthesis, bridged by a latent query embedding and a connector module. This model is trained in three stages on our newly curated, large-scale NSP dataset: text-to-video pre-training, supervised fine-tuning, and reinforcement learning (via GRPO) with our proposed causal consistency reward. Experiments demonstrate our model achieves state-of-the-art performance on our benchmark, advancing the capability of generalist multimodal systems to anticipate what happens next.",
    "arxiv_url": "https://arxiv.org/abs/2512.13015v1",
    "pdf_url": "https://arxiv.org/pdf/2512.13015v1",
    "published_date": "2025-12-15",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "benchmark",
      "text-to-video",
      "video generation"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.13015v1",
      "pdf": "https://arxiv.org/pdf/2512.13015v1"
    },
    "bibtex": ""
  },
  {
    "title": "GenieDrive: Towards Physics-Aware Driving World Model with 4D Occupancy Guided Video Generation",
    "authors": [
      "Zhenya Yang",
      "Zhe Liu",
      "Yuxiang Lu",
      "Liping Hou",
      "Chenxuan Miao",
      "Siyi Peng",
      "Bailan Feng",
      "Xiang Bai",
      "Hengshuang Zhao"
    ],
    "abstract": "Physics-aware driving world model is essential for drive planning, out-of-distribution data synthesis, and closed-loop evaluation. However, existing methods often rely on a single diffusion model to directly map driving actions to videos, which makes learning difficult and leads to physically inconsistent outputs. To overcome these challenges, we propose GenieDrive, a novel framework designed for physics-aware driving video generation. Our approach starts by generating 4D occupancy, which serves as a physics-informed foundation for subsequent video generation. 4D occupancy contains rich physical information, including high-resolution 3D structures and dynamics. To facilitate effective compression of such high-resolution occupancy, we propose a VAE that encodes occupancy into a latent tri-plane representation, reducing the latent size to only 58% of that used in previous methods. We further introduce Mutual Control Attention (MCA) to accurately model the influence of control on occupancy evolution, and we jointly train the VAE and the subsequent prediction module in an end-to-end manner to maximize forecasting accuracy. Together, these designs yield a 7.2% improvement in forecasting mIoU at an inference speed of 41 FPS, while using only 3.47 M parameters. Additionally, a Normalized Multi-View Attention is introduced in the video generation model to generate multi-view driving videos with guidance from our 4D occupancy, significantly improving video quality with a 20.7% reduction in FVD. Experiments demonstrate that GenieDrive enables highly controllable, multi-view consistent, and physics-aware driving video generation.",
    "arxiv_url": "https://arxiv.org/abs/2512.12751v1",
    "pdf_url": "https://arxiv.org/pdf/2512.12751v1",
    "published_date": "2025-12-14",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "controllable",
      "dit",
      "world model",
      "physical",
      "video generation",
      "evaluation",
      "diffusion model",
      "dynamics",
      "physics-aware",
      "physics"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.12751v1",
      "pdf": "https://arxiv.org/pdf/2512.12751v1"
    },
    "bibtex": ""
  },
  {
    "title": "Vision-Enhanced Large Language Models for High-Resolution Image Synthesis and Multimodal Data Interpretation",
    "authors": [
      "Karthikeya KV"
    ],
    "abstract": "This research introduces a transformative framework for integrating Vision-Enhanced Large Language Models (LLMs) with advanced transformer-based architectures to tackle challenges in high-resolution image synthesis and multimodal data interpretation. The proposed model incorporates a rectified flow mechanism that connects noise and data with linear paths, enabling efficient and high-quality generation. A bidirectional tokenization strategy is employed to seamlessly merge inputs from text, image, and video modalities, fostering a unified understanding across diverse data types. By embedding spatial-temporal features and leveraging a hybrid text-image sequence modeling approach, the framework achieves unparalleled fidelity in synthesized images and coherent multimodal representations. The architecture is optimized with a noise-aware learning algorithm, addressing discrepancies in noisy data distributions and improving generative performance under varying input conditions. Rigorous evaluations on benchmark datasets demonstrate a 25% increase in image resolution clarity and a 20% reduction in computational requirements compared to diffusion-based methods. Furthermore, the model exhibits robust scalability and adaptability, showcasing its potential in applications like autonomous systems, creative content generation, and advanced video analysis. This work underscores the role of vision-centric LLMs in redefining capabilities in computer vision and multimodal artificial intelligence.",
    "arxiv_url": "https://arxiv.org/abs/2512.12595v2",
    "pdf_url": "https://arxiv.org/pdf/2512.12595v2",
    "published_date": "2025-12-14",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "creative",
      "rectified flow",
      "dit",
      "efficient",
      "evaluation",
      "architecture",
      "benchmark"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.12595v2",
      "pdf": "https://arxiv.org/pdf/2512.12595v2"
    },
    "bibtex": ""
  },
  {
    "title": "Animus3D: Text-driven 3D Animation via Motion Score Distillation",
    "authors": [
      "Qi Sun",
      "Can Wang",
      "Jiaxiang Shang",
      "Wensen Feng",
      "Jing Liao"
    ],
    "abstract": "We present Animus3D, a text-driven 3D animation framework that generates motion field given a static 3D asset and text prompt. Previous methods mostly leverage the vanilla Score Distillation Sampling (SDS) objective to distill motion from pretrained text-to-video diffusion, leading to animations with minimal movement or noticeable jitter. To address this, our approach introduces a novel SDS alternative, Motion Score Distillation (MSD). Specifically, we introduce a LoRA-enhanced video diffusion model that defines a static source distribution rather than pure noise as in SDS, while another inversion-based noise estimation technique ensures appearance preservation when guiding motion. To further improve motion fidelity, we incorporate explicit temporal and spatial regularization terms that mitigate geometric distortions across time and space. Additionally, we propose a motion refinement module to upscale the temporal resolution and enhance fine-grained details, overcoming the fixed-resolution constraints of the underlying video model. Extensive experiments demonstrate that Animus3D successfully animates static 3D assets from diverse text prompts, generating significantly more substantial and detailed motion than state-of-the-art baselines while maintaining high visual integrity. Code will be released at https://qiisun.github.io/animus3d_page.",
    "arxiv_url": "https://arxiv.org/abs/2512.12534v1",
    "pdf_url": "https://arxiv.org/pdf/2512.12534v1",
    "published_date": "2025-12-14",
    "categories": [
      "cs.CV",
      "cs.GR",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "video diffusion",
      "distillation",
      "dit",
      "text-to-video",
      "diffusion model"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.12534v1",
      "pdf": "https://arxiv.org/pdf/2512.12534v1",
      "project": "https://qiisun.github.io/animus3d_page"
    },
    "bibtex": ""
  },
  {
    "title": "Generative Spatiotemporal Data Augmentation",
    "authors": [
      "Jinfan Zhou",
      "Lixin Luo",
      "Sungmin Eum",
      "Heesung Kwon",
      "Jeong Joon Park"
    ],
    "abstract": "We explore spatiotemporal data augmentation using video foundation models to diversify both camera viewpoints and scene dynamics. Unlike existing approaches based on simple geometric transforms or appearance perturbations, our method leverages off-the-shelf video diffusion models to generate realistic 3D spatial and temporal variations from a given image dataset. Incorporating these synthesized video clips as supplemental training data yields consistent performance gains in low-data settings, such as UAV-captured imagery where annotations are scarce. Beyond empirical improvements, we provide practical guidelines for (i) choosing an appropriate spatiotemporal generative setup, (ii) transferring annotations to synthetic frames, and (iii) addressing disocclusion - regions newly revealed and unlabeled in generated views. Experiments on COCO subsets and UAV-captured datasets show that, when applied judiciously, spatiotemporal augmentation broadens the data distribution along axes underrepresented by traditional and prior generative methods, offering an effective lever for improving model performance in data-scarce regimes.",
    "arxiv_url": "https://arxiv.org/abs/2512.12508v1",
    "pdf_url": "https://arxiv.org/pdf/2512.12508v1",
    "published_date": "2025-12-14",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "dynamics",
      "diffusion model",
      "video diffusion",
      "dit"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.12508v1",
      "pdf": "https://arxiv.org/pdf/2512.12508v1"
    },
    "bibtex": ""
  },
  {
    "title": "Endless World: Real-Time 3D-Aware Long Video Generation",
    "authors": [
      "Ke Zhang",
      "Yiqun Mei",
      "Jiacong Xu",
      "Vishal M. Patel"
    ],
    "abstract": "Producing long, coherent video sequences with stable 3D structure remains a major challenge, particularly in streaming scenarios. Motivated by this, we introduce Endless World, a real-time framework for infinite, 3D-consistent video generation.To support infinite video generation, we introduce a conditional autoregressive training strategy that aligns newly generated content with existing video frames. This design preserves long-range dependencies while remaining computationally efficient, enabling real-time inference on a single GPU without additional training overhead.Moreover, our Endless World integrates global 3D-aware attention to provide continuous geometric guidance across time. Our 3D injection mechanism enforces physical plausibility and geometric consistency throughout extended sequences, addressing key challenges in long-horizon and dynamic scene synthesis.Extensive experiments demonstrate that Endless World produces long, stable, and visually coherent videos, achieving competitive or superior performance to existing methods in both visual fidelity and spatial consistency. Our project has been available on https://bwgzk-keke.github.io/EndlessWorld/.",
    "arxiv_url": "https://arxiv.org/abs/2512.12430v1",
    "pdf_url": "https://arxiv.org/pdf/2512.12430v1",
    "published_date": "2025-12-13",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d-aware",
      "streaming",
      "dit",
      "physical",
      "video generation",
      "long video",
      "autoregressive",
      "efficient"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.12430v1",
      "pdf": "https://arxiv.org/pdf/2512.12430v1",
      "project": "https://bwgzk-keke.github.io/EndlessWorld"
    },
    "bibtex": ""
  },
  {
    "title": "V-Warper: Appearance-Consistent Video Diffusion Personalization via Value Warping",
    "authors": [
      "Hyunkoo Lee",
      "Wooseok Jang",
      "Jini Yang",
      "Taehwan Kim",
      "Sangoh Kim",
      "Sangwon Jung",
      "Seungryong Kim"
    ],
    "abstract": "Video personalization aims to generate videos that faithfully reflect a user-provided subject while following a text prompt. However, existing approaches often rely on heavy video-based finetuning or large-scale video datasets, which impose substantial computational cost and are difficult to scale. Furthermore, they still struggle to maintain fine-grained appearance consistency across frames. To address these limitations, we introduce V-Warper, a training-free coarse-to-fine personalization framework for transformer-based video diffusion models. The framework enhances fine-grained identity fidelity without requiring any additional video training. (1) A lightweight coarse appearance adaptation stage leverages only a small set of reference images, which are already required for the task. This step encodes global subject identity through image-only LoRA and subject-embedding adaptation. (2) A inference-time fine appearance injection stage refines visual fidelity by computing semantic correspondences from RoPE-free mid-layer query--key features. These correspondences guide the warping of appearance-rich value representations into semantically aligned regions of the generation process, with masking ensuring spatial reliability. V-Warper significantly improves appearance fidelity while preserving prompt alignment and motion dynamics, and it achieves these gains efficiently without large-scale video finetuning.",
    "arxiv_url": "https://arxiv.org/abs/2512.12375v1",
    "pdf_url": "https://arxiv.org/pdf/2512.12375v1",
    "published_date": "2025-12-13",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "video diffusion",
      "dit",
      "identity",
      "diffusion model",
      "dynamics",
      "personalization",
      "efficient"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.12375v1",
      "pdf": "https://arxiv.org/pdf/2512.12375v1"
    },
    "bibtex": ""
  },
  {
    "title": "STAGE: Storyboard-Anchored Generation for Cinematic Multi-shot Narrative",
    "authors": [
      "Peixuan Zhang",
      "Zijian Jia",
      "Kaiqi Liu",
      "Shuchen Weng",
      "Si Li",
      "Boxin Shi"
    ],
    "abstract": "While recent advancements in generative models have achieved remarkable visual fidelity in video synthesis, creating coherent multi-shot narratives remains a significant challenge. To address this, keyframe-based approaches have emerged as a promising alternative to computationally intensive end-to-end methods, offering the advantages of fine-grained control and greater efficiency. However, these methods often fail to maintain cross-shot consistency and capture cinematic language. In this paper, we introduce STAGE, a SToryboard-Anchored GEneration workflow to reformulate the keyframe-based multi-shot video generation task. Instead of using sparse keyframes, we propose STEP2 to predict a structural storyboard composed of start-end frame pairs for each shot. We introduce the multi-shot memory pack to ensure long-range entity consistency, the dual-encoding strategy for intra-shot coherence, and the two-stage training scheme to learn cinematic inter-shot transition. We also contribute the large-scale ConStoryBoard dataset, including high-quality movie clips with fine-grained annotations for story progression, cinematic attributes, and human preferences. Extensive experiments demonstrate that STAGE achieves superior performance in structured narrative control and cross-shot coherence.",
    "arxiv_url": "https://arxiv.org/abs/2512.12372v1",
    "pdf_url": "https://arxiv.org/pdf/2512.12372v1",
    "published_date": "2025-12-13",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "video synthesis",
      "video generation"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.12372v1",
      "pdf": "https://arxiv.org/pdf/2512.12372v1"
    },
    "bibtex": ""
  },
  {
    "title": "CineLOG: A Training Free Approach for Cinematic Long Video Generation",
    "authors": [
      "Zahra Dehghanian",
      "Morteza Abolghasemi",
      "Hamid Beigy",
      "Hamid R. Rabiee"
    ],
    "abstract": "Controllable video synthesis is a central challenge in computer vision, yet current models struggle with fine grained control beyond textual prompts, particularly for cinematic attributes like camera trajectory and genre. Existing datasets often suffer from severe data imbalance, noisy labels, or a significant simulation to real gap. To address this, we introduce CineLOG, a new dataset of 5,000 high quality, balanced, and uncut video clips. Each entry is annotated with a detailed scene description, explicit camera instructions based on a standard cinematic taxonomy, and genre label, ensuring balanced coverage across 17 diverse camera movements and 15 film genres. We also present our novel pipeline designed to create this dataset, which decouples the complex text to video (T2V) generation task into four easier stages with more mature technology. To enable coherent, multi shot sequences, we introduce a novel Trajectory Guided Transition Module that generates smooth spatio-temporal interpolation. Extensive human evaluations show that our pipeline significantly outperforms SOTA end to end T2V models in adhering to specific camera and screenplay instructions, while maintaining professional visual quality. All codes and data are available at https://cine-log.pages.dev.",
    "arxiv_url": "https://arxiv.org/abs/2512.12209v1",
    "pdf_url": "https://arxiv.org/pdf/2512.12209v1",
    "published_date": "2025-12-13",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "controllable",
      "text to video",
      "trajectory",
      "video generation",
      "film",
      "evaluation",
      "video synthesis",
      "simulation",
      "long video",
      "t2v"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.12209v1",
      "pdf": "https://arxiv.org/pdf/2512.12209v1",
      "project": "https://cine-log.pages.dev"
    },
    "bibtex": ""
  },
  {
    "title": "AutoMV: An Automatic Multi-Agent System for Music Video Generation",
    "authors": [
      "Xiaoxuan Tang",
      "Xinping Lei",
      "Chaoran Zhu",
      "Shiyun Chen",
      "Ruibin Yuan",
      "Yizhi Li",
      "Changjae Oh",
      "Ge Zhang",
      "Wenhao Huang",
      "Emmanouil Benetos",
      "Yang Liu",
      "Jiaheng Liu",
      "Yinghao Ma"
    ],
    "abstract": "Music-to-Video (M2V) generation for full-length songs faces significant challenges. Existing methods produce short, disjointed clips, failing to align visuals with musical structure, beats, or lyrics, and lack temporal consistency. We propose AutoMV, a multi-agent system that generates full music videos (MVs) directly from a song. AutoMV first applies music processing tools to extract musical attributes, such as structure, vocal tracks, and time-aligned lyrics, and constructs these features as contextual inputs for following agents. The screenwriter Agent and director Agent then use this information to design short script, define character profiles in a shared external bank, and specify camera instructions. Subsequently, these agents call the image generator for keyframes and different video generators for \"story\" or \"singer\" scenes. A Verifier Agent evaluates their output, enabling multi-agent collaboration to produce a coherent longform MV. To evaluate M2V generation, we further propose a benchmark with four high-level categories (Music Content, Technical, Post-production, Art) and twelve ine-grained criteria. This benchmark was applied to compare commercial products, AutoMV, and human-directed MVs with expert human raters: AutoMV outperforms current baselines significantly across all four categories, narrowing the gap to professional MVs. Finally, we investigate using large multimodal models as automatic MV judges; while promising, they still lag behind human expert, highlighting room for future work.",
    "arxiv_url": "https://arxiv.org/abs/2512.12196v1",
    "pdf_url": "https://arxiv.org/pdf/2512.12196v1",
    "published_date": "2025-12-13",
    "categories": [
      "cs.MM",
      "cs.CV",
      "cs.SD",
      "eess.AS"
    ],
    "github_url": "",
    "keywords": [
      "benchmark",
      "music video",
      "video generation",
      "temporal consistency"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.12196v1",
      "pdf": "https://arxiv.org/pdf/2512.12196v1"
    },
    "bibtex": ""
  },
  {
    "title": "SMRABooth: Subject and Motion Representation Alignment for Customized Video Generation",
    "authors": [
      "Xuancheng Xu",
      "Yaning Li",
      "Sisi You",
      "Bing-Kun Bao"
    ],
    "abstract": "Customized video generation aims to produce videos that faithfully preserve the subject's appearance from reference images while maintaining temporally consistent motion from reference videos. Existing methods struggle to ensure both subject appearance similarity and motion pattern consistency due to the lack of object-level guidance for subject and motion. To address this, we propose SMRABooth, which leverages the self-supervised encoder and optical flow encoder to provide object-level subject and motion representations. These representations are aligned with the model during the LoRA fine-tuning process. Our approach is structured in three core stages: (1) We exploit subject representations via a self-supervised encoder to guide subject alignment, enabling the model to capture overall structure of subject and enhance high-level semantic consistency. (2) We utilize motion representations from an optical flow encoder to capture structurally coherent and object-level motion trajectories independent of appearance. (3) We propose a subject-motion association decoupling strategy that applies sparse LoRAs injection across both locations and timing, effectively reducing interference between subject and motion LoRAs. Extensive experiments show that SMRABooth excels in subject and motion customization, maintaining consistent subject appearance and motion patterns, proving its effectiveness in controllable text-to-video generation.",
    "arxiv_url": "https://arxiv.org/abs/2512.12193v1",
    "pdf_url": "https://arxiv.org/pdf/2512.12193v1",
    "published_date": "2025-12-13",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "customization",
      "text-to-video",
      "video generation",
      "controllable"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.12193v1",
      "pdf": "https://arxiv.org/pdf/2512.12193v1"
    },
    "bibtex": ""
  },
  {
    "title": "SPDMark: Selective Parameter Displacement for Robust Video Watermarking",
    "authors": [
      "Samar Fares",
      "Nurbek Tastan",
      "Karthik Nandakumar"
    ],
    "abstract": "The advent of high-quality video generation models has amplified the need for robust watermarking schemes that can be used to reliably detect and track the provenance of generated videos. Existing video watermarking methods based on both post-hoc and in-generation approaches fail to simultaneously achieve imperceptibility, robustness, and computational efficiency. This work introduces a novel framework for in-generation video watermarking called SPDMark (pronounced `SpeedMark') based on selective parameter displacement of a video diffusion model. Watermarks are embedded into the generated videos by modifying a subset of parameters in the generative model. To make the problem tractable, the displacement is modeled as an additive composition of layer-wise basis shifts, where the final composition is indexed by the watermarking key. For parameter efficiency, this work specifically leverages low-rank adaptation (LoRA) to implement the basis shifts. During the training phase, the basis shifts and the watermark extractor are jointly learned by minimizing a combination of message recovery, perceptual similarity, and temporal consistency losses. To detect and localize temporal modifications in the watermarked videos, we use a cryptographic hashing function to derive frame-specific watermark messages from the given base watermarking key. During watermark extraction, maximum bipartite matching is applied to recover the correct frame order, even from temporally tampered videos. Evaluations on both text-to-video and image-to-video generation models demonstrate the ability of SPDMark to generate imperceptible watermarks that can be recovered with high accuracy and also establish its robustness against a variety of common video modifications.",
    "arxiv_url": "https://arxiv.org/abs/2512.12090v1",
    "pdf_url": "https://arxiv.org/pdf/2512.12090v1",
    "published_date": "2025-12-12",
    "categories": [
      "cs.CV",
      "cs.CR",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "temporal consistency",
      "video diffusion",
      "dit",
      "text-to-video",
      "video generation",
      "image-to-video",
      "evaluation",
      "diffusion model"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.12090v1",
      "pdf": "https://arxiv.org/pdf/2512.12090v1"
    },
    "bibtex": ""
  },
  {
    "title": "BAgger: Backwards Aggregation for Mitigating Drift in Autoregressive Video Diffusion Models",
    "authors": [
      "Ryan Po",
      "Eric Ryan Chan",
      "Changan Chen",
      "Gordon Wetzstein"
    ],
    "abstract": "Autoregressive video models are promising for world modeling via next-frame prediction, but they suffer from exposure bias: a mismatch between training on clean contexts and inference on self-generated frames, causing errors to compound and quality to drift over time. We introduce Backwards Aggregation (BAgger), a self-supervised scheme that constructs corrective trajectories from the model's own rollouts, teaching it to recover from its mistakes. Unlike prior approaches that rely on few-step distillation and distribution-matching losses, which can hurt quality and diversity, BAgger trains with standard score or flow matching objectives, avoiding large teachers and long-chain backpropagation through time. We instantiate BAgger on causal diffusion transformers and evaluate on text-to-video, video extension, and multi-prompt generation, observing more stable long-horizon motion and better visual consistency with reduced drift.",
    "arxiv_url": "https://arxiv.org/abs/2512.12080v1",
    "pdf_url": "https://arxiv.org/pdf/2512.12080v1",
    "published_date": "2025-12-12",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "video diffusion",
      "flow matching",
      "distillation",
      "world model",
      "diffusion transformer",
      "text-to-video",
      "diffusion model",
      "autoregressive"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.12080v1",
      "pdf": "https://arxiv.org/pdf/2512.12080v1"
    },
    "bibtex": ""
  },
  {
    "title": "CreativeVR: Diffusion-Prior-Guided Approach for Structure and Motion Restoration in Generative and Real Videos",
    "authors": [
      "Tejas Panambur",
      "Ishan Rajendrakumar Dave",
      "Chongjian Ge",
      "Ersin Yumer",
      "Xue Bai"
    ],
    "abstract": "Modern text-to-video (T2V) diffusion models can synthesize visually compelling clips, yet they remain brittle at fine-scale structure: even state-of-the-art generators often produce distorted faces and hands, warped backgrounds, and temporally inconsistent motion. Such severe structural artifacts also appear in very low-quality real-world videos. Classical video restoration and super-resolution (VR/VSR) methods, in contrast, are tuned for synthetic degradations such as blur and downsampling and tend to stabilize these artifacts rather than repair them, while diffusion-prior restorers are usually trained on photometric noise and offer little control over the trade-off between perceptual quality and fidelity.   We introduce CreativeVR, a diffusion-prior-guided video restoration framework for AI-generated (AIGC) and real videos with severe structural and temporal artifacts. Our deep-adapter-based method exposes a single precision knob that controls how strongly the model follows the input, smoothly trading off between precise restoration on standard degradations and stronger structure- and motion-corrective behavior on challenging content. Our key novelty is a temporally coherent degradation module used during training, which applies carefully designed transformations that produce realistic structural failures.   To evaluate AIGC-artifact restoration, we propose the AIGC54 benchmark with FIQA, semantic and perceptual metrics, and multi-aspect scoring. CreativeVR achieves state-of-the-art results on videos with severe artifacts and performs competitively on standard video restoration benchmarks, while running at practical throughput (about 13 FPS at 720p on a single 80-GB A100). Project page: https://daveishan.github.io/creativevr-webpage/.",
    "arxiv_url": "https://arxiv.org/abs/2512.12060v1",
    "pdf_url": "https://arxiv.org/pdf/2512.12060v1",
    "published_date": "2025-12-12",
    "categories": [
      "cs.CV",
      "cs.LG",
      "cs.MM"
    ],
    "github_url": "",
    "keywords": [
      "video restoration",
      "creative",
      "super-resolution",
      "text-to-video",
      "diffusion model",
      "t2v",
      "benchmark"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.12060v1",
      "pdf": "https://arxiv.org/pdf/2512.12060v1",
      "project": "https://daveishan.github.io/creativevr-webpage"
    },
    "bibtex": ""
  },
  {
    "title": "V-RGBX: Video Editing with Accurate Controls over Intrinsic Properties",
    "authors": [
      "Ye Fang",
      "Tong Wu",
      "Valentin Deschaintre",
      "Duygu Ceylan",
      "Iliyan Georgiev",
      "Chun-Hao Paul Huang",
      "Yiwei Hu",
      "Xuelin Chen",
      "Tuanfeng Yang Wang"
    ],
    "abstract": "Large-scale video generation models have shown remarkable potential in modeling photorealistic appearance and lighting interactions in real-world scenes. However, a closed-loop framework that jointly understands intrinsic scene properties (e.g., albedo, normal, material, and irradiance), leverages them for video synthesis, and supports editable intrinsic representations remains unexplored. We present V-RGBX, the first end-to-end framework for intrinsic-aware video editing. V-RGBX unifies three key capabilities: (1) video inverse rendering into intrinsic channels, (2) photorealistic video synthesis from these intrinsic representations, and (3) keyframe-based video editing conditioned on intrinsic channels. At the core of V-RGBX is an interleaved conditioning mechanism that enables intuitive, physically grounded video editing through user-selected keyframes, supporting flexible manipulation of any intrinsic modality. Extensive qualitative and quantitative results show that V-RGBX produces temporally consistent, photorealistic videos while propagating keyframe edits across sequences in a physically plausible manner. We demonstrate its effectiveness in diverse applications, including object appearance editing and scene-level relighting, surpassing the performance of prior methods.",
    "arxiv_url": "https://arxiv.org/abs/2512.11799v1",
    "pdf_url": "https://arxiv.org/pdf/2512.11799v1",
    "published_date": "2025-12-12",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "dit",
      "video editing",
      "physical",
      "video generation",
      "video synthesis"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.11799v1",
      "pdf": "https://arxiv.org/pdf/2512.11799v1"
    },
    "bibtex": ""
  },
  {
    "title": "AnchorDream: Repurposing Video Diffusion for Embodiment-Aware Robot Data Synthesis",
    "authors": [
      "Junjie Ye",
      "Rong Xue",
      "Basile Van Hoorick",
      "Pavel Tokmakov",
      "Muhammad Zubair Irshad",
      "Yue Wang",
      "Vitor Guizilini"
    ],
    "abstract": "The collection of large-scale and diverse robot demonstrations remains a major bottleneck for imitation learning, as real-world data acquisition is costly and simulators offer limited diversity and fidelity with pronounced sim-to-real gaps. While generative models present an attractive solution, existing methods often alter only visual appearances without creating new behaviors, or suffer from embodiment inconsistencies that yield implausible motions. To address these limitations, we introduce AnchorDream, an embodiment-aware world model that repurposes pretrained video diffusion models for robot data synthesis. AnchorDream conditions the diffusion process on robot motion renderings, anchoring the embodiment to prevent hallucination while synthesizing objects and environments consistent with the robot's kinematics. Starting from only a handful of human teleoperation demonstrations, our method scales them into large, diverse, high-quality datasets without requiring explicit environment modeling. Experiments show that the generated data leads to consistent improvements in downstream policy learning, with relative gains of 36.4% in simulator benchmarks and nearly double performance in real-world studies. These results suggest that grounding generative world models in robot motion provides a practical path toward scaling imitation learning.",
    "arxiv_url": "https://arxiv.org/abs/2512.11797v1",
    "pdf_url": "https://arxiv.org/pdf/2512.11797v1",
    "published_date": "2025-12-12",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "video diffusion",
      "dit",
      "world model",
      "diffusion model",
      "benchmark"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.11797v1",
      "pdf": "https://arxiv.org/pdf/2512.11797v1"
    },
    "bibtex": ""
  },
  {
    "title": "Structure From Tracking: Distilling Structure-Preserving Motion for Video Generation",
    "authors": [
      "Yang Fei",
      "George Stoica",
      "Jingyuan Liu",
      "Qifeng Chen",
      "Ranjay Krishna",
      "Xiaojuan Wang",
      "Benlin Liu"
    ],
    "abstract": "Reality is a dance between rigid constraints and deformable structures. For video models, that means generating motion that preserves fidelity as well as structure. Despite progress in diffusion models, producing realistic structure-preserving motion remains challenging, especially for articulated and deformable objects such as humans and animals. Scaling training data alone, so far, has failed to resolve physically implausible transitions. Existing approaches rely on conditioning with noisy motion representations, such as optical flow or skeletons extracted using an external imperfect model. To address these challenges, we introduce an algorithm to distill structure-preserving motion priors from an autoregressive video tracking model (SAM2) into a bidirectional video diffusion model (CogVideoX). With our method, we train SAM2VideoX, which contains two innovations: (1) a bidirectional feature fusion module that extracts global structure-preserving motion priors from a recurrent model like SAM2; (2) a Local Gram Flow loss that aligns how local features move together. Experiments on VBench and in human studies show that SAM2VideoX delivers consistent gains (+2.60\\% on VBench, 21-22\\% lower FVD, and 71.4\\% human preference) over prior baselines. Specifically, on VBench, we achieve 95.51\\%, surpassing REPA (92.91\\%) by 2.60\\%, and reduce FVD to 360.57, a 21.20\\% and 22.46\\% improvement over REPA- and LoRA-finetuning, respectively. The project website can be found at https://sam2videox.github.io/ .",
    "arxiv_url": "https://arxiv.org/abs/2512.11792v1",
    "pdf_url": "https://arxiv.org/pdf/2512.11792v1",
    "published_date": "2025-12-12",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "video diffusion",
      "dit",
      "physical",
      "video generation",
      "diffusion model",
      "autoregressive"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.11792v1",
      "pdf": "https://arxiv.org/pdf/2512.11792v1",
      "project": "https://sam2videox.github.io"
    },
    "bibtex": ""
  },
  {
    "title": "FactorPortrait: Controllable Portrait Animation via Disentangled Expression, Pose, and Viewpoint",
    "authors": [
      "Jiapeng Tang",
      "Kai Li",
      "Chengxiang Yin",
      "Liuhao Ge",
      "Fei Jiang",
      "Jiu Xu",
      "Matthias Niener",
      "Christian Hne",
      "Timur Bagautdinov",
      "Egor Zakharov",
      "Peihong Guo"
    ],
    "abstract": "We introduce FactorPortrait, a video diffusion method for controllable portrait animation that enables lifelike synthesis from disentangled control signals of facial expressions, head movement, and camera viewpoints. Given a single portrait image, a driving video, and camera trajectories, our method animates the portrait by transferring facial expressions and head movements from the driving video while simultaneously enabling novel view synthesis from arbitrary viewpoints. We utilize a pre-trained image encoder to extract facial expression latents from the driving video as control signals for animation generation. Such latents implicitly capture nuanced facial expression dynamics with identity and pose information disentangled, and they are efficiently injected into the video diffusion transformer through our proposed expression controller. For camera and head pose control, we employ Plcker ray maps and normal maps rendered from 3D body mesh tracking. To train our model, we curate a large-scale synthetic dataset containing diverse combinations of camera viewpoints, head poses, and facial expression dynamics. Extensive experiments demonstrate that our method outperforms existing approaches in realism, expressiveness, control accuracy, and view consistency.",
    "arxiv_url": "https://arxiv.org/abs/2512.11645v1",
    "pdf_url": "https://arxiv.org/pdf/2512.11645v1",
    "published_date": "2025-12-12",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "controllable",
      "video diffusion",
      "identity",
      "diffusion transformer",
      "dynamics",
      "efficient",
      "novel view"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.11645v1",
      "pdf": "https://arxiv.org/pdf/2512.11645v1"
    },
    "bibtex": ""
  },
  {
    "title": "3DTeethSAM: Taming SAM2 for 3D Teeth Segmentation",
    "authors": [
      "Zhiguo Lu",
      "Jianwen Lou",
      "Mingjun Ma",
      "Hairong Jin",
      "Youyi Zheng",
      "Kun Zhou"
    ],
    "abstract": "3D teeth segmentation, involving the localization of tooth instances and their semantic categorization in 3D dental models, is a critical yet challenging task in digital dentistry due to the complexity of real-world dentition. In this paper, we propose 3DTeethSAM, an adaptation of the Segment Anything Model 2 (SAM2) for 3D teeth segmentation. SAM2 is a pretrained foundation model for image and video segmentation, demonstrating a strong backbone in various downstream scenarios. To adapt SAM2 for 3D teeth data, we render images of 3D teeth models from predefined views, apply SAM2 for 2D segmentation, and reconstruct 3D results using 2D-3D projections. Since SAM2's performance depends on input prompts and its initial outputs often have deficiencies, and given its class-agnostic nature, we introduce three light-weight learnable modules: (1) a prompt embedding generator to derive prompt embeddings from image embeddings for accurate mask decoding, (2) a mask refiner to enhance SAM2's initial segmentation results, and (3) a mask classifier to categorize the generated masks. Additionally, we incorporate Deformable Global Attention Plugins (DGAP) into SAM2's image encoder. The DGAP enhances both the segmentation accuracy and the speed of the training process. Our method has been validated on the 3DTeethSeg benchmark, achieving an IoU of 91.90% on high-resolution 3D teeth meshes, establishing a new state-of-the-art in the field.",
    "arxiv_url": "https://arxiv.org/abs/2512.11557v1",
    "pdf_url": "https://arxiv.org/pdf/2512.11557v1",
    "published_date": "2025-12-12",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "benchmark",
      "dit"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.11557v1",
      "pdf": "https://arxiv.org/pdf/2512.11557v1"
    },
    "bibtex": ""
  },
  {
    "title": "Exploring MLLM-Diffusion Information Transfer with MetaCanvas",
    "authors": [
      "Han Lin",
      "Xichen Pan",
      "Ziqi Huang",
      "Ji Hou",
      "Jialiang Wang",
      "Weifeng Chen",
      "Zecheng He",
      "Felix Juefei-Xu",
      "Junzhe Sun",
      "Zhipeng Fan",
      "Ali Thabet",
      "Mohit Bansal",
      "Chu Wang"
    ],
    "abstract": "Multimodal learning has rapidly advanced visual understanding, largely via multimodal large language models (MLLMs) that use powerful LLMs as cognitive cores. In visual generation, however, these powerful core models are typically reduced to global text encoders for diffusion models, leaving most of their reasoning and planning ability unused. This creates a gap: current multimodal LLMs can parse complex layouts, attributes, and knowledge-intensive scenes, yet struggle to generate images or videos with equally precise and structured control. We propose MetaCanvas, a lightweight framework that lets MLLMs reason and plan directly in spatial and spatiotemporal latent spaces and interface tightly with diffusion generators. We empirically implement MetaCanvas on three different diffusion backbones and evaluate it across six tasks, including text-to-image generation, text/image-to-video generation, image/video editing, and in-context video generation, each requiring precise layouts, robust attribute binding, and reasoning-intensive control. MetaCanvas consistently outperforms global-conditioning baselines, suggesting that treating MLLMs as latent-space planners is a promising direction for narrowing the gap between multimodal understanding and generation.",
    "arxiv_url": "https://arxiv.org/abs/2512.11464v1",
    "pdf_url": "https://arxiv.org/pdf/2512.11464v1",
    "published_date": "2025-12-12",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "layout",
      "dit",
      "video editing",
      "video generation",
      "image-to-video",
      "diffusion model"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.11464v1",
      "pdf": "https://arxiv.org/pdf/2512.11464v1"
    },
    "bibtex": ""
  },
  {
    "title": "Flowception: Temporally Expansive Flow Matching for Video Generation",
    "authors": [
      "Tariq Berrada Ifriqi",
      "John Nguyen",
      "Karteek Alahari",
      "Jakob Verbeek",
      "Ricky T. Q. Chen"
    ],
    "abstract": "We present Flowception, a novel non-autoregressive and variable-length video generation framework. Flowception learns a probability path that interleaves discrete frame insertions with continuous frame denoising. Compared to autoregressive methods, Flowception alleviates error accumulation/drift as the frame insertion mechanism during sampling serves as an efficient compression mechanism to handle long-term context. Compared to full-sequence flows, our method reduces FLOPs for training three-fold, while also being more amenable to local attention variants, and allowing to learn the length of videos jointly with their content. Quantitative experimental results show improved FVD and VBench metrics over autoregressive and full-sequence baselines, which is further validated with qualitative results. Finally, by learning to insert and denoise frames in a sequence, Flowception seamlessly integrates different tasks such as image-to-video generation and video interpolation.",
    "arxiv_url": "https://arxiv.org/abs/2512.11438v1",
    "pdf_url": "https://arxiv.org/pdf/2512.11438v1",
    "published_date": "2025-12-12",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "flow matching",
      "video generation",
      "image-to-video",
      "video interpolation",
      "denoising",
      "autoregressive",
      "efficient"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.11438v1",
      "pdf": "https://arxiv.org/pdf/2512.11438v1"
    },
    "bibtex": ""
  },
  {
    "title": "JoyAvatar-Flash: Real-time and Infinite Audio-Driven Avatar Generation with Autoregressive Diffusion",
    "authors": [
      "Chaochao Li",
      "Ruikui Wang",
      "Liangbo Zhou",
      "Jinheng Feng",
      "Huaishao Luo",
      "Huan Zhang",
      "Youzheng Wu",
      "Xiaodong He"
    ],
    "abstract": "Existing DiT-based audio-driven avatar generation methods have achieved considerable progress, yet their broader application is constrained by limitations such as high computational overhead and the inability to synthesize long-duration videos. Autoregressive methods address this problem by applying block-wise autoregressive diffusion methods. However, these methods suffer from the problem of error accumulation and quality degradation. To address this, we propose JoyAvatar-Flash, an audio-driven autoregressive model capable of real-time inference and infinite-length video generation with the following contributions: (1) Progressive Step Bootstrapping (PSB), which allocates more denoising steps to initial frames to stabilize generation and reduce error accumulation; (2) Motion Condition Injection (MCI), enhancing temporal coherence by injecting noise-corrupted previous frames as motion condition; and (3) Unbounded RoPE via Cache-Resetting (URCR), enabling infinite-length generation through dynamic positional encoding. Our 1.3B-parameter causal model achieves 16 FPS on a single GPU and achieves competitive results in visual quality, temporal consistency, and lip synchronization.",
    "arxiv_url": "https://arxiv.org/abs/2512.11423v2",
    "pdf_url": "https://arxiv.org/pdf/2512.11423v2",
    "published_date": "2025-12-12",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "temporal consistency",
      "dit",
      "video generation",
      "denoising",
      "autoregressive",
      "avatar",
      "audio-driven"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.11423v2",
      "pdf": "https://arxiv.org/pdf/2512.11423v2"
    },
    "bibtex": ""
  },
  {
    "title": "Autoregressive Video Autoencoder with Decoupled Temporal and Spatial Context",
    "authors": [
      "Cuifeng Shen",
      "Lumin Xu",
      "Xingguo Zhu",
      "Gengdai Liu"
    ],
    "abstract": "Video autoencoders compress videos into compact latent representations for efficient reconstruction, playing a vital role in enhancing the quality and efficiency of video generation. However, existing video autoencoders often entangle spatial and temporal information, limiting their ability to capture temporal consistency and leading to suboptimal performance. To address this, we propose Autoregressive Video Autoencoder (ARVAE), which compresses and reconstructs each frame conditioned on its predecessor in an autoregressive manner, allowing flexible processing of videos with arbitrary lengths. ARVAE introduces a temporal-spatial decoupled representation that combines downsampled flow field for temporal coherence with spatial relative compensation for newly emerged content, achieving high compression efficiency without information loss. Specifically, the encoder compresses the current and previous frames into the temporal motion and spatial supplement, while the decoder reconstructs the original frame from the latent representations given the preceding frame. A multi-stage training strategy is employed to progressively optimize the model. Extensive experiments demonstrate that ARVAE achieves superior reconstruction quality with extremely lightweight models and small-scale training data. Moreover, evaluations on video generation tasks highlight its strong potential for downstream applications.",
    "arxiv_url": "https://arxiv.org/abs/2512.11293v1",
    "pdf_url": "https://arxiv.org/pdf/2512.11293v1",
    "published_date": "2025-12-12",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "temporal consistency",
      "dit",
      "video generation",
      "evaluation",
      "autoregressive",
      "efficient"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.11293v1",
      "pdf": "https://arxiv.org/pdf/2512.11293v1"
    },
    "bibtex": ""
  },
  {
    "title": "FilmWeaver: Weaving Consistent Multi-Shot Videos with Cache-Guided Autoregressive Diffusion",
    "authors": [
      "Xiangyang Luo",
      "Qingyu Li",
      "Xiaokun Liu",
      "Wenyu Qin",
      "Miao Yang",
      "Meng Wang",
      "Pengfei Wan",
      "Di Zhang",
      "Kun Gai",
      "Shao-Lun Huang"
    ],
    "abstract": "Current video generation models perform well at single-shot synthesis but struggle with multi-shot videos, facing critical challenges in maintaining character and background consistency across shots and flexibly generating videos of arbitrary length and shot count. To address these limitations, we introduce \\textbf{FilmWeaver}, a novel framework designed to generate consistent, multi-shot videos of arbitrary length. First, it employs an autoregressive diffusion paradigm to achieve arbitrary-length video generation. To address the challenge of consistency, our key insight is to decouple the problem into inter-shot consistency and intra-shot coherence. We achieve this through a dual-level cache mechanism: a shot memory caches keyframes from preceding shots to maintain character and scene identity, while a temporal memory retains a history of frames from the current shot to ensure smooth, continuous motion. The proposed framework allows for flexible, multi-round user interaction to create multi-shot videos. Furthermore, due to this decoupled design, our method demonstrates high versatility by supporting downstream tasks such as multi-concept injection and video extension. To facilitate the training of our consistency-aware method, we also developed a comprehensive pipeline to construct a high-quality multi-shot video dataset. Extensive experimental results demonstrate that our method surpasses existing approaches on metrics for both consistency and aesthetic quality, opening up new possibilities for creating more consistent, controllable, and narrative-driven video content. Project Page: https://filmweaver.github.io",
    "arxiv_url": "https://arxiv.org/abs/2512.11274v1",
    "pdf_url": "https://arxiv.org/pdf/2512.11274v1",
    "published_date": "2025-12-12",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "controllable",
      "identity",
      "video generation",
      "film",
      "concept",
      "autoregressive"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.11274v1",
      "pdf": "https://arxiv.org/pdf/2512.11274v1",
      "project": "https://filmweaver.github.io"
    },
    "bibtex": ""
  },
  {
    "title": "PersonaLive! Expressive Portrait Image Animation for Live Streaming",
    "authors": [
      "Zhiyuan Li",
      "Chi-Man Pun",
      "Chen Fang",
      "Jue Wang",
      "Xiaodong Cun"
    ],
    "abstract": "Current diffusion-based portrait animation models predominantly focus on enhancing visual quality and expression realism, while overlooking generation latency and real-time performance, which restricts their application range in the live streaming scenario. We propose PersonaLive, a novel diffusion-based framework towards streaming real-time portrait animation with multi-stage training recipes. Specifically, we first adopt hybrid implicit signals, namely implicit facial representations and 3D implicit keypoints, to achieve expressive image-level motion control. Then, a fewer-step appearance distillation strategy is proposed to eliminate appearance redundancy in the denoising process, greatly improving inference efficiency. Finally, we introduce an autoregressive micro-chunk streaming generation paradigm equipped with a sliding training strategy and a historical keyframe mechanism to enable low-latency and stable long-term video generation. Extensive experiments demonstrate that PersonaLive achieves state-of-the-art performance with up to 7-22x speedup over prior diffusion-based portrait animation models.",
    "arxiv_url": "https://arxiv.org/abs/2512.11253v1",
    "pdf_url": "https://arxiv.org/pdf/2512.11253v1",
    "published_date": "2025-12-12",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "distillation",
      "streaming",
      "video generation",
      "denoising",
      "autoregressive",
      "motion control",
      "image animation"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.11253v1",
      "pdf": "https://arxiv.org/pdf/2512.11253v1"
    },
    "bibtex": ""
  },
  {
    "title": "VFMF: World Modeling by Forecasting Vision Foundation Model Features",
    "authors": [
      "Gabrijel Boduljak",
      "Yushi Lan",
      "Christian Rupprecht",
      "Andrea Vedaldi"
    ],
    "abstract": "Forecasting from partial observations is central to world modeling. Many recent methods represent the world through images, and reduce forecasting to stochastic video generation. Although such methods excel at realism and visual fidelity, predicting pixels is computationally intensive and not directly useful in many applications, as it requires translating RGB into signals useful for decision making. An alternative approach uses features from vision foundation models (VFMs) as world representations, performing deterministic regression to predict future world states. These features can be directly translated into actionable signals such as semantic segmentation and depth, while remaining computationally efficient. However, deterministic regression averages over multiple plausible futures, undermining forecast accuracy by failing to capture uncertainty. To address this crucial limitation, we introduce a generative forecaster that performs autoregressive flow matching in VFM feature space. Our key insight is that generative modeling in this space requires encoding VFM features into a compact latent space suitable for diffusion. We show that this latent space preserves information more effectively than previously used PCA-based alternatives, both for forecasting and other applications, such as image generation. Our latent predictions can be easily decoded into multiple useful and interpretable output modalities: semantic segmentation, depth, surface normals, and even RGB. With matched architecture and compute, our method produces sharper and more accurate predictions than regression across all modalities. Our results suggest that stochastic conditional generation of VFM features offers a promising and scalable foundation for future world models.",
    "arxiv_url": "https://arxiv.org/abs/2512.11225v1",
    "pdf_url": "https://arxiv.org/pdf/2512.11225v1",
    "published_date": "2025-12-12",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "flow matching",
      "dit",
      "world model",
      "video generation",
      "architecture",
      "autoregressive",
      "efficient"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.11225v1",
      "pdf": "https://arxiv.org/pdf/2512.11225v1"
    },
    "bibtex": ""
  },
  {
    "title": "AutoRefiner: Improving Autoregressive Video Diffusion Models via Reflective Refinement Over the Stochastic Sampling Path",
    "authors": [
      "Zhengyang Yu",
      "Akio Hayakawa",
      "Masato Ishii",
      "Qingtao Yu",
      "Takashi Shibuya",
      "Jing Zhang",
      "Yuki Mitsufuji"
    ],
    "abstract": "Autoregressive video diffusion models (AR-VDMs) show strong promise as scalable alternatives to bidirectional VDMs, enabling real-time and interactive applications. Yet there remains room for improvement in their sample fidelity. A promising solution is inference-time alignment, which optimizes the noise space to improve sample fidelity without updating model parameters. Yet, optimization- or search-based methods are computationally impractical for AR-VDMs. Recent text-to-image (T2I) works address this via feedforward noise refiners that modulate sampled noises in a single forward pass. Can such noise refiners be extended to AR-VDMs? We identify the failure of naively extending T2I noise refiners to AR-VDMs and propose AutoRefiner-a noise refiner tailored for AR-VDMs, with two key designs: pathwise noise refinement and a reflective KV-cache. Experiments demonstrate that AutoRefiner serves as an efficient plug-in for AR-VDMs, effectively enhancing sample fidelity by refining noise along stochastic denoising paths.",
    "arxiv_url": "https://arxiv.org/abs/2512.11203v2",
    "pdf_url": "https://arxiv.org/pdf/2512.11203v2",
    "published_date": "2025-12-12",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "interactive",
      "video diffusion",
      "diffusion model",
      "denoising",
      "autoregressive",
      "efficient"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.11203v2",
      "pdf": "https://arxiv.org/pdf/2512.11203v2"
    },
    "bibtex": ""
  },
  {
    "title": "E-RayZer: Self-supervised 3D Reconstruction as Spatial Visual Pre-training",
    "authors": [
      "Qitao Zhao",
      "Hao Tan",
      "Qianqian Wang",
      "Sai Bi",
      "Kai Zhang",
      "Kalyan Sunkavalli",
      "Shubham Tulsiani",
      "Hanwen Jiang"
    ],
    "abstract": "Self-supervised pre-training has revolutionized foundation models for languages, individual 2D images and videos, but remains largely unexplored for learning 3D-aware representations from multi-view images. In this paper, we present E-RayZer, a self-supervised large 3D Vision model that learns truly 3D-aware representations directly from unlabeled images. Unlike prior self-supervised methods such as RayZer that infer 3D indirectly through latent-space view synthesis, E-RayZer operates directly in 3D space, performing self-supervised 3D reconstruction with Explicit geometry. This formulation eliminates shortcut solutions and yields representations that are geometrically grounded. To ensure convergence and scalability, we introduce a novel fine-grained learning curriculum that organizes training from easy to hard samples and harmonizes heterogeneous data sources in an entirely unsupervised manner. Experiments demonstrate that E-RayZer significantly outperforms RayZer on pose estimation, matches or sometimes surpasses fully supervised reconstruction models such as VGGT. Furthermore, its learned representations outperform leading visual pre-training models (e.g., DINOv3, CroCo v2, VideoMAE V2, and RayZer) when transferring to 3D downstream tasks, establishing E-RayZer as a new paradigm for 3D-aware visual pre-training.",
    "arxiv_url": "https://arxiv.org/abs/2512.10950v1",
    "pdf_url": "https://arxiv.org/pdf/2512.10950v1",
    "published_date": "2025-12-11",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d-aware"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.10950v1",
      "pdf": "https://arxiv.org/pdf/2512.10950v1"
    },
    "bibtex": ""
  },
  {
    "title": "AlcheMinT: Fine-grained Temporal Control for Multi-Reference Consistent Video Generation",
    "authors": [
      "Sharath Girish",
      "Viacheslav Ivanov",
      "Tsai-Shien Chen",
      "Hao Chen",
      "Aliaksandr Siarohin",
      "Sergey Tulyakov"
    ],
    "abstract": "Recent advances in subject-driven video generation with large diffusion models have enabled personalized content synthesis conditioned on user-provided subjects. However, existing methods lack fine-grained temporal control over subject appearance and disappearance, which are essential for applications such as compositional video synthesis, storyboarding, and controllable animation. We propose AlcheMinT, a unified framework that introduces explicit timestamps conditioning for subject-driven video generation. Our approach introduces a novel positional encoding mechanism that unlocks the encoding of temporal intervals, associated in our case with subject identities, while seamlessly integrating with the pretrained video generation model positional embeddings. Additionally, we incorporate subject-descriptive text tokens to strengthen binding between visual identity and video captions, mitigating ambiguity during generation. Through token-wise concatenation, AlcheMinT avoids any additional cross-attention modules and incurs negligible parameter overhead. We establish a benchmark evaluating multiple subject identity preservation, video fidelity, and temporal adherence. Experimental results demonstrate that AlcheMinT achieves visual quality matching state-of-the-art video personalization methods, while, for the first time, enabling precise temporal control over multi-subject generation within videos. Project page is at https://snap-research.github.io/Video-AlcheMinT",
    "arxiv_url": "https://arxiv.org/abs/2512.10943v1",
    "pdf_url": "https://arxiv.org/pdf/2512.10943v1",
    "published_date": "2025-12-11",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "controllable",
      "dit",
      "identity",
      "video generation",
      "subject-driven",
      "video synthesis",
      "diffusion model",
      "personalization",
      "benchmark"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.10943v1",
      "pdf": "https://arxiv.org/pdf/2512.10943v1",
      "project": "https://snap-research.github.io/Video-AlcheMinT"
    },
    "bibtex": ""
  },
  {
    "title": "VL-JEPA: Joint Embedding Predictive Architecture for Vision-language",
    "authors": [
      "Delong Chen",
      "Mustafa Shukor",
      "Theo Moutakanni",
      "Willy Chung",
      "Jade Yu",
      "Tejaswi Kasarla",
      "Yejin Bang",
      "Allen Bolourchi",
      "Yann LeCun",
      "Pascale Fung"
    ],
    "abstract": "We introduce VL-JEPA, a vision-language model built on a Joint Embedding Predictive Architecture (JEPA). Instead of autoregressively generating tokens as in classical VLMs, VL-JEPA predicts continuous embeddings of the target texts. By learning in an abstract representation space, the model focuses on task-relevant semantics while abstracting away surface-level linguistic variability. In a strictly controlled comparison against standard token-space VLM training with the same vision encoder and training data, VL-JEPA achieves stronger performance while having 50% fewer trainable parameters. At inference time, a lightweight text decoder is invoked only when needed to translate VL-JEPA predicted embeddings into text. We show that VL-JEPA natively supports selective decoding that reduces the number of decoding operations by 2.85x while maintaining similar performance compared to non-adaptive uniform decoding. Beyond generation, the VL-JEPA's embedding space naturally supports open-vocabulary classification, text-to-video retrieval, and discriminative VQA without any architecture modification. On eight video classification and eight video retrieval datasets, the average performance VL-JEPA surpasses that of CLIP, SigLIP2, and Perception Encoder. At the same time, the model achieves comparable performance as classical VLMs (InstructBLIP, QwenVL) on four VQA datasets: GQA, TallyQA, POPE and POPEv2, despite only having 1.6B parameters.",
    "arxiv_url": "https://arxiv.org/abs/2512.10942v2",
    "pdf_url": "https://arxiv.org/pdf/2512.10942v2",
    "published_date": "2025-12-11",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "architecture",
      "text-to-video",
      "autoregressive"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.10942v2",
      "pdf": "https://arxiv.org/pdf/2512.10942v2"
    },
    "bibtex": ""
  },
  {
    "title": "OmniView: An All-Seeing Diffusion Model for 3D and 4D View Synthesis",
    "authors": [
      "Xiang Fan",
      "Sharath Girish",
      "Vivek Ramanujan",
      "Chaoyang Wang",
      "Ashkan Mirzaei",
      "Petr Sushko",
      "Aliaksandr Siarohin",
      "Sergey Tulyakov",
      "Ranjay Krishna"
    ],
    "abstract": "Prior approaches injecting camera control into diffusion models have focused on specific subsets of 4D consistency tasks: novel view synthesis, text-to-video with camera control, image-to-video, amongst others. Therefore, these fragmented approaches are trained on disjoint slices of available 3D/4D data. We introduce OmniView, a unified framework that generalizes across a wide range of 4D consistency tasks. Our method separately represents space, time, and view conditions, enabling flexible combinations of these inputs. For example, OmniView can synthesize novel views from static, dynamic, and multiview inputs, extrapolate trajectories forward and backward in time, and create videos from text or image prompts with full camera control. OmniView is competitive with task-specific models across diverse benchmarks and metrics, improving image quality scores among camera-conditioned diffusion models by up to 33\\% in multiview NVS LLFF dataset, 60\\% in dynamic NVS Neural 3D Video benchmark, 20\\% in static camera control on RE-10K, and reducing camera trajectory errors by 4x in text-conditioned video generation. With strong generalizability in one model, OmniView demonstrates the feasibility of a generalist 4D video model. Project page is available at https://snap-research.github.io/OmniView/",
    "arxiv_url": "https://arxiv.org/abs/2512.10940v2",
    "pdf_url": "https://arxiv.org/pdf/2512.10940v2",
    "published_date": "2025-12-11",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "3d video",
      "dit",
      "text-to-video",
      "video generation",
      "image-to-video",
      "trajectory",
      "diffusion model",
      "benchmark",
      "novel view",
      "camera control"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.10940v2",
      "pdf": "https://arxiv.org/pdf/2512.10940v2",
      "project": "https://snap-research.github.io/OmniView"
    },
    "bibtex": ""
  },
  {
    "title": "Lang2Motion: Bridging Language and Motion through Joint Embedding Spaces",
    "authors": [
      "Bishoy Galoaa",
      "Xiangyu Bai",
      "Sarah Ostadabbas"
    ],
    "abstract": "We present Lang2Motion, a framework for language-guided point trajectory generation by aligning motion manifolds with joint embedding spaces. Unlike prior work focusing on human motion or video synthesis, we generate explicit trajectories for arbitrary objects using motion extracted from real-world videos via point tracking. Our transformer-based auto-encoder learns trajectory representations through dual supervision: textual motion descriptions and rendered trajectory visualizations, both mapped through CLIP's frozen encoders. Lang2Motion achieves 34.2% Recall@1 on text-to-trajectory retrieval, outperforming video-based methods by 12.5 points, and improves motion accuracy by 33-52% (12.4 ADE vs 18.3-25.3) compared to video generation baselines. We demonstrate 88.3% Top-1 accuracy on human action recognition despite training only on diverse object motions, showing effective transfer across motion domains. Lang2Motion supports style transfer, semantic interpolation, and latent-space editing through CLIP-aligned trajectory representations.",
    "arxiv_url": "https://arxiv.org/abs/2512.10617v1",
    "pdf_url": "https://arxiv.org/pdf/2512.10617v1",
    "published_date": "2025-12-11",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "dit",
      "trajectory",
      "video generation",
      "video synthesis",
      "human motion",
      "style"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.10617v1",
      "pdf": "https://arxiv.org/pdf/2512.10617v1"
    },
    "bibtex": ""
  },
  {
    "title": "Audio-sync Video Instance Editing with Granularity-Aware Mask Refiner",
    "authors": [
      "Haojie Zheng",
      "Shuchen Weng",
      "Jingqi Liu",
      "Siqi Yang",
      "Boxin Shi",
      "Xinlong Wang"
    ],
    "abstract": "Recent advancements in video generation highlight that realistic audio-visual synchronization is crucial for engaging content creation. However, existing video editing methods largely overlook audio-visual synchronization and lack the fine-grained spatial and temporal controllability required for precise instance-level edits. In this paper, we propose AVI-Edit, a framework for audio-sync video instance editing. We propose a granularity-aware mask refiner that iteratively refines coarse user-provided masks into precise instance-level regions. We further design a self-feedback audio agent to curate high-quality audio guidance, providing fine-grained temporal control. To facilitate this task, we additionally construct a large-scale dataset with instance-centric correspondence and comprehensive annotations. Extensive experiments demonstrate that AVI-Edit outperforms state-of-the-art methods in visual quality, condition following, and audio-visual synchronization. Project page: https://hjzheng.net/projects/AVI-Edit/.",
    "arxiv_url": "https://arxiv.org/abs/2512.10571v1",
    "pdf_url": "https://arxiv.org/pdf/2512.10571v1",
    "published_date": "2025-12-11",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "video editing",
      "video generation",
      "dit"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.10571v1",
      "pdf": "https://arxiv.org/pdf/2512.10571v1",
      "project": "https://hjzheng.net/projects/AVI-Edit"
    },
    "bibtex": ""
  },
  {
    "title": "ShotDirector: Directorially Controllable Multi-Shot Video Generation with Cinematographic Transitions",
    "authors": [
      "Xiaoxue Wu",
      "Xinyuan Chen",
      "Yaohui Wang",
      "Yu Qiao"
    ],
    "abstract": "Shot transitions play a pivotal role in multi-shot video generation, as they determine the overall narrative expression and the directorial design of visual storytelling. However, recent progress has primarily focused on low-level visual consistency across shots, neglecting how transitions are designed and how cinematographic language contributes to coherent narrative expression. This often leads to mere sequential shot changes without intentional film-editing patterns. To address this limitation, we propose ShotDirector, an efficient framework that integrates parameter-level camera control and hierarchical editing-pattern-aware prompting. Specifically, we adopt a camera control module that incorporates 6-DoF poses and intrinsic settings to enable precise camera information injection. In addition, a shot-aware mask mechanism is employed to introduce hierarchical prompts aware of professional editing patterns, allowing fine-grained control over shot content. Through this design, our framework effectively combines parameter-level conditions with high-level semantic guidance, achieving film-like controllable shot transitions. To facilitate training and evaluation, we construct ShotWeaver40K, a dataset that captures the priors of film-like editing patterns, and develop a set of evaluation metrics for controllable multi-shot video generation. Extensive experiments demonstrate the effectiveness of our framework.",
    "arxiv_url": "https://arxiv.org/abs/2512.10286v1",
    "pdf_url": "https://arxiv.org/pdf/2512.10286v1",
    "published_date": "2025-12-11",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "controllable",
      "dit",
      "video generation",
      "film",
      "evaluation",
      "efficient",
      "camera control"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.10286v1",
      "pdf": "https://arxiv.org/pdf/2512.10286v1"
    },
    "bibtex": ""
  },
  {
    "title": "MotionEdit: Benchmarking and Learning Motion-Centric Image Editing",
    "authors": [
      "Yixin Wan",
      "Lei Ke",
      "Wenhao Yu",
      "Kai-Wei Chang",
      "Dong Yu"
    ],
    "abstract": "We introduce MotionEdit, a novel dataset for motion-centric image editing-the task of modifying subject actions and interactions while preserving identity, structure, and physical plausibility. Unlike existing image editing datasets that focus on static appearance changes or contain only sparse, low-quality motion edits, MotionEdit provides high-fidelity image pairs depicting realistic motion transformations extracted and verified from continuous videos. This new task is not only scientifically challenging but also practically significant, powering downstream applications such as frame-controlled video synthesis and animation.   To evaluate model performance on the novel task, we introduce MotionEdit-Bench, a benchmark that challenges models on motion-centric edits and measures model performance with generative, discriminative, and preference-based metrics. Benchmark results reveal that motion editing remains highly challenging for existing state-of-the-art diffusion-based editing models. To address this gap, we propose MotionNFT (Motion-guided Negative-aware Fine Tuning), a post-training framework that computes motion alignment rewards based on how well the motion flow between input and model-edited images matches the ground-truth motion, guiding models toward accurate motion transformations. Extensive experiments on FLUX.1 Kontext and Qwen-Image-Edit show that MotionNFT consistently improves editing quality and motion fidelity of both base models on the motion editing task without sacrificing general editing ability, demonstrating its effectiveness. Our code is at https://github.com/elainew728/motion-edit/.",
    "arxiv_url": "https://arxiv.org/abs/2512.10284v2",
    "pdf_url": "https://arxiv.org/pdf/2512.10284v2",
    "published_date": "2025-12-11",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "github_url": "https://github.com/elainew728/motion-edit/",
    "keywords": [
      "dit",
      "identity",
      "physical",
      "video synthesis",
      "benchmark"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.10284v2",
      "pdf": "https://arxiv.org/pdf/2512.10284v2",
      "github": "https://github.com/elainew728/motion-edit"
    },
    "bibtex": ""
  },
  {
    "title": "ReViSE: Towards Reason-Informed Video Editing in Unified Models with Self-Reflective Learning",
    "authors": [
      "Xinyu Liu",
      "Hangjie Yuan",
      "Yujie Wei",
      "Jiazheng Xing",
      "Yujin Han",
      "Jiahao Pan",
      "Yanbiao Ma",
      "Chi-Min Chan",
      "Kang Zhao",
      "Shiwei Zhang",
      "Wenhan Luo",
      "Yike Guo"
    ],
    "abstract": "Video unified models exhibit strong capabilities in understanding and generation, yet they struggle with reason-informed visual editing even when equipped with powerful internal vision-language models (VLMs). We attribute this gap to two factors: 1) existing datasets are inadequate for training and evaluating reasoning-aware video editing, and 2) an inherent disconnect between the models' reasoning and editing capabilities, which prevents the rich understanding from effectively instructing the editing process. Bridging this gap requires an integrated framework that connects reasoning with visual transformation. To address this gap, we introduce the Reason-Informed Video Editing (RVE) task, which requires reasoning about physical plausibility and causal dynamics during editing. To support systematic evaluation, we construct RVE-Bench, a comprehensive benchmark with two complementary subsets: Reasoning-Informed Video Editing and In-Context Video Generation. These subsets cover diverse reasoning dimensions and real-world editing scenarios. Building upon this foundation, we propose the ReViSE, a Self-Reflective Reasoning (SRF) framework that unifies generation and evaluation within a single architecture. The model's internal VLM provides intrinsic feedback by assessing whether the edited video logically satisfies the given instruction. The differential feedback that refines the generator's reasoning behavior during training. Extensive experiments on RVE-Bench demonstrate that ReViSE significantly enhances editing accuracy and visual fidelity, achieving a 32% improvement of the Overall score in the reasoning-informed video editing subset over state-of-the-art methods.",
    "arxiv_url": "https://arxiv.org/abs/2512.09924v2",
    "pdf_url": "https://arxiv.org/pdf/2512.09924v2",
    "published_date": "2025-12-10",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "dit",
      "video editing",
      "physical",
      "video generation",
      "evaluation",
      "architecture",
      "dynamics",
      "benchmark"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.09924v2",
      "pdf": "https://arxiv.org/pdf/2512.09924v2"
    },
    "bibtex": ""
  },
  {
    "title": "UniUGP: Unifying Understanding, Generation, and Planing For End-to-end Autonomous Driving",
    "authors": [
      "Hao Lu",
      "Ziyang Liu",
      "Guangfeng Jiang",
      "Yuanfei Luo",
      "Sheng Chen",
      "Yangang Zhang",
      "Ying-Cong Chen"
    ],
    "abstract": "Autonomous driving (AD) systems struggle in long-tail scenarios due to limited world knowledge and weak visual dynamic modeling. Existing vision-language-action (VLA)-based methods cannot leverage unlabeled videos for visual causal learning, while world model-based methods lack reasoning capabilities from large language models. In this paper, we construct multiple specialized datasets providing reasoning and planning annotations for complex scenarios. Then, a unified Understanding-Generation-Planning framework, named UniUGP, is proposed to synergize scene reasoning, future video generation, and trajectory planning through a hybrid expert architecture. By integrating pre-trained VLMs and video generation models, UniUGP leverages visual dynamics and semantic reasoning to enhance planning performance. Taking multi-frame observations and language instructions as input, it produces interpretable chain-of-thought reasoning, physically consistent trajectories, and coherent future videos. We introduce a four-stage training strategy that progressively builds these capabilities across multiple existing AD datasets, along with the proposed specialized datasets. Experiments demonstrate state-of-the-art performance in perception, reasoning, and decision-making, with superior generalization to challenging long-tail situations.",
    "arxiv_url": "https://arxiv.org/abs/2512.09864v1",
    "pdf_url": "https://arxiv.org/pdf/2512.09864v1",
    "published_date": "2025-12-10",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "world model",
      "physical",
      "trajectory",
      "video generation",
      "autonomous driving",
      "architecture",
      "dynamics"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.09864v1",
      "pdf": "https://arxiv.org/pdf/2512.09864v1"
    },
    "bibtex": ""
  },
  {
    "title": "Composing Concepts from Images and Videos via Concept-prompt Binding",
    "authors": [
      "Xianghao Kong",
      "Zeyu Zhang",
      "Yuwei Guo",
      "Zhuoran Zhao",
      "Songchun Zhang",
      "Anyi Rao"
    ],
    "abstract": "Visual concept composition, which aims to integrate different elements from images and videos into a single, coherent visual output, still falls short in accurately extracting complex concepts from visual inputs and flexibly combining concepts from both images and videos. We introduce Bind & Compose, a one-shot method that enables flexible visual concept composition by binding visual concepts with corresponding prompt tokens and composing the target prompt with bound tokens from various sources. It adopts a hierarchical binder structure for cross-attention conditioning in Diffusion Transformers to encode visual concepts into corresponding prompt tokens for accurate decomposition of complex visual concepts. To improve concept-token binding accuracy, we design a Diversify-and-Absorb Mechanism that uses an extra absorbent token to eliminate the impact of concept-irrelevant details when training with diversified prompts. To enhance the compatibility between image and video concepts, we present a Temporal Disentanglement Strategy that decouples the training process of video concepts into two stages with a dual-branch binder structure for temporal modeling. Evaluations demonstrate that our method achieves superior concept consistency, prompt fidelity, and motion quality over existing approaches, opening up new possibilities for visual creativity.",
    "arxiv_url": "https://arxiv.org/abs/2512.09824v1",
    "pdf_url": "https://arxiv.org/pdf/2512.09824v1",
    "published_date": "2025-12-10",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.MM"
    ],
    "github_url": "",
    "keywords": [
      "evaluation",
      "diffusion transformer",
      "concept",
      "dit"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.09824v1",
      "pdf": "https://arxiv.org/pdf/2512.09824v1"
    },
    "bibtex": ""
  },
  {
    "title": "VHOI: Controllable Video Generation of Human-Object Interactions from Sparse Trajectories via Motion Densification",
    "authors": [
      "Wanyue Zhang",
      "Lin Geng Foo",
      "Thabo Beeler",
      "Rishabh Dabral",
      "Christian Theobalt"
    ],
    "abstract": "Synthesizing realistic human-object interactions (HOI) in video is challenging due to the complex, instance-specific interaction dynamics of both humans and objects. Incorporating controllability in video generation further adds to the complexity. Existing controllable video generation approaches face a trade-off: sparse controls like keypoint trajectories are easy to specify but lack instance-awareness, while dense signals such as optical flow, depths or 3D meshes are informative but costly to obtain. We propose VHOI, a two-stage framework that first densifies sparse trajectories into HOI mask sequences, and then fine-tunes a video diffusion model conditioned on these dense masks. We introduce a novel HOI-aware motion representation that uses color encodings to distinguish not only human and object motion, but also body-part-specific dynamics. This design incorporates a human prior into the conditioning signal and strengthens the model's ability to understand and generate realistic HOI dynamics. Experiments demonstrate state-of-the-art results in controllable HOI video generation. VHOI is not limited to interaction-only scenarios and can also generate full human navigation leading up to object interactions in an end-to-end manner. Project page: https://vcai.mpi-inf.mpg.de/projects/vhoi/.",
    "arxiv_url": "https://arxiv.org/abs/2512.09646v1",
    "pdf_url": "https://arxiv.org/pdf/2512.09646v1",
    "published_date": "2025-12-10",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "controllable",
      "video diffusion",
      "dit",
      "video generation",
      "diffusion model",
      "dynamics"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.09646v1",
      "pdf": "https://arxiv.org/pdf/2512.09646v1",
      "project": "https://vcai.mpi-inf.mpg.de/projects/vhoi"
    },
    "bibtex": ""
  },
  {
    "title": "Label-free Motion-Conditioned Diffusion Model for Cardiac Ultrasound Synthesis",
    "authors": [
      "Zhe Li",
      "Hadrien Reynaud",
      "Johanna P Mller",
      "Bernhard Kainz"
    ],
    "abstract": "Ultrasound echocardiography is essential for the non-invasive, real-time assessment of cardiac function, but the scarcity of labelled data, driven by privacy restrictions and the complexity of expert annotation, remains a major obstacle for deep learning methods. We propose the Motion Conditioned Diffusion Model (MCDM), a label-free latent diffusion framework that synthesises realistic echocardiography videos conditioned on self-supervised motion features. To extract these features, we design the Motion and Appearance Feature Extractor (MAFE), which disentangles motion and appearance representations from videos. Feature learning is further enhanced by two auxiliary objectives: a re-identification loss guided by pseudo appearance features and an optical flow loss guided by pseudo flow fields. Evaluated on the EchoNet-Dynamic dataset, MCDM achieves competitive video generation performance, producing temporally coherent and clinically realistic sequences without reliance on manual labels. These results demonstrate the potential of self-supervised conditioning for scalable echocardiography synthesis. Our code is available at https://github.com/ZheLi2020/LabelfreeMCDM.",
    "arxiv_url": "https://arxiv.org/abs/2512.09418v1",
    "pdf_url": "https://arxiv.org/pdf/2512.09418v1",
    "published_date": "2025-12-10",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/ZheLi2020/LabelfreeMCDM",
    "keywords": [
      "sound",
      "diffusion model",
      "video generation",
      "dit"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.09418v1",
      "pdf": "https://arxiv.org/pdf/2512.09418v1",
      "github": "https://github.com/ZheLi2020/LabelfreeMCDM"
    },
    "bibtex": ""
  },
  {
    "title": "DirectSwap: Mask-Free Cross-Identity Training and Benchmarking for Expression-Consistent Video Head Swapping",
    "authors": [
      "Yanan Wang",
      "Shengcai Liao",
      "Panwen Hu",
      "Xin Li",
      "Fan Yang",
      "Xiaodan Liang"
    ],
    "abstract": "Video head swapping aims to replace the entire head of a video subject, including facial identity, head shape, and hairstyle, with that of a reference image, while preserving the target body, background, and motion dynamics. Due to the lack of ground-truth paired swapping data, prior methods typically train on cross-frame pairs of the same person within a video and rely on mask-based inpainting to mitigate identity leakage. Beyond potential boundary artifacts, this paradigm struggles to recover essential cues occluded by the mask, such as facial pose, expressions, and motion dynamics. To address these issues, we prompt a video editing model to synthesize new heads for existing videos as fake swapping inputs, while maintaining frame-synchronized facial poses and expressions. This yields HeadSwapBench, the first cross-identity paired dataset for video head swapping, which supports both training (\\TrainNum{} videos) and benchmarking (\\TestNum{} videos) with genuine outputs. Leveraging this paired supervision, we propose DirectSwap, a mask-free, direct video head-swapping framework that extends an image U-Net into a video diffusion model with a motion module and conditioning inputs. Furthermore, we introduce the Motion- and Expression-Aware Reconstruction (MEAR) loss, which reweights the diffusion loss per pixel using frame-difference magnitudes and facial-landmark proximity, thereby enhancing cross-frame coherence in motion and expressions. Extensive experiments demonstrate that DirectSwap achieves state-of-the-art visual quality, identity fidelity, and motion and expression consistency across diverse in-the-wild video scenes. We will release the source code and the HeadSwapBench dataset to facilitate future research.",
    "arxiv_url": "https://arxiv.org/abs/2512.09417v1",
    "pdf_url": "https://arxiv.org/pdf/2512.09417v1",
    "published_date": "2025-12-10",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "video diffusion",
      "dit",
      "video editing",
      "identity",
      "style",
      "diffusion model",
      "dynamics",
      "benchmark"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.09417v1",
      "pdf": "https://arxiv.org/pdf/2512.09417v1"
    },
    "bibtex": ""
  },
  {
    "title": "H2R-Grounder: A Paired-Data-Free Paradigm for Translating Human Interaction Videos into Physically Grounded Robot Videos",
    "authors": [
      "Hai Ci",
      "Xiaokang Liu",
      "Pei Yang",
      "Yiren Song",
      "Mike Zheng Shou"
    ],
    "abstract": "Robots that learn manipulation skills from everyday human videos could acquire broad capabilities without tedious robot data collection. We propose a video-to-video translation framework that converts ordinary human-object interaction videos into motion-consistent robot manipulation videos with realistic, physically grounded interactions. Our approach does not require any paired human-robot videos for training only a set of unpaired robot videos, making the system easy to scale. We introduce a transferable representation that bridges the embodiment gap: by inpainting the robot arm in training videos to obtain a clean background and overlaying a simple visual cue (a marker and arrow indicating the gripper's position and orientation), we can condition a generative model to insert the robot arm back into the scene. At test time, we apply the same process to human videos (inpainting the person and overlaying human pose cues) and generate high-quality robot videos that mimic the human's actions. We fine-tune a SOTA video diffusion model (Wan 2.2) in an in-context learning manner to ensure temporal coherence and leveraging of its rich prior knowledge. Empirical results demonstrate that our approach achieves significantly more realistic and grounded robot motions compared to baselines, pointing to a promising direction for scaling up robot learning from unlabeled human videos. Project page: https://showlab.github.io/H2R-Grounder/",
    "arxiv_url": "https://arxiv.org/abs/2512.09406v1",
    "pdf_url": "https://arxiv.org/pdf/2512.09406v1",
    "published_date": "2025-12-10",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "video diffusion",
      "dit",
      "physical",
      "video-to-video",
      "diffusion model",
      "video translation"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.09406v1",
      "pdf": "https://arxiv.org/pdf/2512.09406v1",
      "project": "https://showlab.github.io/H2R-Grounder"
    },
    "bibtex": ""
  },
  {
    "title": "StereoWorld: Geometry-Aware Monocular-to-Stereo Video Generation",
    "authors": [
      "Ke Xing",
      "Xiaojie Jin",
      "Longfei Li",
      "Yuyang Yin",
      "Hanwen Liang",
      "Guixun Luo",
      "Chen Fang",
      "Jue Wang",
      "Konstantinos N. Plataniotis",
      "Yao Zhao",
      "Yunchao Wei"
    ],
    "abstract": "The growing adoption of XR devices has fueled strong demand for high-quality stereo video, yet its production remains costly and artifact-prone. To address this challenge, we present StereoWorld, an end-to-end framework that repurposes a pretrained video generator for high-fidelity monocular-to-stereo video generation. Our framework jointly conditions the model on the monocular video input while explicitly supervising the generation with a geometry-aware regularization to ensure 3D structural fidelity. A spatio-temporal tiling scheme is further integrated to enable efficient, high-resolution synthesis. To enable large-scale training and evaluation, we curate a high-definition stereo video dataset containing over 11M frames aligned to natural human interpupillary distance (IPD). Extensive experiments demonstrate that StereoWorld substantially outperforms prior methods, generating stereo videos with superior visual fidelity and geometric consistency. The project webpage is available at https://ke-xing.github.io/StereoWorld/.",
    "arxiv_url": "https://arxiv.org/abs/2512.09363v2",
    "pdf_url": "https://arxiv.org/pdf/2512.09363v2",
    "published_date": "2025-12-10",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "evaluation",
      "video generation",
      "efficient",
      "dit"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.09363v2",
      "pdf": "https://arxiv.org/pdf/2512.09363v2",
      "project": "https://ke-xing.github.io/StereoWorld"
    },
    "bibtex": ""
  },
  {
    "title": "VABench: A Comprehensive Benchmark for Audio-Video Generation",
    "authors": [
      "Daili Hua",
      "Xizhi Wang",
      "Bohan Zeng",
      "Xinyi Huang",
      "Hao Liang",
      "Junbo Niu",
      "Xinlong Chen",
      "Quanqing Xu",
      "Wentao Zhang"
    ],
    "abstract": "Recent advances in video generation have been remarkable, enabling models to produce visually compelling videos with synchronized audio. While existing video generation benchmarks provide comprehensive metrics for visual quality, they lack convincing evaluations for audio-video generation, especially for models aiming to generate synchronized audio-video outputs. To address this gap, we introduce VABench, a comprehensive and multi-dimensional benchmark framework designed to systematically evaluate the capabilities of synchronous audio-video generation. VABench encompasses three primary task types: text-to-audio-video (T2AV), image-to-audio-video (I2AV), and stereo audio-video generation. It further establishes two major evaluation modules covering 15 dimensions. These dimensions specifically assess pairwise similarities (text-video, text-audio, video-audio), audio-video synchronization, lip-speech consistency, and carefully curated audio and video question-answering (QA) pairs, among others. Furthermore, VABench covers seven major content categories: animals, human sounds, music, environmental sounds, synchronous physical sounds, complex scenes, and virtual worlds. We provide a systematic analysis and visualization of the evaluation results, aiming to establish a new standard for assessing video generation models with synchronous audio capabilities and to promote the comprehensive advancement of the field.",
    "arxiv_url": "https://arxiv.org/abs/2512.09299v1",
    "pdf_url": "https://arxiv.org/pdf/2512.09299v1",
    "published_date": "2025-12-10",
    "categories": [
      "cs.CV",
      "cs.SD"
    ],
    "github_url": "",
    "keywords": [
      "physical",
      "video generation",
      "evaluation",
      "sound",
      "benchmark"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.09299v1",
      "pdf": "https://arxiv.org/pdf/2512.09299v1"
    },
    "bibtex": ""
  },
  {
    "title": "GimbalDiffusion: Gravity-Aware Camera Control for Video Generation",
    "authors": [
      "Frdric Fortier-Chouinard",
      "Yannick Hold-Geoffroy",
      "Valentin Deschaintre",
      "Matheus Gadelha",
      "Jean-Franois Lalonde"
    ],
    "abstract": "Recent progress in text-to-video generation has achieved remarkable realism, yet fine-grained control over camera motion and orientation remains elusive. Existing approaches typically encode camera trajectories through relative or ambiguous representations, limiting explicit geometric control. We introduce GimbalDiffusion, a framework that enables camera control grounded in physical-world coordinates, using gravity as a global reference. Instead of describing motion relative to previous frames, our method defines camera trajectories in an absolute coordinate system, allowing precise and interpretable control over camera parameters without requiring an initial reference frame. We leverage panoramic 360-degree videos to construct a wide variety of camera trajectories, well beyond the predominantly straight, forward-facing trajectories seen in conventional video data. To further enhance camera guidance, we introduce null-pitch conditioning, an annotation strategy that reduces the model's reliance on text content when conflicting with camera specifications (e.g., generating grass while the camera points towards the sky). Finally, we establish a benchmark for camera-aware video generation by rebalancing SpatialVID-HQ for comprehensive evaluation under wide camera pitch variation. Together, these contributions advance the controllability and robustness of text-to-video models, enabling precise, gravity-aligned camera manipulation within generative frameworks.",
    "arxiv_url": "https://arxiv.org/abs/2512.09112v1",
    "pdf_url": "https://arxiv.org/pdf/2512.09112v1",
    "published_date": "2025-12-09",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "dit",
      "physical",
      "text-to-video",
      "video generation",
      "evaluation",
      "benchmark",
      "camera control"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.09112v1",
      "pdf": "https://arxiv.org/pdf/2512.09112v1"
    },
    "bibtex": ""
  },
  {
    "title": "Astra: General Interactive World Model with Autoregressive Denoising",
    "authors": [
      "Yixuan Zhu",
      "Jiaqi Feng",
      "Wenzhao Zheng",
      "Yuan Gao",
      "Xin Tao",
      "Pengfei Wan",
      "Jie Zhou",
      "Jiwen Lu"
    ],
    "abstract": "Recent advances in diffusion transformers have empowered video generation models to generate high-quality video clips from texts or images. However, world models with the ability to predict long-horizon futures from past observations and actions remain underexplored, especially for general-purpose scenarios and various forms of actions. To bridge this gap, we introduce Astra, an interactive general world model that generates real-world futures for diverse scenarios (e.g., autonomous driving, robot grasping) with precise action interactions (e.g., camera motion, robot action). We propose an autoregressive denoising architecture and use temporal causal attention to aggregate past observations and support streaming outputs. We use a noise-augmented history memory to avoid over-reliance on past frames to balance responsiveness with temporal coherence. For precise action control, we introduce an action-aware adapter that directly injects action signals into the denoising process. We further develop a mixture of action experts that dynamically route heterogeneous action modalities, enhancing versatility across diverse real-world tasks such as exploration, manipulation, and camera control. Astra achieves interactive, consistent, and general long-term video prediction and supports various forms of interactions. Experiments across multiple datasets demonstrate the improvements of Astra in fidelity, long-range prediction, and action alignment over existing state-of-the-art world models.",
    "arxiv_url": "https://arxiv.org/abs/2512.08931v3",
    "pdf_url": "https://arxiv.org/pdf/2512.08931v3",
    "published_date": "2025-12-09",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "interactive",
      "video prediction",
      "streaming",
      "world model",
      "diffusion transformer",
      "video generation",
      "autonomous driving",
      "architecture",
      "denoising",
      "autoregressive",
      "camera control"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.08931v3",
      "pdf": "https://arxiv.org/pdf/2512.08931v3"
    },
    "bibtex": ""
  },
  {
    "title": "Self-Evolving 3D Scene Generation from a Single Image",
    "authors": [
      "Kaizhi Zheng",
      "Yue Fan",
      "Jing Gu",
      "Zishuo Xu",
      "Xuehai He",
      "Xin Eric Wang"
    ],
    "abstract": "Generating high-quality, textured 3D scenes from a single image remains a fundamental challenge in vision and graphics. Recent image-to-3D generators recover reasonable geometry from single views, but their object-centric training limits generalization to complex, large-scale scenes with faithful structure and texture. We present EvoScene, a self-evolving, training-free framework that progressively reconstructs complete 3D scenes from single images. The key idea is combining the complementary strengths of existing models: geometric reasoning from 3D generation models and visual knowledge from video generation models. Through three iterative stages--Spatial Prior Initialization, Visual-guided 3D Scene Mesh Generation, and Spatial-guided Novel View Generation--EvoScene alternates between 2D and 3D domains, gradually improving both structure and appearance. Experiments on diverse scenes demonstrate that EvoScene achieves superior geometric stability, view-consistent textures, and unseen-region completion compared to strong baselines, producing ready-to-use 3D meshes for practical applications.",
    "arxiv_url": "https://arxiv.org/abs/2512.08905v1",
    "pdf_url": "https://arxiv.org/pdf/2512.08905v1",
    "published_date": "2025-12-09",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "novel view",
      "video generation"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.08905v1",
      "pdf": "https://arxiv.org/pdf/2512.08905v1"
    },
    "bibtex": ""
  },
  {
    "title": "Wan-Move: Motion-controllable Video Generation via Latent Trajectory Guidance",
    "authors": [
      "Ruihang Chu",
      "Yefei He",
      "Zhekai Chen",
      "Shiwei Zhang",
      "Xiaogang Xu",
      "Bin Xia",
      "Dingdong Wang",
      "Hongwei Yi",
      "Xihui Liu",
      "Hengshuang Zhao",
      "Yu Liu",
      "Yingya Zhang",
      "Yujiu Yang"
    ],
    "abstract": "We present Wan-Move, a simple and scalable framework that brings motion control to video generative models. Existing motion-controllable methods typically suffer from coarse control granularity and limited scalability, leaving their outputs insufficient for practical use. We narrow this gap by achieving precise and high-quality motion control. Our core idea is to directly make the original condition features motion-aware for guiding video synthesis. To this end, we first represent object motions with dense point trajectories, allowing fine-grained control over the scene. We then project these trajectories into latent space and propagate the first frame's features along each trajectory, producing an aligned spatiotemporal feature map that tells how each scene element should move. This feature map serves as the updated latent condition, which is naturally integrated into the off-the-shelf image-to-video model, e.g., Wan-I2V-14B, as motion guidance without any architecture change. It removes the need for auxiliary motion encoders and makes fine-tuning base models easily scalable. Through scaled training, Wan-Move generates 5-second, 480p videos whose motion controllability rivals Kling 1.5 Pro's commercial Motion Brush, as indicated by user studies. To support comprehensive evaluation, we further design MoveBench, a rigorously curated benchmark featuring diverse content categories and hybrid-verified annotations. It is distinguished by larger data volume, longer video durations, and high-quality motion annotations. Extensive experiments on MoveBench and the public dataset consistently show Wan-Move's superior motion quality. Code, models, and benchmark data are made publicly available.",
    "arxiv_url": "https://arxiv.org/abs/2512.08765v1",
    "pdf_url": "https://arxiv.org/pdf/2512.08765v1",
    "published_date": "2025-12-09",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "controllable",
      "motion control",
      "dit",
      "trajectory",
      "video generation",
      "image-to-video",
      "i2v",
      "evaluation",
      "video synthesis",
      "architecture",
      "benchmark"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.08765v1",
      "pdf": "https://arxiv.org/pdf/2512.08765v1"
    },
    "bibtex": ""
  },
  {
    "title": "Towards Lossless Ultimate Vision Token Compression for VLMs",
    "authors": [
      "Dehua Zheng",
      "Mouxiao Huang",
      "Borui Jiang",
      "Hailin Hu",
      "Xinghao Chen"
    ],
    "abstract": "Visual language models encounter challenges in computational efficiency and latency, primarily due to the substantial redundancy in the token representations of high-resolution images and videos. Current attention/similarity-based compression algorithms suffer from either position bias or class imbalance, leading to significant accuracy degradation. They also fail to generalize to shallow LLM layers, which exhibit weaker cross-modal interactions. To address this, we extend token compression to the visual encoder through an effective iterative merging scheme that is orthogonal in spatial axes to accelerate the computation across the entire VLM. Furthermoer, we integrate a spectrum pruning unit into LLM through an attention/similarity-free low-pass filter, which gradually prunes redundant visual tokens and is fully compatible to modern FlashAttention. On this basis, we propose Lossless Ultimate Vision tokens Compression (LUVC) framework. LUVC systematically compresses visual tokens until complete elimination at the final layer of LLM, so that the high-dimensional visual features are gradually fused into the multimodal queries. The experiments show that LUVC achieves a 2 speedup inference in language model with negligible accuracy degradation, and the training-free characteristic enables immediate deployment across multiple VLMs.",
    "arxiv_url": "https://arxiv.org/abs/2512.09010v1",
    "pdf_url": "https://arxiv.org/pdf/2512.09010v1",
    "published_date": "2025-12-09",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.09010v1",
      "pdf": "https://arxiv.org/pdf/2512.09010v1"
    },
    "bibtex": ""
  },
  {
    "title": "Disturbance-Free Surgical Video Generation from Multi-Camera Shadowless Lamps for Open Surgery",
    "authors": [
      "Yuna Kato",
      "Shohei Mori",
      "Hideo Saito",
      "Yoshifumi Takatsume",
      "Hiroki Kajita",
      "Mariko Isogawa"
    ],
    "abstract": "Video recordings of open surgeries are greatly required for education and research purposes. However, capturing unobstructed videos is challenging since surgeons frequently block the camera field of view. To avoid occlusion, the positions and angles of the camera must be frequently adjusted, which is highly labor-intensive. Prior work has addressed this issue by installing multiple cameras on a shadowless lamp and arranging them to fully surround the surgical area. This setup increases the chances of some cameras capturing an unobstructed view. However, manual image alignment is needed in post-processing since camera configurations change every time surgeons move the lamp for optimal lighting. This paper aims to fully automate this alignment task. The proposed method identifies frames in which the lighting system moves, realigns them, and selects the camera with the least occlusion to generate a video that consistently presents the surgical field from a fixed perspective. A user study involving surgeons demonstrated that videos generated by our method were superior to those produced by conventional methods in terms of the ease of confirming the surgical area and the comfort during video viewing. Additionally, our approach showed improvements in video quality over existing techniques. Furthermore, we implemented several synthesis options for the proposed view-synthesis method and conducted a user study to assess surgeons' preferences for each option.",
    "arxiv_url": "https://arxiv.org/abs/2512.08577v1",
    "pdf_url": "https://arxiv.org/pdf/2512.08577v1",
    "published_date": "2025-12-09",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "education",
      "video generation",
      "dit"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.08577v1",
      "pdf": "https://arxiv.org/pdf/2512.08577v1"
    },
    "bibtex": ""
  },
  {
    "title": "GeoDiffMM: Geometry-Guided Conditional Diffusion for Motion Magnification",
    "authors": [
      "Xuedeng Liu",
      "Jiabao Guo",
      "Zheng Zhang",
      "Fei Wang",
      "Zhi Liu",
      "Dan Guo"
    ],
    "abstract": "Video Motion Magnification (VMM) amplifies subtle macroscopic motions to a perceptible level. Recently, existing mainstream Eulerian approaches address amplification-induced noise via decoupling representation learning such as texture, shape and frequancey schemes, but they still struggle to separate photon noise from true micro-motion when motion displacements are very small. We propose GeoDiffMM, a novel diffusion-based Lagrangian VMM framework conditioned on optical flow as a geometric cue, enabling structurally consistent motion magnification. Specifically, we design a Noise-free Optical Flow Augmentation strategy that synthesizes diverse nonrigid motion fields without photon noise as supervision, helping the model learn more accurate geometry-aware optial flow and generalize better. Next, we develop a Diffusion Motion Magnifier that conditions the denoising process on (i) optical flow as a geometry prior and (ii) a learnable magnification factor controlling magnitude, thereby selectively amplifying motion components consistent with scene semantics and structure while suppressing content-irrelevant perturbations. Finally, we perform Flow-based Video Synthesis to map the amplified motion back to the image domain with high fidelity. Extensive experiments on real and synthetic datasets show that GeoDiffMM outperforms state-of-the-art methods and significantly improves motion magnification.",
    "arxiv_url": "https://arxiv.org/abs/2512.08325v1",
    "pdf_url": "https://arxiv.org/pdf/2512.08325v1",
    "published_date": "2025-12-09",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "video synthesis",
      "denoising",
      "dit"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.08325v1",
      "pdf": "https://arxiv.org/pdf/2512.08325v1"
    },
    "bibtex": ""
  },
  {
    "title": "EgoX: Egocentric Video Generation from a Single Exocentric Video",
    "authors": [
      "Taewoong Kang",
      "Kinam Kim",
      "Dohyeon Kim",
      "Minho Park",
      "Junha Hyung",
      "Jaegul Choo"
    ],
    "abstract": "Egocentric perception enables humans to experience and understand the world directly from their own point of view. Translating exocentric (third-person) videos into egocentric (first-person) videos opens up new possibilities for immersive understanding but remains highly challenging due to extreme camera pose variations and minimal view overlap. This task requires faithfully preserving visible content while synthesizing unseen regions in a geometrically consistent manner. To achieve this, we present EgoX, a novel framework for generating egocentric videos from a single exocentric input. EgoX leverages the pretrained spatio temporal knowledge of large-scale video diffusion models through lightweight LoRA adaptation and introduces a unified conditioning strategy that combines exocentric and egocentric priors via width and channel wise concatenation. Additionally, a geometry-guided self-attention mechanism selectively attends to spatially relevant regions, ensuring geometric coherence and high visual fidelity. Our approach achieves coherent and realistic egocentric video generation while demonstrating strong scalability and robustness across unseen and in-the-wild videos.",
    "arxiv_url": "https://arxiv.org/abs/2512.08269v1",
    "pdf_url": "https://arxiv.org/pdf/2512.08269v1",
    "published_date": "2025-12-09",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "diffusion model",
      "video generation",
      "video diffusion",
      "dit"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.08269v1",
      "pdf": "https://arxiv.org/pdf/2512.08269v1"
    },
    "bibtex": ""
  },
  {
    "title": "Embodied Tree of Thoughts: Deliberate Manipulation Planning with Embodied World Model",
    "authors": [
      "Wenjiang Xu",
      "Cindy Wang",
      "Rui Fang",
      "Mingkang Zhang",
      "Lusong Li",
      "Jing Xu",
      "Jiayuan Gu",
      "Zecui Zeng",
      "Rui Chen"
    ],
    "abstract": "World models have emerged as a pivotal component in robot manipulation planning, enabling agents to predict future environmental states and reason about the consequences of actions before execution. While video-generation models are increasingly adopted, they often lack rigorous physical grounding, leading to hallucinations and a failure to maintain consistency in long-horizon physical constraints. To address these limitations, we propose Embodied Tree of Thoughts (EToT), a novel Real2Sim2Real planning framework that leverages a physics-based interactive digital twin as an embodied world model. EToT formulates manipulation planning as a tree search expanded through two synergistic mechanisms: (1) Priori Branching, which generates diverse candidate execution paths based on semantic and spatial analysis; and (2) Reflective Branching, which utilizes VLMs to diagnose execution failures within the simulator and iteratively refine the planning tree with corrective actions. By grounding high-level reasoning in a physics simulator, our framework ensures that generated plans adhere to rigid-body dynamics and collision constraints. We validate EToT on a suite of short- and long-horizon manipulation tasks, where it consistently outperforms baselines by effectively predicting physical dynamics and adapting to potential failures. Website at https://embodied-tree-of-thoughts.github.io .",
    "arxiv_url": "https://arxiv.org/abs/2512.08188v1",
    "pdf_url": "https://arxiv.org/pdf/2512.08188v1",
    "published_date": "2025-12-09",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "interactive",
      "world model",
      "physical",
      "dynamics",
      "physics"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.08188v1",
      "pdf": "https://arxiv.org/pdf/2512.08188v1",
      "project": "https://embodied-tree-of-thoughts.github.io"
    },
    "bibtex": ""
  },
  {
    "title": "A Comparative Study of EMG- and IMU-based Gesture Recognition at the Wrist and Forearm",
    "authors": [
      "Soroush Baghernezhad",
      "Elaheh Mohammadreza",
      "Vinicius Prado da Fonseca",
      "Ting Zou",
      "Xianta Jiang"
    ],
    "abstract": "Gestures are an integral part of our daily interactions with the environment. Hand gesture recognition (HGR) is the process of interpreting human intent through various input modalities, such as visual data (images and videos) and bio-signals. Bio-signals are widely used in HGR due to their ability to be captured non-invasively via sensors placed on the arm. Among these, surface electromyography (sEMG), which measures the electrical activity of muscles, is the most extensively studied modality. However, less-explored alternatives such as inertial measurement units (IMUs) can provide complementary information on subtle muscle movements, which makes them valuable for gesture recognition. In this study, we investigate the potential of using IMU signals from different muscle groups to capture user intent. Our results demonstrate that IMU signals contain sufficient information to serve as the sole input sensor for static gesture recognition. Moreover, we compare different muscle groups and check the quality of pattern recognition on individual muscle groups. We further found that tendon-induced micro-movement captured by IMUs is a major contributor to static gesture recognition. We believe that leveraging muscle micro-movement information can enhance the usability of prosthetic arms for amputees. This approach also offers new possibilities for hand gesture recognition in fields such as robotics, teleoperation, sign language interpretation, and beyond.",
    "arxiv_url": "https://arxiv.org/abs/2512.07997v1",
    "pdf_url": "https://arxiv.org/pdf/2512.07997v1",
    "published_date": "2025-12-08",
    "categories": [
      "cs.HC",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "gesture",
      "robotics"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.07997v1",
      "pdf": "https://arxiv.org/pdf/2512.07997v1"
    },
    "bibtex": ""
  },
  {
    "title": "UnityVideo: Unified Multi-Modal Multi-Task Learning for Enhancing World-Aware Video Generation",
    "authors": [
      "Jiehui Huang",
      "Yuechen Zhang",
      "Xu He",
      "Yuan Gao",
      "Zhi Cen",
      "Bin Xia",
      "Yan Zhou",
      "Xin Tao",
      "Pengfei Wan",
      "Jiaya Jia"
    ],
    "abstract": "Recent video generation models demonstrate impressive synthesis capabilities but remain limited by single-modality conditioning, constraining their holistic world understanding. This stems from insufficient cross-modal interaction and limited modal diversity for comprehensive world knowledge representation. To address these limitations, we introduce UnityVideo, a unified framework for world-aware video generation that jointly learns across multiple modalities (segmentation masks, human skeletons, DensePose, optical flow, and depth maps) and training paradigms. Our approach features two core components: (1) dynamic noising to unify heterogeneous training paradigms, and (2) a modality switcher with an in-context learner that enables unified processing via modular parameters and contextual learning. We contribute a large-scale unified dataset with 1.3M samples. Through joint optimization, UnityVideo accelerates convergence and significantly enhances zero-shot generalization to unseen data. We demonstrate that UnityVideo achieves superior video quality, consistency, and improved alignment with physical world constraints. Code and data can be found at: https://github.com/dvlab-research/UnityVideo",
    "arxiv_url": "https://arxiv.org/abs/2512.07831v1",
    "pdf_url": "https://arxiv.org/pdf/2512.07831v1",
    "published_date": "2025-12-08",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/dvlab-research/UnityVideo",
    "keywords": [
      "multi-modal",
      "physical",
      "video generation",
      "dit"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.07831v1",
      "pdf": "https://arxiv.org/pdf/2512.07831v1",
      "github": "https://github.com/dvlab-research/UnityVideo"
    },
    "bibtex": ""
  },
  {
    "title": "WorldReel: 4D Video Generation with Consistent Geometry and Motion Modeling",
    "authors": [
      "Shaoheng Fang",
      "Hanwen Jiang",
      "Yunpeng Bai",
      "Niloy J. Mitra",
      "Qixing Huang"
    ],
    "abstract": "Recent video generators achieve striking photorealism, yet remain fundamentally inconsistent in 3D. We present WorldReel, a 4D video generator that is natively spatio-temporally consistent. WorldReel jointly produces RGB frames together with 4D scene representations, including pointmaps, camera trajectory, and dense flow mapping, enabling coherent geometry and appearance modeling over time. Our explicit 4D representation enforces a single underlying scene that persists across viewpoints and dynamic content, yielding videos that remain consistent even under large non-rigid motion and significant camera movement. We train WorldReel by carefully combining synthetic and real data: synthetic data providing precise 4D supervision (geometry, motion, and camera), while real videos contribute visual diversity and realism. This blend allows WorldReel to generalize to in-the-wild footage while preserving strong geometric fidelity. Extensive experiments demonstrate that WorldReel sets a new state-of-the-art for consistent video generation with dynamic scenes and moving cameras, improving metrics of geometric consistency, motion coherence, and reducing view-time artifacts over competing methods. We believe that WorldReel brings video generation closer to 4D-consistent world modeling, where agents can render, interact, and reason about scenes through a single and stable spatiotemporal representation.",
    "arxiv_url": "https://arxiv.org/abs/2512.07821v1",
    "pdf_url": "https://arxiv.org/pdf/2512.07821v1",
    "published_date": "2025-12-08",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "world model",
      "trajectory",
      "video generation"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.07821v1",
      "pdf": "https://arxiv.org/pdf/2512.07821v1"
    },
    "bibtex": ""
  },
  {
    "title": "OneStory: Coherent Multi-Shot Video Generation with Adaptive Memory",
    "authors": [
      "Zhaochong An",
      "Menglin Jia",
      "Haonan Qiu",
      "Zijian Zhou",
      "Xiaoke Huang",
      "Zhiheng Liu",
      "Weiming Ren",
      "Kumara Kahatapitiya",
      "Ding Liu",
      "Sen He",
      "Chenyang Zhang",
      "Tao Xiang",
      "Fanny Yang",
      "Serge Belongie",
      "Tian Xie"
    ],
    "abstract": "Storytelling in real-world videos often unfolds through multiple shots -- discontinuous yet semantically connected clips that together convey a coherent narrative. However, existing multi-shot video generation (MSV) methods struggle to effectively model long-range cross-shot context, as they rely on limited temporal windows or single keyframe conditioning, leading to degraded performance under complex narratives. In this work, we propose OneStory, enabling global yet compact cross-shot context modeling for consistent and scalable narrative generation. OneStory reformulates MSV as a next-shot generation task, enabling autoregressive shot synthesis while leveraging pretrained image-to-video (I2V) models for strong visual conditioning. We introduce two key modules: a Frame Selection module that constructs a semantically-relevant global memory based on informative frames from prior shots, and an Adaptive Conditioner that performs importance-guided patchification to generate compact context for direct conditioning. We further curate a high-quality multi-shot dataset with referential captions to mirror real-world storytelling patterns, and design effective training strategies under the next-shot paradigm. Finetuned from a pretrained I2V model on our curated 60K dataset, OneStory achieves state-of-the-art narrative coherence across diverse and complex scenes in both text- and image-conditioned settings, enabling controllable and immersive long-form video storytelling.",
    "arxiv_url": "https://arxiv.org/abs/2512.07802v1",
    "pdf_url": "https://arxiv.org/pdf/2512.07802v1",
    "published_date": "2025-12-08",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "controllable",
      "dit",
      "long-form",
      "video generation",
      "image-to-video",
      "i2v",
      "autoregressive"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.07802v1",
      "pdf": "https://arxiv.org/pdf/2512.07802v1"
    },
    "bibtex": ""
  },
  {
    "title": "Unison: A Fully Automatic, Task-Universal, and Low-Cost Framework for Unified Understanding and Generation",
    "authors": [
      "Shihao Zhao",
      "Yitong Chen",
      "Zeyinzi Jiang",
      "Bojia Zi",
      "Shaozhe Hao",
      "Yu Liu",
      "Chaojie Mao",
      "Kwan-Yee K. Wong"
    ],
    "abstract": "Unified understanding and generation is a highly appealing research direction in multimodal learning. There exist two approaches: one trains a transformer via an auto-regressive paradigm, and the other adopts a two-stage scheme connecting pre-trained understanding and generative models for alignment fine-tuning. The former demands massive data and computing resources unaffordable for ordinary researchers. Though the latter requires a lower training cost, existing works often suffer from limited task coverage or poor generation quality. Both approaches lack the ability to parse input meta-information (such as task type, image resolution, video duration, etc.) and require manual parameter configuration that is tedious and non-intelligent. In this paper, we propose Unison which adopts the two-stage scheme while preserving the capabilities of the pre-trained models well. With an extremely low training cost, we cover a variety of multimodal understanding tasks, including text, image, and video understanding, as well as diverse generation tasks, such as text-to-visual content generation, editing, controllable generation, and IP-based reference generation. We also equip our model with the ability to automatically parse user intentions, determine the target task type, and accurately extract the meta-information required for the corresponding task. This enables full automation of various multimodal tasks without human intervention. Experiments demonstrate that, under a low-cost setting of only 500k training samples and 50 GPU hours, our model can accurately and automatically identify tasks and extract relevant parameters, and achieve superior performance across a variety of understanding and generation tasks.",
    "arxiv_url": "https://arxiv.org/abs/2512.07747v1",
    "pdf_url": "https://arxiv.org/pdf/2512.07747v1",
    "published_date": "2025-12-08",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "controllable",
      "dit"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.07747v1",
      "pdf": "https://arxiv.org/pdf/2512.07747v1"
    },
    "bibtex": ""
  },
  {
    "title": "ViSA: 3D-Aware Video Shading for Real-Time Upper-Body Avatar Creation",
    "authors": [
      "Fan Yang",
      "Heyuan Li",
      "Peihao Li",
      "Weihao Yuan",
      "Lingteng Qiu",
      "Chaoyue Song",
      "Cheng Chen",
      "Yisheng He",
      "Shifeng Zhang",
      "Xiaoguang Han",
      "Steven Hoi",
      "Guosheng Lin"
    ],
    "abstract": "Generating high-fidelity upper-body 3D avatars from one-shot input image remains a significant challenge. Current 3D avatar generation methods, which rely on large reconstruction models, are fast and capable of producing stable body structures, but they often suffer from artifacts such as blurry textures and stiff, unnatural motion. In contrast, generative video models show promising performance by synthesizing photorealistic and dynamic results, but they frequently struggle with unstable behavior, including body structural errors and identity drift. To address these limitations, we propose a novel approach that combines the strengths of both paradigms. Our framework employs a 3D reconstruction model to provide robust structural and appearance priors, which in turn guides a real-time autoregressive video diffusion model for rendering. This process enables the model to synthesize high-frequency, photorealistic details and fluid dynamics in real time, effectively reducing texture blur and motion stiffness while preventing the structural inconsistencies common in video generation methods. By uniting the geometric stability of 3D reconstruction with the generative capabilities of video models, our method produces high-fidelity digital avatars with realistic appearance and dynamic, temporally coherent motion. Experiments demonstrate that our approach significantly reduces artifacts and achieves substantial improvements in visual quality over leading methods, providing a robust and efficient solution for real-time applications such as gaming and virtual reality. Project page: https://lhyfst.github.io/visa",
    "arxiv_url": "https://arxiv.org/abs/2512.07720v2",
    "pdf_url": "https://arxiv.org/pdf/2512.07720v2",
    "published_date": "2025-12-08",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "video diffusion",
      "3d-aware",
      "identity",
      "video generation",
      "diffusion model",
      "dynamics",
      "autoregressive",
      "efficient",
      "avatar"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.07720v2",
      "pdf": "https://arxiv.org/pdf/2512.07720v2",
      "project": "https://lhyfst.github.io/visa"
    },
    "bibtex": ""
  },
  {
    "title": "UnCageNet: Tracking and Pose Estimation of Caged Animal",
    "authors": [
      "Sayak Dutta",
      "Harish Katti",
      "Shashikant Verma",
      "Shanmuganathan Raman"
    ],
    "abstract": "Animal tracking and pose estimation systems, such as STEP (Simultaneous Tracking and Pose Estimation) and ViTPose, experience substantial performance drops when processing images and videos with cage structures and systematic occlusions. We present a three-stage preprocessing pipeline that addresses this limitation through: (1) cage segmentation using a Gabor-enhanced ResNet-UNet architecture with tunable orientation filters, (2) cage inpainting using CRFill for content-aware reconstruction of occluded regions, and (3) evaluation of pose estimation and tracking on the uncaged frames. Our Gabor-enhanced segmentation model leverages orientation-aware features with 72 directional kernels to accurately identify and segment cage structures that severely impair the performance of existing methods. Experimental validation demonstrates that removing cage occlusions through our pipeline enables pose estimation and tracking performance comparable to that in environments without occlusions. We also observe significant improvements in keypoint detection accuracy and trajectory consistency.",
    "arxiv_url": "https://arxiv.org/abs/2512.07712v2",
    "pdf_url": "https://arxiv.org/pdf/2512.07712v2",
    "published_date": "2025-12-08",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "evaluation",
      "architecture",
      "trajectory"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.07712v2",
      "pdf": "https://arxiv.org/pdf/2512.07712v2"
    },
    "bibtex": ""
  },
  {
    "title": "More than Segmentation: Benchmarking SAM 3 for Segmentation, 3D Perception, and Reconstruction in Robotic Surgery",
    "authors": [
      "Wenzhen Dong",
      "Jieming Yu",
      "Yiming Huang",
      "Hongqiu Wang",
      "Lei Zhu",
      "Albert C. S. Chung",
      "Hongliang Ren",
      "Long Bai"
    ],
    "abstract": "The recent SAM 3 and SAM 3D have introduced significant advancements over the predecessor, SAM 2, particularly with the integration of language-based segmentation and enhanced 3D perception capabilities. SAM 3 supports zero-shot segmentation across a wide range of prompts, including point, bounding box, and language-based prompts, allowing for more flexible and intuitive interactions with the model. In this empirical evaluation, we assess the performance of SAM 3 in robot-assisted surgery, benchmarking its zero-shot segmentation with point and bounding box prompts and exploring its effectiveness in dynamic video tracking, alongside its newly introduced language prompt segmentation. While language prompts show potential, their performance in the surgical domain is currently suboptimal, highlighting the need for further domain-specific training. Additionally, we investigate SAM 3D's depth reconstruction abilities, demonstrating its capacity to process surgical scene data and reconstruct 3D anatomical structures from 2D images. Through comprehensive testing on the MICCAI EndoVis 2017 and EndoVis 2018 benchmarks, SAM 3 shows clear improvements over SAM and SAM 2 in both image and video segmentation under spatial prompts, while the zero-shot evaluations of SAM 3D on SCARED, StereoMIS, and EndoNeRF indicate strong monocular depth estimation and realistic 3D instrument reconstruction, yet also reveal remaining limitations in complex, highly dynamic surgical scenes.",
    "arxiv_url": "https://arxiv.org/abs/2512.07596v2",
    "pdf_url": "https://arxiv.org/pdf/2512.07596v2",
    "published_date": "2025-12-08",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "benchmark",
      "evaluation",
      "dit"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.07596v2",
      "pdf": "https://arxiv.org/pdf/2512.07596v2"
    },
    "bibtex": ""
  },
  {
    "title": "MultiMotion: Multi Subject Video Motion Transfer via Video Diffusion Transformer",
    "authors": [
      "Penghui Liu",
      "Jiangshan Wang",
      "Yutong Shen",
      "Shanhui Mo",
      "Chenyang Qi",
      "Yue Ma"
    ],
    "abstract": "Multi-object video motion transfer poses significant challenges for Diffusion Transformer (DiT) architectures due to inherent motion entanglement and lack of object-level control. We present MultiMotion, a novel unified framework that overcomes these limitations. Our core innovation is Maskaware Attention Motion Flow (AMF), which utilizes SAM2 masks to explicitly disentangle and control motion features for multiple objects within the DiT pipeline. Furthermore, we introduce RectPC, a high-order predictor-corrector solver for efficient and accurate sampling, particularly beneficial for multi-entity generation. To facilitate rigorous evaluation, we construct the first benchmark dataset specifically for DiT-based multi-object motion transfer. MultiMotion demonstrably achieves precise, semantically aligned, and temporally coherent motion transfer for multiple distinct objects, maintaining DiT's high quality and scalability. The code is in the supp.",
    "arxiv_url": "https://arxiv.org/abs/2512.07500v2",
    "pdf_url": "https://arxiv.org/pdf/2512.07500v2",
    "published_date": "2025-12-08",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "video diffusion",
      "dit",
      "diffusion transformer",
      "efficient",
      "evaluation",
      "architecture",
      "benchmark"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.07500v2",
      "pdf": "https://arxiv.org/pdf/2512.07500v2"
    },
    "bibtex": ""
  },
  {
    "title": "Unified Video Editing with Temporal Reasoner",
    "authors": [
      "Xiangpeng Yang",
      "Ji Xie",
      "Yiyuan Yang",
      "Yan Huang",
      "Min Xu",
      "Qiang Wu"
    ],
    "abstract": "Existing video editing methods face a critical trade-off: expert models offer precision but rely on task-specific priors like masks, hindering unification; conversely, unified temporal in-context learning models are mask-free but lack explicit spatial cues, leading to weak instruction-to-region mapping and imprecise localization. To resolve this conflict, we propose VideoCoF, a novel Chain-of-Frames approach inspired by Chain-of-Thought reasoning. VideoCoF enforces a ``see, reason, then edit\" procedure by compelling the video diffusion model to first predict reasoning tokens (edit-region latents) before generating the target video tokens. This explicit reasoning step removes the need for user-provided masks while achieving precise instruction-to-region alignment and fine-grained video editing. Furthermore, we introduce a RoPE alignment strategy that leverages these reasoning tokens to ensure motion alignment and enable length extrapolation beyond the training duration. We demonstrate that with a minimal data cost of only 50k video pairs, VideoCoF achieves state-of-the-art performance on VideoCoF-Bench, validating the efficiency and effectiveness of our approach. Our code, weight, data are available at https://github.com/knightyxp/VideoCoF.",
    "arxiv_url": "https://arxiv.org/abs/2512.07469v1",
    "pdf_url": "https://arxiv.org/pdf/2512.07469v1",
    "published_date": "2025-12-08",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/knightyxp/VideoCoF",
    "keywords": [
      "diffusion model",
      "video diffusion",
      "video editing",
      "dit"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.07469v1",
      "pdf": "https://arxiv.org/pdf/2512.07469v1",
      "github": "https://github.com/knightyxp/VideoCoF"
    },
    "bibtex": ""
  },
  {
    "title": "ContextAnyone: Context-Aware Diffusion for Character-Consistent Text-to-Video Generation",
    "authors": [
      "Ziyang Mai",
      "Yu-Wing Tai"
    ],
    "abstract": "Text-to-video (T2V) generation has advanced rapidly, yet maintaining consistent character identities across scenes remains a major challenge. Existing personalization methods often focus on facial identity but fail to preserve broader contextual cues such as hairstyle, outfit, and body shape, which are critical for visual coherence. We propose \\textbf{ContextAnyone}, a context-aware diffusion framework that achieves character-consistent video generation from text and a single reference image. Our method jointly reconstructs the reference image and generates new video frames, enabling the model to fully perceive and utilize reference information. Reference information is effectively integrated into a DiT-based diffusion backbone through a novel Emphasize-Attention module that selectively reinforces reference-aware features and prevents identity drift across frames. A dual-guidance loss combines diffusion and reference reconstruction objectives to enhance appearance fidelity, while the proposed Gap-RoPE positional embedding separates reference and video tokens to stabilize temporal modeling. Experiments demonstrate that ContextAnyone outperforms existing reference-to-video methods in identity consistency and visual quality, generating coherent and context-preserving character videos across diverse motions and scenes. Project page: \\href{https://github.com/ziyang1106/ContextAnyone}{https://github.com/ziyang1106/ContextAnyone}.",
    "arxiv_url": "https://arxiv.org/abs/2512.07328v1",
    "pdf_url": "https://arxiv.org/pdf/2512.07328v1",
    "published_date": "2025-12-08",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "https://github.com/ziyang1106/ContextAnyone",
    "keywords": [
      "dit",
      "identity",
      "text-to-video",
      "video generation",
      "personalization",
      "t2v",
      "style"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.07328v1",
      "pdf": "https://arxiv.org/pdf/2512.07328v1",
      "github": "https://github.com/ziyang1106/ContextAnyone"
    },
    "bibtex": ""
  },
  {
    "title": "Unified Camera Positional Encoding for Controlled Video Generation",
    "authors": [
      "Cheng Zhang",
      "Boying Li",
      "Meng Wei",
      "Yan-Pei Cao",
      "Camilo Cruz Gambardella",
      "Dinh Phung",
      "Jianfei Cai"
    ],
    "abstract": "Transformers have emerged as a universal backbone across 3D perception, video generation, and world models for autonomous driving and embodied AI, where understanding camera geometry is essential for grounding visual observations in three-dimensional space. However, existing camera encoding methods often rely on simplified pinhole assumptions, restricting generalization across the diverse intrinsics and lens distortions in real-world cameras. We introduce Relative Ray Encoding, a geometry-consistent representation that unifies complete camera information, including 6-DoF poses, intrinsics, and lens distortions. To evaluate its capability under diverse controllability demands, we adopt camera-controlled text-to-video generation as a testbed task. Within this setting, we further identify pitch and roll as two components effective for Absolute Orientation Encoding, enabling full control over the initial camera orientation. Together, these designs form UCPE (Unified Camera Positional Encoding), which integrates into a pretrained video Diffusion Transformer through a lightweight spatial attention adapter, adding less than 1% trainable parameters while achieving state-of-the-art camera controllability and visual fidelity. To facilitate systematic training and evaluation, we construct a large video dataset covering a wide range of camera motions and lens types. Extensive experiments validate the effectiveness of UCPE in camera-controllable video generation and highlight its potential as a general camera representation for Transformers across future multi-view, video, and 3D tasks. Code will be available at https://github.com/chengzhag/UCPE.",
    "arxiv_url": "https://arxiv.org/abs/2512.07237v1",
    "pdf_url": "https://arxiv.org/pdf/2512.07237v1",
    "published_date": "2025-12-08",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/chengzhag/UCPE",
    "keywords": [
      "video diffusion",
      "controllable",
      "world model",
      "diffusion transformer",
      "text-to-video",
      "video generation",
      "autonomous driving",
      "evaluation",
      "camera control"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.07237v1",
      "pdf": "https://arxiv.org/pdf/2512.07237v1",
      "github": "https://github.com/chengzhag/UCPE"
    },
    "bibtex": ""
  },
  {
    "title": "VideoVLA: Video Generators Can Be Generalizable Robot Manipulators",
    "authors": [
      "Yichao Shen",
      "Fangyun Wei",
      "Zhiying Du",
      "Yaobo Liang",
      "Yan Lu",
      "Jiaolong Yang",
      "Nanning Zheng",
      "Baining Guo"
    ],
    "abstract": "Generalization in robot manipulation is essential for deploying robots in open-world environments and advancing toward artificial general intelligence. While recent Vision-Language-Action (VLA) models leverage large pre-trained understanding models for perception and instruction following, their ability to generalize to novel tasks, objects, and settings remains limited. In this work, we present VideoVLA, a simple approach that explores the potential of transforming large video generation models into robotic VLA manipulators. Given a language instruction and an image, VideoVLA predicts an action sequence as well as the future visual outcomes. Built on a multi-modal Diffusion Transformer, VideoVLA jointly models video, language, and action modalities, using pre-trained video generative models for joint visual and action forecasting. Our experiments show that high-quality imagined futures correlate with reliable action predictions and task success, highlighting the importance of visual imagination in manipulation. VideoVLA demonstrates strong generalization, including imitating other embodiments' skills and handling novel objects. This dual-prediction strategy - forecasting both actions and their visual consequences - explores a paradigm shift in robot learning and unlocks generalization capabilities in manipulation systems.",
    "arxiv_url": "https://arxiv.org/abs/2512.06963v1",
    "pdf_url": "https://arxiv.org/pdf/2512.06963v1",
    "published_date": "2025-12-07",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "multi-modal",
      "diffusion transformer",
      "video generation"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.06963v1",
      "pdf": "https://arxiv.org/pdf/2512.06963v1"
    },
    "bibtex": ""
  },
  {
    "title": "Scaling Zero-Shot Reference-to-Video Generation",
    "authors": [
      "Zijian Zhou",
      "Shikun Liu",
      "Haozhe Liu",
      "Haonan Qiu",
      "Zhaochong An",
      "Weiming Ren",
      "Zhiheng Liu",
      "Xiaoke Huang",
      "Kam Woh Ng",
      "Tian Xie",
      "Xiao Han",
      "Yuren Cong",
      "Hang Li",
      "Chuyan Zhu",
      "Aditya Patel",
      "Tao Xiang",
      "Sen He"
    ],
    "abstract": "Reference-to-video (R2V) generation aims to synthesize videos that align with a text prompt while preserving the subject identity from reference images. However, current R2V methods are hindered by the reliance on explicit reference image-video-text triplets, whose construction is highly expensive and difficult to scale. We bypass this bottleneck by introducing Saber, a scalable zero-shot framework that requires no explicit R2V data. Trained exclusively on video-text pairs, Saber employs a masked training strategy and a tailored attention-based model design to learn identity-consistent and reference-aware representations. Mask augmentation techniques are further integrated to mitigate copy-paste artifacts common in reference-to-video generation. Moreover, Saber demonstrates remarkable generalization capabilities across a varying number of references and achieves superior performance on the OpenS2V-Eval benchmark compared to methods trained with R2V data.",
    "arxiv_url": "https://arxiv.org/abs/2512.06905v1",
    "pdf_url": "https://arxiv.org/pdf/2512.06905v1",
    "published_date": "2025-12-07",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "benchmark",
      "identity",
      "video generation"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.06905v1",
      "pdf": "https://arxiv.org/pdf/2512.06905v1"
    },
    "bibtex": ""
  },
  {
    "title": "Pseudo Anomalies Are All You Need: Diffusion-Based Generation for Weakly-Supervised Video Anomaly Detection",
    "authors": [
      "Satoshi Hashimoto",
      "Hitoshi Nishimura",
      "Yanan Wang",
      "Mori Kurokawa"
    ],
    "abstract": "Deploying video anomaly detection in practice is hampered by the scarcity and collection cost of real abnormal footage. We address this by training without any real abnormal videos while evaluating under the standard weakly supervised split, and we introduce PA-VAD, a generation-driven approach that learns a detector from synthesized pseudo-abnormal videos paired with real normal videos, using only a small set of real normal images to drive synthesis. For synthesis, we select class-relevant initial images with CLIP and refine textual prompts with a vision-language model to improve fidelity and scene consistency before invoking a video diffusion model. For training, we mitigate excessive spatiotemporal magnitude in synthesized anomalies by an domain-aligned regularized module that combines domain alignment and memory usage-aware updates. Extensive experiments show that our approach reaches 98.2% on ShanghaiTech and 82.5% on UCF-Crime, surpassing the strongest real-abnormal method on ShanghaiTech by +0.6% and outperforming the UVAD state-of-the-art on UCF-Crime by +1.9%. The results demonstrate that high-accuracy anomaly detection can be obtained without collecting real anomalies, providing a practical path toward scalable deployment.",
    "arxiv_url": "https://arxiv.org/abs/2512.06845v1",
    "pdf_url": "https://arxiv.org/pdf/2512.06845v1",
    "published_date": "2025-12-07",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "diffusion model",
      "video diffusion"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.06845v1",
      "pdf": "https://arxiv.org/pdf/2512.06845v1"
    },
    "bibtex": ""
  },
  {
    "title": "Generative Teaching via Code",
    "authors": [
      "Yuheng Wang",
      "Runde Yang",
      "Lin Wu",
      "Jie Zhang",
      "Jingru Fan",
      "Ruoyu Fu",
      "Tianle Zhou",
      "Huatao Li",
      "Siheng Chen",
      "Weinan E",
      "Chen Qian"
    ],
    "abstract": "The scalability of high-quality online education is hindered by the high costs and slow cycles of labor-intensive manual content creation. Despite advancements in video generation, current approaches often fail to ensure pedagogical structure and precise control due to their pixel-level, black-box nature. In this paper, we propose Generative Teaching, a novel paradigm that transitions educators from manual creators to high-level directors, allowing them to focus on pedagogical intent while autonomous agents handle the execution. To realize this vision, we introduce TeachMaster, a multi-agent framework that leverages code as an intermediate semantic medium. Unlike traditional video generation methods, TeachMaster orchestrates a collaborative team of agents--spanning planning, design, and rendering--to automate the production of interpretable, editable, and curriculum-ready educational videos. Experiments validate that TeachMaster significantly boosts production efficiency without compromising structural coherence or visual fidelity, providing a robust solution for scalable education.",
    "arxiv_url": "https://arxiv.org/abs/2601.04204v1",
    "pdf_url": "https://arxiv.org/pdf/2601.04204v1",
    "published_date": "2025-12-07",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL",
      "cs.HC",
      "cs.MA"
    ],
    "github_url": "",
    "keywords": [
      "education",
      "video generation",
      "dit"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.04204v1",
      "pdf": "https://arxiv.org/pdf/2601.04204v1"
    },
    "bibtex": ""
  },
  {
    "title": "VDOT: Efficient Unified Video Creation via Optimal Transport Distillation",
    "authors": [
      "Yutong Wang",
      "Haiyu Zhang",
      "Tianfan Xue",
      "Yu Qiao",
      "Yaohui Wang",
      "Chang Xu",
      "Xinyuan Chen"
    ],
    "abstract": "The rapid development of generative models has significantly advanced image and video applications. Among these, video creation, aimed at generating videos under various conditions, has gained substantial attention. However, existing video creation models either focus solely on a few specific conditions or suffer from excessively long generation times due to complex model inference, making them impractical for real-world applications. To mitigate these issues, we propose an efficient unified video creation model, named VDOT. Concretely, we model the training process with the distribution matching distillation (DMD) paradigm. Instead of using the Kullback-Leibler (KL) minimization, we additionally employ a novel computational optimal transport (OT) technique to optimize the discrepancy between the real and fake score distributions. The OT distance inherently imposes geometric constraints, mitigating potential zero-forcing or gradient collapse issues that may arise during KL-based distillation within the few-step generation scenario, and thus, enhances the efficiency and stability of the distillation process. Further, we integrate a discriminator to enable the model to perceive real video data, thereby enhancing the quality of generated videos. To support training unified video creation models, we propose a fully automated pipeline for video data annotation and filtering that accommodates multiple video creation tasks. Meanwhile, we curate a unified testing benchmark, UVCBench, to standardize evaluation. Experiments demonstrate that our 4-step VDOT outperforms or matches other baselines with 100 denoising steps.",
    "arxiv_url": "https://arxiv.org/abs/2512.06802v3",
    "pdf_url": "https://arxiv.org/pdf/2512.06802v3",
    "published_date": "2025-12-07",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "distillation",
      "dit",
      "efficient",
      "evaluation",
      "denoising",
      "benchmark"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.06802v3",
      "pdf": "https://arxiv.org/pdf/2512.06802v3"
    },
    "bibtex": ""
  },
  {
    "title": "RunawayEvil: Jailbreaking the Image-to-Video Generative Models",
    "authors": [
      "Songping Wang",
      "Rufan Qian",
      "Yueming Lyu",
      "Qinglong Liu",
      "Linzhuang Zou",
      "Jie Qin",
      "Songhua Liu",
      "Caifeng Shan"
    ],
    "abstract": "Image-to-Video (I2V) generation synthesizes dynamic visual content from image and text inputs, providing significant creative control. However, the security of such multimodal systems, particularly their vulnerability to jailbreak attacks, remains critically underexplored. To bridge this gap, we propose RunawayEvil, the first multimodal jailbreak framework for I2V models with dynamic evolutionary capability. Built on a \"Strategy-Tactic-Action\" paradigm, our framework exhibits self-amplifying attack through three core components: (1) Strategy-Aware Command Unit that enables the attack to self-evolve its strategies through reinforcement learning-driven strategy customization and LLM-based strategy exploration; (2) Multimodal Tactical Planning Unit that generates coordinated text jailbreak instructions and image tampering guidelines based on the selected strategies; (3) Tactical Action Unit that executes and evaluates the multimodal coordinated attacks. This self-evolving architecture allows the framework to continuously adapt and intensify its attack strategies without human intervention. Extensive experiments demonstrate RunawayEvil achieves state-of-the-art attack success rates on commercial I2V models, such as Open-Sora 2.0 and CogVideoX. Specifically, RunawayEvil outperforms existing methods by 58.5 to 79 percent on COCO2017. This work provides a critical tool for vulnerability analysis of I2V models, thereby laying a foundation for more robust video generation systems.",
    "arxiv_url": "https://arxiv.org/abs/2512.06674v1",
    "pdf_url": "https://arxiv.org/pdf/2512.06674v1",
    "published_date": "2025-12-07",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "creative",
      "video generation",
      "image-to-video",
      "i2v",
      "architecture",
      "customization"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.06674v1",
      "pdf": "https://arxiv.org/pdf/2512.06674v1"
    },
    "bibtex": ""
  },
  {
    "title": "MIND-V: Hierarchical Video Generation for Long-Horizon Robotic Manipulation with RL-based Physical Alignment",
    "authors": [
      "Ruicheng Zhang",
      "Mingyang Zhang",
      "Jun Zhou",
      "Zhangrui Guo",
      "Xiaofan Liu",
      "Zunnan Xu",
      "Zhizhou Zhong",
      "Puxin Yan",
      "Haocheng Luo",
      "Xiu Li"
    ],
    "abstract": "Embodied imitation learning is constrained by the scarcity of diverse, long-horizon robotic manipulation data. Existing video generation models for this domain are limited to synthesizing short clips of simple actions and often rely on manually defined trajectories. To this end, we introduce MIND-V, a hierarchical framework designed to synthesize physically plausible and logically coherent videos of long-horizon robotic manipulation. Inspired by cognitive science, MIND-V bridges high-level reasoning with pixel-level synthesis through three core components: a Semantic Reasoning Hub (SRH) that leverages a pre-trained vision-language model for task planning; a Behavioral Semantic Bridge (BSB) that translates abstract instructions into domain-invariant representations; and a Motor Video Generator (MVG) for conditional video rendering. MIND-V employs Staged Visual Future Rollouts, a test-time optimization strategy to enhance long-horizon robustness. To align the generated videos with physical laws, we introduce a GRPO reinforcement learning post-training phase guided by a novel Physical Foresight Coherence (PFC) reward. PFC leverages the V-JEPA world model to enforce physical plausibility by aligning the predicted and actual dynamic evolutions in the feature space. MIND-V demonstrates state-of-the-art performance in long-horizon robotic manipulation video generation, establishing a scalable and controllable paradigm for embodied data synthesis.",
    "arxiv_url": "https://arxiv.org/abs/2512.06628v1",
    "pdf_url": "https://arxiv.org/pdf/2512.06628v1",
    "published_date": "2025-12-07",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "controllable",
      "dit",
      "world model",
      "physical",
      "video generation"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.06628v1",
      "pdf": "https://arxiv.org/pdf/2512.06628v1"
    },
    "bibtex": ""
  },
  {
    "title": "ShadowWolf -- Automatic Labelling, Evaluation and Model Training Optimised for Camera Trap Wildlife Images",
    "authors": [
      "Jens Dede",
      "Anna Frster"
    ],
    "abstract": "The continuous growth of the global human population is leading to the expansion of human habitats, resulting in decreasing wildlife spaces and increasing human-wildlife interactions. These interactions can range from minor disturbances, such as raccoons in urban waste bins, to more severe consequences, including species extinction. As a result, the monitoring of wildlife is gaining significance in various contexts. Artificial intelligence (AI) offers a solution by automating the recognition of animals in images and videos, thereby reducing the manual effort required for wildlife monitoring. Traditional AI training involves three main stages: image collection, labelling, and model training. However, the variability, for example, in the landscape (e.g., mountains, open fields, forests), weather (e.g., rain, fog, sunshine), lighting (e.g., day, night), and camera-animal distances presents significant challenges to model robustness and adaptability in real-world scenarios.   In this work, we propose a unified framework, called ShadowWolf, designed to address these challenges by integrating and optimizing the stages of AI model training and evaluation. The proposed framework enables dynamic model retraining to adjust to changes in environmental conditions and application requirements, thereby reducing labelling efforts and allowing for on-site model adaptation. This adaptive and unified approach enhances the accuracy and efficiency of wildlife monitoring systems, promoting more effective and scalable conservation efforts.",
    "arxiv_url": "https://arxiv.org/abs/2512.06521v1",
    "pdf_url": "https://arxiv.org/pdf/2512.06521v1",
    "published_date": "2025-12-06",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "evaluation",
      "dit"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.06521v1",
      "pdf": "https://arxiv.org/pdf/2512.06521v1"
    },
    "bibtex": ""
  },
  {
    "title": "Are AI-Generated Driving Videos Ready for Autonomous Driving? A Diagnostic Evaluation Framework",
    "authors": [
      "Xinhao Xiang",
      "Abhijeet Rastogi",
      "Jiawei Zhang"
    ],
    "abstract": "Recent text-to-video models have enabled the generation of high-resolution driving scenes from natural language prompts. These AI-generated driving videos (AIGVs) offer a low-cost, scalable alternative to real or simulator data for autonomous driving (AD). But a key question remains: can such videos reliably support training and evaluation of AD models? We present a diagnostic framework that systematically studies this question. First, we introduce a taxonomy of frequent AIGV failure modes, including visual artifacts, physically implausible motion, and violations of traffic semantics, and demonstrate their negative impact on object detection, tracking, and instance segmentation. To support this analysis, we build ADGV-Bench, a driving-focused benchmark with human quality annotations and dense labels for multiple perception tasks. We then propose ADGVE, a driving-aware evaluator that combines static semantics, temporal cues, lane obedience signals, and Vision-Language Model(VLM)-guided reasoning into a single quality score for each clip. Experiments show that blindly adding raw AIGVs can degrade perception performance, while filtering them with ADGVE consistently improves both general video quality assessment metrics and downstream AD models, and turns AIGVs into a beneficial complement to real-world data. Our study highlights both the risks and the promise of AIGVs, and provides practical tools for safely leveraging large-scale video generation in future AD pipelines.",
    "arxiv_url": "https://arxiv.org/abs/2512.06376v1",
    "pdf_url": "https://arxiv.org/pdf/2512.06376v1",
    "published_date": "2025-12-06",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "physical",
      "text-to-video",
      "video generation",
      "autonomous driving",
      "evaluation",
      "benchmark"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.06376v1",
      "pdf": "https://arxiv.org/pdf/2512.06376v1"
    },
    "bibtex": ""
  },
  {
    "title": "Tracking-Guided 4D Generation: Foundation-Tracker Motion Priors for 3D Model Animation",
    "authors": [
      "Su Sun",
      "Cheng Zhao",
      "Himangi Mittal",
      "Gaurav Mittal",
      "Rohith Kukkala",
      "Yingjie Victor Chen",
      "Mei Chen"
    ],
    "abstract": "Generating dynamic 4D objects from sparse inputs is difficult because it demands joint preservation of appearance and motion coherence across views and time while suppressing artifacts and temporal drift. We hypothesize that the view discrepancy arises from supervision limited to pixel- or latent-space video-diffusion losses, which lack explicitly temporally aware, feature-level tracking guidance. We present \\emph{Track4DGen}, a two-stage framework that couples a multi-view video diffusion model with a foundation point tracker and a hybrid 4D Gaussian Splatting (4D-GS) reconstructor. The central idea is to explicitly inject tracker-derived motion priors into intermediate feature representations for both multi-view video generation and 4D-GS. In Stage One, we enforce dense, feature-level point correspondences inside the diffusion generator, producing temporally consistent features that curb appearance drift and enhance cross-view coherence. In Stage Two, we reconstruct a dynamic 4D-GS using a hybrid motion encoding that concatenates co-located diffusion features (carrying Stage-One tracking priors) with Hex-plane features, and augment them with 4D Spherical Harmonics for higher-fidelity dynamics modeling. \\emph{Track4DGen} surpasses baselines on both multi-view video generation and 4D generation benchmarks, yielding temporally stable, text-editable 4D assets. Lastly, we curate \\emph{Sketchfab28}, a high-quality dataset for benchmarking object-centric 4D generation and fostering future research.",
    "arxiv_url": "https://arxiv.org/abs/2512.06158v1",
    "pdf_url": "https://arxiv.org/pdf/2512.06158v1",
    "published_date": "2025-12-05",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "video diffusion",
      "dit",
      "video generation",
      "multi-view video",
      "diffusion model",
      "4d generation",
      "dynamics",
      "benchmark"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.06158v1",
      "pdf": "https://arxiv.org/pdf/2512.06158v1"
    },
    "bibtex": ""
  },
  {
    "title": "World Models That Know When They Don't Know: Controllable Video Generation with Calibrated Uncertainty",
    "authors": [
      "Zhiting Mei",
      "Tenny Yin",
      "Micah Baker",
      "Ola Shorinwa",
      "Anirudha Majumdar"
    ],
    "abstract": "Recent advances in generative video models have led to significant breakthroughs in high-fidelity video synthesis, specifically in controllable video generation where the generated video is conditioned on text and action inputs, e.g., in instruction-guided video editing and world modeling in robotics. Despite these exceptional capabilities, controllable video models often hallucinate - generating future video frames that are misaligned with physical reality - which raises serious concerns in many tasks such as robot policy evaluation and planning. However, state-of-the-art video models lack the ability to assess and express their confidence, impeding hallucination mitigation. To rigorously address this challenge, we propose C3, an uncertainty quantification (UQ) method for training continuous-scale calibrated controllable video models for dense confidence estimation at the subpatch level, precisely localizing the uncertainty in each generated video frame. Our UQ method introduces three core innovations to empower video models to estimate their uncertainty. First, our method develops a novel framework that trains video models for correctness and calibration via strictly proper scoring rules. Second, we estimate the video model's uncertainty in latent space, avoiding training instability and prohibitive training costs associated with pixel-space approaches. Third, we map the dense latent-space uncertainty to interpretable pixel-level uncertainty in the RGB space for intuitive visualization, providing high-resolution uncertainty heatmaps that identify untrustworthy regions. Through extensive experiments on large-scale robot learning datasets (Bridge and DROID) and real-world evaluations, we demonstrate that our method not only provides calibrated uncertainty estimates within the training distribution, but also enables effective out-of-distribution detection.",
    "arxiv_url": "https://arxiv.org/abs/2512.05927v1",
    "pdf_url": "https://arxiv.org/pdf/2512.05927v1",
    "published_date": "2025-12-05",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "controllable",
      "dit",
      "video editing",
      "world model",
      "physical",
      "video generation",
      "robotics",
      "evaluation",
      "video synthesis"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.05927v1",
      "pdf": "https://arxiv.org/pdf/2512.05927v1"
    },
    "bibtex": ""
  },
  {
    "title": "Bring Your Dreams to Life: Continual Text-to-Video Customization",
    "authors": [
      "Jiahua Dong",
      "Xudong Wang",
      "Wenqi Liang",
      "Zongyan Han",
      "Meng Cao",
      "Duzhen Zhang",
      "Hanbin Zhao",
      "Zhi Han",
      "Salman Khan",
      "Fahad Shahbaz Khan"
    ],
    "abstract": "Customized text-to-video generation (CTVG) has recently witnessed great progress in generating tailored videos from user-specific text. However, most CTVG methods assume that personalized concepts remain static and do not expand incrementally over time. Additionally, they struggle with forgetting and concept neglect when continuously learning new concepts, including subjects and motions. To resolve the above challenges, we develop a novel Continual Customized Video Diffusion (CCVD) model, which can continuously learn new concepts to generate videos across various text-to-video generation tasks by tackling forgetting and concept neglect. To address catastrophic forgetting, we introduce a concept-specific attribute retention module and a task-aware concept aggregation strategy. They can capture the unique characteristics and identities of old concepts during training, while combining all subject and motion adapters of old concepts based on their relevance during testing. Besides, to tackle concept neglect, we develop a controllable conditional synthesis to enhance regional features and align video contexts with user conditions, by incorporating layer-specific region attention-guided noise estimation. Extensive experimental comparisons demonstrate that our CCVD outperforms existing CTVG baselines on both the DreamVideo and Wan 2.1 backbones. The code is available at https://github.com/JiahuaDong/CCVD.",
    "arxiv_url": "https://arxiv.org/abs/2512.05802v2",
    "pdf_url": "https://arxiv.org/pdf/2512.05802v2",
    "published_date": "2025-12-05",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/JiahuaDong/CCVD",
    "keywords": [
      "controllable",
      "video diffusion",
      "dit",
      "text-to-video",
      "video generation",
      "concept",
      "customization"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.05802v2",
      "pdf": "https://arxiv.org/pdf/2512.05802v2",
      "github": "https://github.com/JiahuaDong/CCVD"
    },
    "bibtex": ""
  },
  {
    "title": "USV: Unified Sparsification for Accelerating Video Diffusion Models",
    "authors": [
      "Xinjian Wu",
      "Hongmei Wang",
      "Yuan Zhou",
      "Qinglin Lu"
    ],
    "abstract": "The scalability of high-fidelity video diffusion models (VDMs) is constrained by two key sources of redundancy: the quadratic complexity of global spatio-temporal attention and the computational overhead of long iterative denoising trajectories. Existing accelerators -- such as sparse attention and step-distilled samplers -- typically target a single dimension in isolation and quickly encounter diminishing returns, as the remaining bottlenecks become dominant. In this work, we introduce USV (Unified Sparsification for Video diffusion models), an end-to-end trainable framework that overcomes this limitation by jointly orchestrating sparsification across both the model's internal computation and its sampling process. USV learns a dynamic, data- and timestep-dependent sparsification policy that prunes redundant attention connections, adaptively merges semantically similar tokens, and reduces denoising steps, treating them not as independent tricks but as coordinated actions within a single optimization objective. This multi-dimensional co-design enables strong mutual reinforcement among previously disjoint acceleration strategies. Extensive experiments on large-scale video generation benchmarks demonstrate that USV achieves up to 83.3% speedup in the denoising process and 22.7% end-to-end acceleration, while maintaining high visual fidelity. Our results highlight unified, dynamic sparsification as a practical path toward efficient, high-quality video generation.",
    "arxiv_url": "https://arxiv.org/abs/2512.05754v1",
    "pdf_url": "https://arxiv.org/pdf/2512.05754v1",
    "published_date": "2025-12-05",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "acceleration",
      "video diffusion",
      "video generation",
      "efficient",
      "diffusion model",
      "denoising",
      "benchmark"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.05754v1",
      "pdf": "https://arxiv.org/pdf/2512.05754v1"
    },
    "bibtex": ""
  },
  {
    "title": "InverseCrafter: Efficient Video ReCapture as a Latent Domain Inverse Problem",
    "authors": [
      "Yeobin Hong",
      "Suhyeon Lee",
      "Hyungjin Chung",
      "Jong Chul Ye"
    ],
    "abstract": "Recent approaches to controllable 4D video generation often rely on fine-tuning pre-trained Video Diffusion Models (VDMs). This dominant paradigm is computationally expensive, requiring large-scale datasets and architectural modifications, and frequently suffers from catastrophic forgetting of the model's original generative priors. Here, we propose InverseCrafter, an efficient inpainting inverse solver that reformulates the 4D generation task as an inpainting problem solved in the latent space. The core of our method is a principled mechanism to encode the pixel space degradation operator into a continuous, multi-channel latent mask, thereby bypassing the costly bottleneck of repeated VAE operations and backpropagation. InverseCrafter not only achieves comparable novel view generation and superior measurement consistency in camera control tasks with near-zero computational overhead, but also excels at general-purpose video inpainting with editing. Code is available at https://github.com/yeobinhong/InverseCrafter.",
    "arxiv_url": "https://arxiv.org/abs/2512.05672v1",
    "pdf_url": "https://arxiv.org/pdf/2512.05672v1",
    "published_date": "2025-12-05",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "github_url": "https://github.com/yeobinhong/InverseCrafter",
    "keywords": [
      "controllable",
      "video diffusion",
      "dit",
      "video generation",
      "diffusion model",
      "4d generation",
      "efficient",
      "video inpainting",
      "novel view",
      "camera control"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.05672v1",
      "pdf": "https://arxiv.org/pdf/2512.05672v1",
      "github": "https://github.com/yeobinhong/InverseCrafter"
    },
    "bibtex": ""
  },
  {
    "title": "ProPhy: Progressive Physical Alignment for Dynamic World Simulation",
    "authors": [
      "Zijun Wang",
      "Panwen Hu",
      "Jing Wang",
      "Terry Jingchen Zhang",
      "Yuhao Cheng",
      "Long Chen",
      "Yiqiang Yan",
      "Zutao Jiang",
      "Hanhui Li",
      "Xiaodan Liang"
    ],
    "abstract": "Recent advances in video generation have shown remarkable potential for constructing world simulators. However, current models still struggle to produce physically consistent results, particularly when handling large-scale or complex dynamics. This limitation arises primarily because existing approaches respond isotropically to physical prompts and neglect the fine-grained alignment between generated content and localized physical cues. To address these challenges, we propose ProPhy, a Progressive Physical Alignment Framework that enables explicit physics-aware conditioning and anisotropic generation. ProPhy employs a two-stage Mixture-of-Physics-Experts (MoPE) mechanism for discriminative physical prior extraction, where Semantic Experts infer semantic-level physical principles from textual descriptions, and Refinement Experts capture token-level physical dynamics. This mechanism allows the model to learn fine-grained, physics-aware video representations that better reflect underlying physical laws. Furthermore, we introduce a physical alignment strategy that transfers the physical reasoning capabilities of vision-language models (VLMs) into the Refinement Experts, facilitating a more accurate representation of dynamic physical phenomena. Extensive experiments on physics-aware video generation benchmarks demonstrate that ProPhy produces more realistic, dynamic, and physically coherent results than existing state-of-the-art methods.",
    "arxiv_url": "https://arxiv.org/abs/2512.05564v1",
    "pdf_url": "https://arxiv.org/pdf/2512.05564v1",
    "published_date": "2025-12-05",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "dit",
      "physical",
      "video generation",
      "world simulator",
      "dynamics",
      "simulation",
      "physics-aware",
      "benchmark",
      "physics"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.05564v1",
      "pdf": "https://arxiv.org/pdf/2512.05564v1"
    },
    "bibtex": ""
  },
  {
    "title": "WaterWave: Bridging Underwater Image Enhancement into Video Streams via Wavelet-based Temporal Consistency Field",
    "authors": [
      "Qi Zhu",
      "Jingyi Zhang",
      "Naishan Zheng",
      "Wei Yu",
      "Jinghao Zhang",
      "Deyi Ji",
      "Feng Zhao"
    ],
    "abstract": "Underwater video pairs are fairly difficult to obtain due to the complex underwater imaging. In this case, most existing video underwater enhancement methods are performed by directly applying the single-image enhancement model frame by frame, but a natural issue is lacking temporal consistency. To relieve the problem, we rethink the temporal manifold inherent in natural videos and observe a temporal consistency prior in dynamic scenes from the local temporal frequency perspective. Building upon the specific prior and no paired-data condition, we propose an implicit representation manner for enhanced video signals, which is conducted in the wavelet-based temporal consistency field, WaterWave. Specifically, under the constraints of the prior, we progressively filter and attenuate the inconsistent components while preserving motion details and scenes, achieving a natural-flowing video. Furthermore, to represent temporal frequency bands more accurately, an underwater flow correction module is designed to rectify estimated flows considering the transmission in underwater scenes. Extensive experiments demonstrate that WaterWave significantly enhances the quality of videos generated using single-image underwater enhancements. Additionally, our method demonstrates high potential in downstream underwater tracking tasks, such as UOSTrack and MAT, outperforming the original video by a large margin, i.e., 19.7% and 9.7% on precise respectively.",
    "arxiv_url": "https://arxiv.org/abs/2512.05492v1",
    "pdf_url": "https://arxiv.org/pdf/2512.05492v1",
    "published_date": "2025-12-05",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "temporal consistency",
      "dit"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.05492v1",
      "pdf": "https://arxiv.org/pdf/2512.05492v1"
    },
    "bibtex": ""
  },
  {
    "title": "Delving into Latent Spectral Biasing of Video VAEs for Superior Diffusability",
    "authors": [
      "Shizhan Liu",
      "Xinran Deng",
      "Zhuoyi Yang",
      "Jiayan Teng",
      "Xiaotao Gu",
      "Jie Tang"
    ],
    "abstract": "Latent diffusion models pair VAEs with diffusion backbones, and the structure of VAE latents strongly influences the difficulty of diffusion training. However, existing video VAEs typically focus on reconstruction fidelity, overlooking latent structure. We present a statistical analysis of video VAE latent spaces and identify two spectral properties essential for diffusion training: a spatio-temporal frequency spectrum biased toward low frequencies, and a channel-wise eigenspectrum dominated by a few modes. To induce these properties, we propose two lightweight, backbone-agnostic regularizers: Local Correlation Regularization and Latent Masked Reconstruction. Experiments show that our Spectral-Structured VAE (SSVAE) achieves a $3\\times$ speedup in text-to-video generation convergence and a 10\\% gain in video reward, outperforming strong open-source VAEs. The code is available at https://github.com/zai-org/SSVAE.",
    "arxiv_url": "https://arxiv.org/abs/2512.05394v1",
    "pdf_url": "https://arxiv.org/pdf/2512.05394v1",
    "published_date": "2025-12-05",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/zai-org/SSVAE",
    "keywords": [
      "diffusion model",
      "text-to-video",
      "video generation"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.05394v1",
      "pdf": "https://arxiv.org/pdf/2512.05394v1",
      "github": "https://github.com/zai-org/SSVAE"
    },
    "bibtex": ""
  },
  {
    "title": "IE2Video: Adapting Pretrained Diffusion Models for Event-Based Video Reconstruction",
    "authors": [
      "Dmitrii Torbunov",
      "Onur Okuducu",
      "Yi Huang",
      "Odera Dim",
      "Rebecca Coles",
      "Yonggang Cui",
      "Yihui Ren"
    ],
    "abstract": "Continuous video monitoring in surveillance, robotics, and wearable systems faces a fundamental power constraint: conventional RGB cameras consume substantial energy through fixed-rate capture. Event cameras offer sparse, motion-driven sensing with low power consumption, but produce asynchronous event streams rather than RGB video. We propose a hybrid capture paradigm that records sparse RGB keyframes alongside continuous event streams, then reconstructs full RGB video offline -- reducing capture power consumption while maintaining standard video output for downstream applications. We introduce the Image and Event to Video (IE2Video) task: reconstructing RGB video sequences from a single initial frame and subsequent event camera data. We investigate two architectural strategies: adapting an autoregressive model (HyperE2VID) for RGB generation, and injecting event representations into a pretrained text-to-video diffusion model (LTX) via learned encoders and low-rank adaptation. Our experiments demonstrate that the diffusion-based approach achieves 33\\% better perceptual quality than the autoregressive baseline (0.283 vs 0.422 LPIPS). We validate our approach across three event camera datasets (BS-ERGB, HS-ERGB far/close) at varying sequence lengths (32-128 frames), demonstrating robust cross-dataset generalization with strong performance on unseen capture configurations.",
    "arxiv_url": "https://arxiv.org/abs/2512.05240v1",
    "pdf_url": "https://arxiv.org/pdf/2512.05240v1",
    "published_date": "2025-12-04",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "video diffusion",
      "text-to-video",
      "robotics",
      "diffusion model",
      "autoregressive"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.05240v1",
      "pdf": "https://arxiv.org/pdf/2512.05240v1"
    },
    "bibtex": ""
  },
  {
    "title": "Light-X: Generative 4D Video Rendering with Camera and Illumination Control",
    "authors": [
      "Tianqi Liu",
      "Zhaoxi Chen",
      "Zihao Huang",
      "Shaocong Xu",
      "Saining Zhang",
      "Chongjie Ye",
      "Bohan Li",
      "Zhiguo Cao",
      "Wei Li",
      "Hao Zhao",
      "Ziwei Liu"
    ],
    "abstract": "Recent advances in illumination control extend image-based methods to video, yet still facing a trade-off between lighting fidelity and temporal consistency. Moving beyond relighting, a key step toward generative modeling of real-world scenes is the joint control of camera trajectory and illumination, since visual dynamics are inherently shaped by both geometry and lighting. To this end, we present Light-X, a video generation framework that enables controllable rendering from monocular videos with both viewpoint and illumination control. 1) We propose a disentangled design that decouples geometry and lighting signals: geometry and motion are captured via dynamic point clouds projected along user-defined camera trajectories, while illumination cues are provided by a relit frame consistently projected into the same geometry. These explicit, fine-grained cues enable effective disentanglement and guide high-quality illumination. 2) To address the lack of paired multi-view and multi-illumination videos, we introduce Light-Syn, a degradation-based pipeline with inverse-mapping that synthesizes training pairs from in-the-wild monocular footage. This strategy yields a dataset covering static, dynamic, and AI-generated scenes, ensuring robust training. Extensive experiments show that Light-X outperforms baseline methods in joint camera-illumination control and surpasses prior video relighting methods under both text- and background-conditioned settings.",
    "arxiv_url": "https://arxiv.org/abs/2512.05115v2",
    "pdf_url": "https://arxiv.org/pdf/2512.05115v2",
    "published_date": "2025-12-04",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "temporal consistency",
      "controllable",
      "dit",
      "trajectory",
      "video generation",
      "dynamics"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.05115v2",
      "pdf": "https://arxiv.org/pdf/2512.05115v2"
    },
    "bibtex": ""
  },
  {
    "title": "NeuralRemaster: Phase-Preserving Diffusion for Structure-Aligned Generation",
    "authors": [
      "Yu Zeng",
      "Charles Ochoa",
      "Mingyuan Zhou",
      "Vishal M. Patel",
      "Vitor Guizilini",
      "Rowan McAllister"
    ],
    "abstract": "Standard diffusion corrupts data using Gaussian noise whose Fourier coefficients have random magnitudes and random phases. While effective for unconditional or text-to-image generation, corrupting phase components destroys spatial structure, making it ill-suited for tasks requiring geometric consistency, such as re-rendering, simulation enhancement, and image-to-image translation. We introduce Phase-Preserving Diffusion -PD, a model-agnostic reformulation of the diffusion process that preserves input phase while randomizing magnitude, enabling structure-aligned generation without architectural changes or additional parameters. We further propose Frequency-Selective Structured (FSS) noise, which provides continuous control over structural rigidity via a single frequency-cutoff parameter. -PD adds no inference-time cost and is compatible with any diffusion model for images or videos. Across photorealistic and stylized re-rendering, as well as sim-to-real enhancement for driving planners, -PD produces controllable, spatially aligned results. When applied to the CARLA simulator, -PD improves CARLA-to-Waymo planner performance by 50\\%. The method is complementary to existing conditioning approaches and broadly applicable to image-to-image and video-to-video generation. Videos, additional examples, and code are available on our \\href{https://yuzeng-at-tri.github.io/ppd-page/}{project page}.",
    "arxiv_url": "https://arxiv.org/abs/2512.05106v2",
    "pdf_url": "https://arxiv.org/pdf/2512.05106v2",
    "published_date": "2025-12-04",
    "categories": [
      "cs.CV",
      "cs.GR",
      "cs.LG",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "controllable",
      "dit",
      "video generation",
      "video-to-video",
      "diffusion model",
      "simulation",
      "efficient"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.05106v2",
      "pdf": "https://arxiv.org/pdf/2512.05106v2",
      "project": "https://yuzeng-at-tri.github.io/ppd-page"
    },
    "bibtex": ""
  },
  {
    "title": "TV2TV: A Unified Framework for Interleaved Language and Video Generation",
    "authors": [
      "Xiaochuang Han",
      "Youssef Emad",
      "Melissa Hall",
      "John Nguyen",
      "Karthik Padthe",
      "Liam Robbins",
      "Amir Bar",
      "Delong Chen",
      "Michal Drozdzal",
      "Maha Elbayad",
      "Yushi Hu",
      "Shang-Wen Li",
      "Sreya Dutta Roy",
      "Jakob Verbeek",
      "XuDong Wang",
      "Marjan Ghazvininejad",
      "Luke Zettlemoyer",
      "Emily Dinan"
    ],
    "abstract": "Video generation models are rapidly advancing, but can still struggle with complex video outputs that require significant semantic branching or repeated high-level reasoning about what should happen next. In this paper, we introduce a new class of omni video-text models that integrate ideas from recent LM reasoning advances to address this challenge. More specifically, we present TV2TV, a unified generative modeling framework which decomposes video generation into an interleaved text and video generation process. TV2TV jointly learns language modeling (next-token prediction) and video flow matching (next-frame prediction) using a Mixture-of-Transformers (MoT) architecture. At inference time, TV2TV decides when to alternate between generating text and video frames, allowing the model to \"think in words\" about subsequent content before ``acting in pixels'' to produce frames. This design offloads much of the responsibility for deciding what should happen next to the language modeling tower, enabling improved visual quality and prompt alignment of generated videos. It also enables fine-grained controllability, allowing users to modify the video generation trajectory through text interventions at any point in the process. In controlled experiments on video game data, TV2TV demonstrates substantial improvements in both visual quality and controllability. TV2TV also scales to natural videos, as we show by augmenting sports videos with interleaved natural language action descriptions using vision-language models (VLMs). Training TV2TV on this corpus yields strong visual quality and prompt alignment, showcasing the model's ability to reason about and generate complex real-world action sequences. Together, these results highlight TV2TV as a promising step toward video generation with open-ended textual reasoning and control.",
    "arxiv_url": "https://arxiv.org/abs/2512.05103v2",
    "pdf_url": "https://arxiv.org/pdf/2512.05103v2",
    "published_date": "2025-12-04",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "architecture",
      "trajectory",
      "video generation",
      "flow matching"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.05103v2",
      "pdf": "https://arxiv.org/pdf/2512.05103v2"
    },
    "bibtex": ""
  },
  {
    "title": "From Generated Human Videos to Physically Plausible Robot Trajectories",
    "authors": [
      "James Ni",
      "Zekai Wang",
      "Wei Lin",
      "Amir Bar",
      "Yann LeCun",
      "Trevor Darrell",
      "Jitendra Malik",
      "Roei Herzig"
    ],
    "abstract": "Video generation models are rapidly improving in their ability to synthesize human actions in novel contexts, holding the potential to serve as high-level planners for contextual robot control. To realize this potential, a key research question remains open: how can a humanoid execute the human actions from generated videos in a zero-shot manner? This challenge arises because generated videos are often noisy and exhibit morphological distortions that make direct imitation difficult compared to real video. To address this, we introduce a two-stage pipeline. First, we lift video pixels into a 4D human representation and then retarget to the humanoid morphology. Second, we propose GenMimic-a physics-aware reinforcement learning policy conditioned on 3D keypoints, and trained with symmetry regularization and keypoint-weighted tracking rewards. As a result, GenMimic can mimic human actions from noisy, generated videos. We curate GenMimicBench, a synthetic human-motion dataset generated using two video generation models across a spectrum of actions and contexts, establishing a benchmark for assessing zero-shot generalization and policy robustness. Extensive experiments demonstrate improvements over strong baselines in simulation and confirm coherent, physically stable motion tracking on a Unitree G1 humanoid robot without fine-tuning. This work offers a promising path to realizing the potential of video generation models as high-level policies for robot control.",
    "arxiv_url": "https://arxiv.org/abs/2512.05094v2",
    "pdf_url": "https://arxiv.org/pdf/2512.05094v2",
    "published_date": "2025-12-04",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "dit",
      "physical",
      "video generation",
      "simulation",
      "physics-aware",
      "benchmark",
      "physics"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.05094v2",
      "pdf": "https://arxiv.org/pdf/2512.05094v2"
    },
    "bibtex": ""
  },
  {
    "title": "Deep Forcing: Training-Free Long Video Generation with Deep Sink and Participative Compression",
    "authors": [
      "Jung Yi",
      "Wooseok Jang",
      "Paul Hyunbin Cho",
      "Jisu Nam",
      "Heeji Yoon",
      "Seungryong Kim"
    ],
    "abstract": "Recent advances in autoregressive video diffusion have enabled real-time frame streaming, yet existing solutions still suffer from temporal repetition, drift, and motion deceleration. We find that naively applying StreamingLLM-style attention sinks to video diffusion leads to fidelity degradation and motion stagnation. To overcome this, we introduce Deep Forcing, which consists of two training-free mechanisms that address this without any fine-tuning. Specifically, 1) Deep Sink dedicates half of the sliding window to persistent sink tokens and re-aligns their temporal RoPE phase to the current timeline, stabilizing global context during long rollouts. 2) Participative Compression performs importance-aware KV cache pruning that preserves only tokens actively participating in recent attention while safely discarding redundant and degraded history, minimizing error accumulation under out-of-distribution length generation. Together, these components enable over 12x extrapolation (e.g. 5s-trained to 60s+ generation) with better imaging quality than LongLive, better aesthetic quality than RollingForcing, almost maintaining overall consistency, and substantial gains in dynamic degree, all while maintaining real-time generation. Our results demonstrate that training-free KV-cache management can match or exceed training-based approaches for autoregressively streaming long-video generation.",
    "arxiv_url": "https://arxiv.org/abs/2512.05081v1",
    "pdf_url": "https://arxiv.org/pdf/2512.05081v1",
    "published_date": "2025-12-04",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "video diffusion",
      "streaming",
      "video generation",
      "long video",
      "autoregressive",
      "style"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.05081v1",
      "pdf": "https://arxiv.org/pdf/2512.05081v1"
    },
    "bibtex": ""
  },
  {
    "title": "BulletTime: Decoupled Control of Time and Camera Pose for Video Generation",
    "authors": [
      "Yiming Wang",
      "Qihang Zhang",
      "Shengqu Cai",
      "Tong Wu",
      "Jan Ackermann",
      "Zhengfei Kuang",
      "Yang Zheng",
      "Frano Raji",
      "Siyu Tang",
      "Gordon Wetzstein"
    ],
    "abstract": "Emerging video diffusion models achieve high visual fidelity but fundamentally couple scene dynamics with camera motion, limiting their ability to provide precise spatial and temporal control. We introduce a 4D-controllable video diffusion framework that explicitly decouples scene dynamics from camera pose, enabling fine-grained manipulation of both scene dynamics and camera viewpoint. Our framework takes continuous world-time sequences and camera trajectories as conditioning inputs, injecting them into the video diffusion model through a 4D positional encoding in the attention layer and adaptive normalizations for feature modulation. To train this model, we curate a unique dataset in which temporal and camera variations are independently parameterized; this dataset will be made public. Experiments show that our model achieves robust real-world 4D control across diverse timing patterns and camera trajectories, while preserving high generation quality and outperforming prior work in controllability. See our website for video results: https://19reborn.github.io/Bullet4D/",
    "arxiv_url": "https://arxiv.org/abs/2512.05076v1",
    "pdf_url": "https://arxiv.org/pdf/2512.05076v1",
    "published_date": "2025-12-04",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "controllable",
      "video diffusion",
      "dit",
      "video generation",
      "diffusion model",
      "dynamics"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.05076v1",
      "pdf": "https://arxiv.org/pdf/2512.05076v1",
      "project": "https://19reborn.github.io/Bullet4D"
    },
    "bibtex": ""
  },
  {
    "title": "Generative Neural Video Compression via Video Diffusion Prior",
    "authors": [
      "Qi Mao",
      "Hao Cheng",
      "Tinghan Yang",
      "Libiao Jin",
      "Siwei Ma"
    ],
    "abstract": "We present GNVC-VD, the first DiT-based generative neural video compression framework built upon an advanced video generation foundation model, where spatio-temporal latent compression and sequence-level generative refinement are unified within a single codec. Existing perceptual codecs primarily rely on pre-trained image generative priors to restore high-frequency details, but their frame-wise nature lacks temporal modeling and inevitably leads to perceptual flickering. To address this, GNVC-VD introduces a unified flow-matching latent refinement module that leverages a video diffusion transformer to jointly enhance intra- and inter-frame latents through sequence-level denoising, ensuring consistent spatio-temporal details. Instead of denoising from pure Gaussian noise as in video generation, GNVC-VD initializes refinement from decoded spatio-temporal latents and learns a correction term that adapts the diffusion prior to compression-induced degradation. A conditioning adaptor further injects compression-aware cues into intermediate DiT layers, enabling effective artifact removal while maintaining temporal coherence under extreme bitrate constraints. Extensive experiments show that GNVC-VD surpasses both traditional and learned codecs in perceptual quality and significantly reduces the flickering artifacts that persist in prior generative approaches, even below 0.01 bpp, highlighting the promise of integrating video-native generative priors into neural codecs for next-generation perceptual video compression.",
    "arxiv_url": "https://arxiv.org/abs/2512.05016v1",
    "pdf_url": "https://arxiv.org/pdf/2512.05016v1",
    "published_date": "2025-12-04",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "video diffusion",
      "dit",
      "diffusion transformer",
      "video generation",
      "denoising"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.05016v1",
      "pdf": "https://arxiv.org/pdf/2512.05016v1"
    },
    "bibtex": ""
  },
  {
    "title": "Reward Forcing: Efficient Streaming Video Generation with Rewarded Distribution Matching Distillation",
    "authors": [
      "Yunhong Lu",
      "Yanhong Zeng",
      "Haobo Li",
      "Hao Ouyang",
      "Qiuyu Wang",
      "Ka Leong Cheng",
      "Jiapeng Zhu",
      "Hengyuan Cao",
      "Zhipeng Zhang",
      "Xing Zhu",
      "Yujun Shen",
      "Min Zhang"
    ],
    "abstract": "Efficient streaming video generation is critical for simulating interactive and dynamic worlds. Existing methods distill few-step video diffusion models with sliding window attention, using initial frames as sink tokens to maintain attention performance and reduce error accumulation. However, video frames become overly dependent on these static tokens, resulting in copied initial frames and diminished motion dynamics. To address this, we introduce Reward Forcing, a novel framework with two key designs. First, we propose EMA-Sink, which maintains fixed-size tokens initialized from initial frames and continuously updated by fusing evicted tokens via exponential moving average as they exit the sliding window. Without additional computation cost, EMA-Sink tokens capture both long-term context and recent dynamics, preventing initial frame copying while maintaining long-horizon consistency. Second, to better distill motion dynamics from teacher models, we propose a novel Rewarded Distribution Matching Distillation (Re-DMD). Vanilla distribution matching treats every training sample equally, limiting the model's ability to prioritize dynamic content. Instead, Re-DMD biases the model's output distribution toward high-reward regions by prioritizing samples with greater dynamics rated by a vision-language model. Re-DMD significantly enhances motion quality while preserving data fidelity. We include both quantitative and qualitative experiments to show that Reward Forcing achieves state-of-the-art performance on standard benchmarks while enabling high-quality streaming video generation at 23.1 FPS on a single H100 GPU.",
    "arxiv_url": "https://arxiv.org/abs/2512.04678v2",
    "pdf_url": "https://arxiv.org/pdf/2512.04678v2",
    "published_date": "2025-12-04",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "interactive",
      "video diffusion",
      "distillation",
      "streaming",
      "dit",
      "video generation",
      "efficient",
      "diffusion model",
      "dynamics",
      "benchmark"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.04678v2",
      "pdf": "https://arxiv.org/pdf/2512.04678v2"
    },
    "bibtex": ""
  },
  {
    "title": "Live Avatar: Streaming Real-time Audio-Driven Avatar Generation with Infinite Length",
    "authors": [
      "Yubo Huang",
      "Hailong Guo",
      "Fangtai Wu",
      "Shifeng Zhang",
      "Shijie Huang",
      "Qijun Gan",
      "Lin Liu",
      "Sirui Zhao",
      "Enhong Chen",
      "Jiaming Liu",
      "Steven Hoi"
    ],
    "abstract": "Existing diffusion-based video generation methods are fundamentally constrained by sequential computation and long-horizon inconsistency, limiting their practical adoption in real-time, streaming audio-driven avatar synthesis. We present Live Avatar, an algorithm-system co-designed framework that enables efficient, high-fidelity, and infinite-length avatar generation using a 14-billion-parameter diffusion model. Our approach introduces Timestep-forcing Pipeline Parallelism (TPP), a distributed inference paradigm that pipelines denoising steps across multiple GPUs, effectively breaking the autoregressive bottleneck and ensuring stable, low-latency real-time streaming. To further enhance temporal consistency and mitigate identity drift and color artifacts, we propose the Rolling Sink Frame Mechanism (RSFM), which maintains sequence fidelity by dynamically recalibrating appearance using a cached reference image. Additionally, we leverage Self-Forcing Distribution Matching Distillation to facilitate causal, streamable adaptation of large-scale models without sacrificing visual quality. Live Avatar demonstrates state-of-the-art performance, reaching 20 FPS end-to-end generation on 5 H800 GPUs, and, to the best of our knowledge, is the first to achieve practical, real-time, high-fidelity avatar generation at this scale. Our work establishes a new paradigm for deploying advanced diffusion models in industrial long-form video synthesis applications.",
    "arxiv_url": "https://arxiv.org/abs/2512.04677v3",
    "pdf_url": "https://arxiv.org/pdf/2512.04677v3",
    "published_date": "2025-12-04",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "temporal consistency",
      "distillation",
      "streaming",
      "dit",
      "identity",
      "long-form",
      "video generation",
      "video synthesis",
      "diffusion model",
      "denoising",
      "autoregressive",
      "efficient",
      "avatar",
      "audio-driven"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.04677v3",
      "pdf": "https://arxiv.org/pdf/2512.04677v3"
    },
    "bibtex": ""
  },
  {
    "title": "DreamFoley: Scalable VLMs for High-Fidelity Video-to-Audio Generation",
    "authors": [
      "Fu Li",
      "Weichao Zhao",
      "You Li",
      "Zhichao Zhou",
      "Dongliang He"
    ],
    "abstract": "Recent advances in video generation have achieved remarkable improvements in visual content fidelity. However, the absence of synchronized audio severely undermines immersive experience and restricts practical applications of these technologies. To address this challenge, several pioneering works have explored diffusion transformer architectures for generating plausible video-synchronized audio, including Kling-foley, HunyuanVideo-foley and Thinksound. Distinct from existing works, we introduce an autoregressive audio generation architecture (DreamFoley) that harnesses the capabilities of large vision-language models (VLMs) to jointly model sequential interactions among video, audio, and text modalities. Our approach features a dual-visual encoder module that effectively captures both audio-aligned and text-aligned visual features. Additionally, we employ a Residual Vector Quantization audio tokenizer with a delay-pattern generation scheme to balance the trade-off between training efficiency and audio quality. Moreover, we introduce the classifier-free guidance strategy into VLMs to bootstrap generated audio quality. Furthermore, we establish an efficient data production pipeline to scale audio-video-text triple collection. Finally, extensive experiments are conducted to validate the effectiveness of our model, achieving promising performance across popular benchmarks. We hope that the findings in this study provide a strong foundation for future video-to-audio generation research. We also release the previously missing audio-visual textual descriptions from the public benchmark, aiming to facilitate subsequent researchers in conducting more convenient and effective evaluations and comparisons.",
    "arxiv_url": "https://arxiv.org/abs/2512.06022v1",
    "pdf_url": "https://arxiv.org/pdf/2512.06022v1",
    "published_date": "2025-12-04",
    "categories": [
      "cs.SD",
      "cs.MM"
    ],
    "github_url": "",
    "keywords": [
      "dit",
      "diffusion transformer",
      "video generation",
      "efficient",
      "evaluation",
      "architecture",
      "sound",
      "autoregressive",
      "benchmark"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.06022v1",
      "pdf": "https://arxiv.org/pdf/2512.06022v1"
    },
    "bibtex": ""
  },
  {
    "title": "Denoise to Track: Harnessing Video Diffusion Priors for Robust Correspondence",
    "authors": [
      "Tianyu Yuan",
      "Yuanbo Yang",
      "Lin-Zhuo Chen",
      "Yao Yao",
      "Zhuzhong Qian"
    ],
    "abstract": "In this work, we introduce HeFT (Head-Frequency Tracker), a zero-shot point tracking framework that leverages the visual priors of pretrained video diffusion models. To better understand how they encode spatiotemporal information, we analyze the internal representations of Video Diffusion Transformer (VDiT). Our analysis reveals that attention heads act as minimal functional units with distinct specializations for matching, semantic understanding, and positional encoding. Additionally, we find that the low-frequency components in VDiT features are crucial for establishing correspondences, whereas the high-frequency components tend to introduce noise. Building on these insights, we propose a head- and frequency-aware feature selection strategy that jointly selects the most informative attention head and low-frequency components to enhance tracking performance. Specifically, our method extracts discriminative features through single-step denoising, applies feature selection, and employs soft-argmax localization with forward-backward consistency checks for correspondence estimation. Extensive experiments on TAP-Vid benchmarks demonstrate that HeFT achieves state-of-the-art zero-shot tracking performance, approaching the accuracy of supervised methods while eliminating the need for annotated training data. Our work further underscores the promise of video diffusion models as powerful foundation models for a wide range of downstream tasks, paving the way toward unified visual foundation models.",
    "arxiv_url": "https://arxiv.org/abs/2512.04619v1",
    "pdf_url": "https://arxiv.org/pdf/2512.04619v1",
    "published_date": "2025-12-04",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "video diffusion",
      "dit",
      "diffusion transformer",
      "diffusion model",
      "denoising",
      "benchmark"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.04619v1",
      "pdf": "https://arxiv.org/pdf/2512.04619v1"
    },
    "bibtex": ""
  },
  {
    "title": "X-Humanoid: Robotize Human Videos to Generate Humanoid Videos at Scale",
    "authors": [
      "Pei Yang",
      "Hai Ci",
      "Yiren Song",
      "Mike Zheng Shou"
    ],
    "abstract": "The advancement of embodied AI has unlocked significant potential for intelligent humanoid robots. However, progress in both Vision-Language-Action (VLA) models and world models is severely hampered by the scarcity of large-scale, diverse training data. A promising solution is to \"robotize\" web-scale human videos, which has been proven effective for policy training. However, these solutions mainly \"overlay\" robot arms to egocentric videos, which cannot handle complex full-body motions and scene occlusions in third-person videos, making them unsuitable for robotizing humans. To bridge this gap, we introduce X-Humanoid, a generative video editing approach that adapts the powerful Wan 2.2 model into a video-to-video structure and finetunes it for the human-to-humanoid translation task. This finetuning requires paired human-humanoid videos, so we designed a scalable data creation pipeline, turning community assets into 17+ hours of paired synthetic videos using Unreal Engine. We then apply our trained model to 60 hours of the Ego-Exo4D videos, generating and releasing a new large-scale dataset of over 3.6 million \"robotized\" humanoid video frames. Quantitative analysis and user studies confirm our method's superiority over existing baselines: 69% of users rated it best for motion consistency, and 62.1% for embodiment correctness.",
    "arxiv_url": "https://arxiv.org/abs/2512.04537v1",
    "pdf_url": "https://arxiv.org/pdf/2512.04537v1",
    "published_date": "2025-12-04",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "dit",
      "video editing",
      "world model",
      "video-to-video",
      "body motion"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.04537v1",
      "pdf": "https://arxiv.org/pdf/2512.04537v1"
    },
    "bibtex": ""
  },
  {
    "title": "Refaade: Editing Object with Given Reference Texture",
    "authors": [
      "Youze Huang",
      "Penghui Ruan",
      "Bojia Zi",
      "Xianbiao Qi",
      "Jianan Wang",
      "Rong Xiao"
    ],
    "abstract": "Recent advances in diffusion models have brought remarkable progress in image and video editing, yet some tasks remain underexplored. In this paper, we introduce a new task, Object Retexture, which transfers local textures from a reference object to a target object in images or videos. To perform this task, a straightforward solution is to use ControlNet conditioned on the source structure and the reference texture. However, this approach suffers from limited controllability for two reasons: conditioning on the raw reference image introduces unwanted structural information, and it fails to disentangle the visual texture and structure information of the source. To address this problem, we propose Refaade, a method that consists of two key designs to achieve precise and controllable texture transfer in both images and videos. First, we employ a texture remover trained on paired textured/untextured 3D mesh renderings to remove appearance information while preserving the geometry and motion of source videos. Second, we disrupt the reference global layout using a jigsaw permutation, encouraging the model to focus on local texture statistics rather than the global layout of the object. Extensive experiments demonstrate superior visual quality, precise editing, and controllability, outperforming strong baselines in both quantitative and human evaluations. Code is available at https://github.com/fishZe233/Refacade.",
    "arxiv_url": "https://arxiv.org/abs/2512.04534v1",
    "pdf_url": "https://arxiv.org/pdf/2512.04534v1",
    "published_date": "2025-12-04",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/fishZe233/Refacade",
    "keywords": [
      "layout",
      "controllable",
      "dit",
      "video editing",
      "evaluation",
      "diffusion model"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.04534v1",
      "pdf": "https://arxiv.org/pdf/2512.04534v1",
      "github": "https://github.com/fishZe233/Refacade"
    },
    "bibtex": ""
  },
  {
    "title": "VideoSSM: Autoregressive Long Video Generation with Hybrid State-Space Memory",
    "authors": [
      "Yifei Yu",
      "Xiaoshan Wu",
      "Xinting Hu",
      "Tao Hu",
      "Yangtian Sun",
      "Xiaoyang Lyu",
      "Bo Wang",
      "Lin Ma",
      "Yuewen Ma",
      "Zhongrui Wang",
      "Xiaojuan Qi"
    ],
    "abstract": "Autoregressive (AR) diffusion enables streaming, interactive long-video generation by producing frames causally, yet maintaining coherence over minute-scale horizons remains challenging due to accumulated errors, motion drift, and content repetition. We approach this problem from a memory perspective, treating video synthesis as a recurrent dynamical process that requires coordinated short- and long-term context. We propose VideoSSM, a Long Video Model that unifies AR diffusion with a hybrid state-space memory. The state-space model (SSM) serves as an evolving global memory of scene dynamics across the entire sequence, while a context window provides local memory for motion cues and fine details. This hybrid design preserves global consistency without frozen, repetitive patterns, supports prompt-adaptive interaction, and scales in linear time with sequence length. Experiments on short- and long-range benchmarks demonstrate state-of-the-art temporal consistency and motion stability among autoregressive video generator especially at minute-scale horizons, enabling content diversity and interactive prompt-based control, thereby establishing a scalable, memory-aware framework for long video generation.",
    "arxiv_url": "https://arxiv.org/abs/2512.04519v1",
    "pdf_url": "https://arxiv.org/pdf/2512.04519v1",
    "published_date": "2025-12-04",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "interactive",
      "temporal consistency",
      "streaming",
      "video generation",
      "video synthesis",
      "dynamics",
      "long video",
      "autoregressive",
      "benchmark"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.04519v1",
      "pdf": "https://arxiv.org/pdf/2512.04519v1"
    },
    "bibtex": ""
  },
  {
    "title": "EgoLCD: Egocentric Video Generation with Long Context Diffusion",
    "authors": [
      "Liuzhou Zhang",
      "Jiarui Ye",
      "Yuanlei Wang",
      "Ming Zhong",
      "Mingju Cao",
      "Wanke Xia",
      "Bowen Zeng",
      "Zeyu Zhang",
      "Hao Tang"
    ],
    "abstract": "Generating long, coherent egocentric videos is difficult, as hand-object interactions and procedural tasks require reliable long-term memory. Existing autoregressive models suffer from content drift, where object identity and scene semantics degrade over time. To address this challenge, we introduce EgoLCD, an end-to-end framework for egocentric long-context video generation that treats long video synthesis as a problem of efficient and stable memory management. EgoLCD combines a Long-Term Sparse KV Cache for stable global context with an attention-based short-term memory, extended by LoRA for local adaptation. A Memory Regulation Loss enforces consistent memory usage, and Structured Narrative Prompting provides explicit temporal guidance. Extensive experiments on the EgoVid-5M benchmark demonstrate that EgoLCD achieves state-of-the-art performance in both perceptual quality and temporal consistency, effectively mitigating generative forgetting and representing a significant step toward building scalable world models for embodied AI. Code: https://github.com/AIGeeksGroup/EgoLCD. Website: https://aigeeksgroup.github.io/EgoLCD.",
    "arxiv_url": "https://arxiv.org/abs/2512.04515v1",
    "pdf_url": "https://arxiv.org/pdf/2512.04515v1",
    "published_date": "2025-12-04",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/AIGeeksGroup/EgoLCD",
    "keywords": [
      "temporal consistency",
      "world model",
      "identity",
      "video generation",
      "efficient",
      "video synthesis",
      "long video",
      "autoregressive",
      "benchmark"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.04515v1",
      "pdf": "https://arxiv.org/pdf/2512.04515v1",
      "github": "https://github.com/AIGeeksGroup/EgoLCD",
      "project": "https://aigeeksgroup.github.io/EgoLCD"
    },
    "bibtex": ""
  },
  {
    "title": "DeRA: Decoupled Representation Alignment for Video Tokenization",
    "authors": [
      "Pengbo Guo",
      "Junke Wang",
      "Zhen Xing",
      "Chengxu Liu",
      "Daoguo Dong",
      "Xueming Qian",
      "Zuxuan Wu"
    ],
    "abstract": "This paper presents DeRA, a novel 1D video tokenizer that decouples the spatial-temporal representation learning in video tokenization to achieve better training efficiency and performance. Specifically, DeRA maintains a compact 1D latent space while factorizing video encoding into appearance and motion streams, which are aligned with pretrained vision foundation models to capture the spatial semantics and temporal dynamics in videos separately. To address the gradient conflicts introduced by the heterogeneous supervision, we further propose the Symmetric Alignment-Conflict Projection (SACP) module that proactively reformulates gradients by suppressing the components along conflicting directions. Extensive experiments demonstrate that DeRA outperforms LARP, the previous state-of-the-art video tokenizer by 25% on UCF-101 in terms of rFVD. Moreover, using DeRA for autoregressive video generation, we also achieve new state-of-the-art results on both UCF-101 class-conditional generation and K600 frame prediction.",
    "arxiv_url": "https://arxiv.org/abs/2512.04483v1",
    "pdf_url": "https://arxiv.org/pdf/2512.04483v1",
    "published_date": "2025-12-04",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "dynamics",
      "video generation",
      "autoregressive",
      "dit"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.04483v1",
      "pdf": "https://arxiv.org/pdf/2512.04483v1"
    },
    "bibtex": ""
  },
  {
    "title": "Data-regularized Reinforcement Learning for Diffusion Models at Scale",
    "authors": [
      "Haotian Ye",
      "Kaiwen Zheng",
      "Jiashu Xu",
      "Puheng Li",
      "Huayu Chen",
      "Jiaqi Han",
      "Sheng Liu",
      "Qinsheng Zhang",
      "Hanzi Mao",
      "Zekun Hao",
      "Prithvijit Chattopadhyay",
      "Dinghao Yang",
      "Liang Feng",
      "Maosheng Liao",
      "Junjie Bai",
      "Ming-Yu Liu",
      "James Zou",
      "Stefano Ermon"
    ],
    "abstract": "Aligning generative diffusion models with human preferences via reinforcement learning (RL) is critical yet challenging. Most existing algorithms are often vulnerable to reward hacking, such as quality degradation, over-stylization, or reduced diversity. Our analysis demonstrates that this can be attributed to the inherent limitations of their regularization, which provides unreliable penalties. We introduce Data-regularized Diffusion Reinforcement Learning (DDRL), a novel framework that uses the forward KL divergence to anchor the policy to an off-policy data distribution. Theoretically, DDRL enables robust, unbiased integration of RL with standard diffusion training. Empirically, this translates into a simple yet effective algorithm that combines reward maximization with diffusion loss minimization. With over a million GPU hours of experiments and ten thousand double-blind human evaluations, we demonstrate on high-resolution video generation tasks that DDRL significantly improves rewards while alleviating the reward hacking seen in baselines, achieving the highest human preference and establishing a robust and scalable paradigm for diffusion post-training.",
    "arxiv_url": "https://arxiv.org/abs/2512.04332v3",
    "pdf_url": "https://arxiv.org/pdf/2512.04332v3",
    "published_date": "2025-12-03",
    "categories": [
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "evaluation",
      "diffusion model",
      "video generation"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.04332v3",
      "pdf": "https://arxiv.org/pdf/2512.04332v3"
    },
    "bibtex": ""
  },
  {
    "title": "MoReGen: Multi-Agent Motion-Reasoning Engine for Code-based Text-to-Video Synthesis",
    "authors": [
      "Xiangyu Bai",
      "He Liang",
      "Bishoy Galoaa",
      "Utsav Nandi",
      "Shayda Moezzi",
      "Yuhang He",
      "Sarah Ostadabbas"
    ],
    "abstract": "While text-to-video (T2V) generation has achieved remarkable progress in photorealism, generating intent-aligned videos that faithfully obey physics principles remains a core challenge. In this work, we systematically study Newtonian motion-controlled text-to-video generation and evaluation, emphasizing physical precision and motion coherence. We introduce MoReGen, a motion-aware, physics-grounded T2V framework that integrates multi-agent LLMs, physics simulators, and renderers to generate reproducible, physically accurate videos from text prompts in the code domain. To quantitatively assess physical validity, we propose object-trajectory correspondence as a direct evaluation metric and present MoReSet, a benchmark of 1,275 human-annotated videos spanning nine classes of Newtonian phenomena with scene descriptions, spatiotemporal relations, and ground-truth trajectories. Using MoReSet, we conduct experiments on existing T2V models, evaluating their physical validity through both our MoRe metrics and existing physics-based evaluators. Our results reveal that state-of-the-art models struggle to maintain physical validity, while MoReGen establishes a principled direction toward physically coherent video synthesis.",
    "arxiv_url": "https://arxiv.org/abs/2512.04221v2",
    "pdf_url": "https://arxiv.org/pdf/2512.04221v2",
    "published_date": "2025-12-03",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "dit",
      "physical",
      "text-to-video",
      "video generation",
      "trajectory",
      "evaluation",
      "video synthesis",
      "t2v",
      "benchmark",
      "physics"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.04221v2",
      "pdf": "https://arxiv.org/pdf/2512.04221v2"
    },
    "bibtex": ""
  },
  {
    "title": "Beyond Flicker: Detecting Kinematic Inconsistencies for Generalizable Deepfake Video Detection",
    "authors": [
      "Alejandro Cobo",
      "Roberto Valle",
      "Jos Miguel Buenaposada",
      "Luis Baumela"
    ],
    "abstract": "Generalizing deepfake detection to unseen manipulations remains a key challenge. A recent approach to tackle this issue is to train a network with pristine face images that have been manipulated with hand-crafted artifacts to extract more generalizable clues. While effective for static images, extending this to the video domain is an open issue. Existing methods model temporal artifacts as frame-to-frame instabilities, overlooking a key vulnerability: the violation of natural motion dependencies between different facial regions. In this paper, we propose a synthetic video generation method that creates training data with subtle kinematic inconsistencies. We train an autoencoder to decompose facial landmark configurations into motion bases. By manipulating these bases, we selectively break the natural correlations in facial movements and introduce these artifacts into pristine videos via face morphing. A network trained on our data learns to spot these sophisticated biomechanical flaws, achieving state-of-the-art generalization results on several popular benchmarks.",
    "arxiv_url": "https://arxiv.org/abs/2512.04175v1",
    "pdf_url": "https://arxiv.org/pdf/2512.04175v1",
    "published_date": "2025-12-03",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "benchmark",
      "video generation"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.04175v1",
      "pdf": "https://arxiv.org/pdf/2512.04175v1"
    },
    "bibtex": ""
  },
  {
    "title": "RELIC: Interactive Video World Model with Long-Horizon Memory",
    "authors": [
      "Yicong Hong",
      "Yiqun Mei",
      "Chongjian Ge",
      "Yiran Xu",
      "Yang Zhou",
      "Sai Bi",
      "Yannick Hold-Geoffroy",
      "Mike Roberts",
      "Matthew Fisher",
      "Eli Shechtman",
      "Kalyan Sunkavalli",
      "Feng Liu",
      "Zhengqi Li",
      "Hao Tan"
    ],
    "abstract": "A truly interactive world model requires three key ingredients: real-time long-horizon streaming, consistent spatial memory, and precise user control. However, most existing approaches address only one of these aspects in isolation, as achieving all three simultaneously is highly challenging-for example, long-term memory mechanisms often degrade real-time performance. In this work, we present RELIC, a unified framework that tackles these three challenges altogether. Given a single image and a text description, RELIC enables memory-aware, long-duration exploration of arbitrary scenes in real time. Built upon recent autoregressive video-diffusion distillation techniques, our model represents long-horizon memory using highly compressed historical latent tokens encoded with both relative actions and absolute camera poses within the KV cache. This compact, camera-aware memory structure supports implicit 3D-consistent content retrieval and enforces long-term coherence with minimal computational overhead. In parallel, we fine-tune a bidirectional teacher video model to generate sequences beyond its original 5-second training horizon, and transform it into a causal student generator using a new memory-efficient self-forcing paradigm that enables full-context distillation over long-duration teacher as well as long student self-rollouts. Implemented as a 14B-parameter model and trained on a curated Unreal Engine-rendered dataset, RELIC achieves real-time generation at 16 FPS while demonstrating more accurate action following, more stable long-horizon streaming, and more robust spatial-memory retrieval compared with prior work. These capabilities establish RELIC as a strong foundation for the next generation of interactive world modeling.",
    "arxiv_url": "https://arxiv.org/abs/2512.04040v1",
    "pdf_url": "https://arxiv.org/pdf/2512.04040v1",
    "published_date": "2025-12-03",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "interactive",
      "distillation",
      "streaming",
      "world model",
      "autoregressive",
      "efficient"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.04040v1",
      "pdf": "https://arxiv.org/pdf/2512.04040v1"
    },
    "bibtex": ""
  },
  {
    "title": "Zero-Shot Video Translation and Editing with Frame Spatial-Temporal Correspondence",
    "authors": [
      "Shuai Yang",
      "Junxin Lin",
      "Yifan Zhou",
      "Ziwei Liu",
      "Chen Change Loy"
    ],
    "abstract": "The remarkable success in text-to-image diffusion models has motivated extensive investigation of their potential for video applications. Zero-shot techniques aim to adapt image diffusion models for videos without requiring further model training. Recent methods largely emphasize integrating inter-frame correspondence into attention mechanisms. However, the soft constraint applied to identify the valid features to attend is insufficient, which could lead to temporal inconsistency. In this paper, we present FRESCO, which integrates intra-frame correspondence with inter-frame correspondence to formulate a more robust spatial-temporal constraint. This enhancement ensures a consistent transformation of semantically similar content between frames. Our method goes beyond attention guidance to explicitly optimize features, achieving high spatial-temporal consistency with the input video, significantly enhancing the visual coherence of manipulated videos. We verify FRESCO adaptations on two zero-shot tasks of video-to-video translation and text-guided video editing. Comprehensive experiments demonstrate the effectiveness of our framework in generating high-quality, coherent videos, highlighting a significant advance over current zero-shot methods.",
    "arxiv_url": "https://arxiv.org/abs/2512.03905v1",
    "pdf_url": "https://arxiv.org/pdf/2512.03905v1",
    "published_date": "2025-12-03",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "temporal consistency",
      "dit",
      "video editing",
      "video-to-video",
      "diffusion model",
      "video translation"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.03905v1",
      "pdf": "https://arxiv.org/pdf/2512.03905v1"
    },
    "bibtex": ""
  },
  {
    "title": "ReCamDriving: LiDAR-Free Camera-Controlled Novel Trajectory Video Generation",
    "authors": [
      "Yaokun Li",
      "Shuaixian Wang",
      "Mantang Guo",
      "Jiehui Huang",
      "Taojun Ding",
      "Mu Hu",
      "Kaixuan Wang",
      "Shaojie Shen",
      "Guang Tan"
    ],
    "abstract": "We propose ReCamDriving, a purely vision-based, camera-controlled novel-trajectory video generation framework. While repair-based methods fail to restore complex artifacts and LiDAR-based approaches rely on sparse and incomplete cues, ReCamDriving leverages dense and scene-complete 3DGS renderings for explicit geometric guidance, achieving precise camera-controllable generation. To mitigate overfitting to restoration behaviors when conditioned on 3DGS renderings, ReCamDriving adopts a two-stage training paradigm: the first stage uses camera poses for coarse control, while the second stage incorporates 3DGS renderings for fine-grained viewpoint and geometric guidance. Furthermore, we present a 3DGS-based cross-trajectory data curation strategy to eliminate the train-test gap in camera transformation patterns, enabling scalable multi-trajectory supervision from monocular videos. Based on this strategy, we construct the ParaDrive dataset, containing over 110K parallel-trajectory video pairs. Extensive experiments demonstrate that ReCamDriving achieves state-of-the-art camera controllability and structural consistency.",
    "arxiv_url": "https://arxiv.org/abs/2512.03621v2",
    "pdf_url": "https://arxiv.org/pdf/2512.03621v2",
    "published_date": "2025-12-03",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "controllable",
      "dit",
      "trajectory",
      "video generation",
      "camera control"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.03621v2",
      "pdf": "https://arxiv.org/pdf/2512.03621v2"
    },
    "bibtex": ""
  },
  {
    "title": "LAMP: Language-Assisted Motion Planning for Controllable Video Generation",
    "authors": [
      "Muhammed Burak Kizil",
      "Enes Sanli",
      "Niloy J. Mitra",
      "Erkut Erdem",
      "Aykut Erdem",
      "Duygu Ceylan"
    ],
    "abstract": "Video generation has achieved remarkable progress in visual fidelity and controllability, enabling conditioning on text, layout, or motion. Among these, motion control - specifying object dynamics and camera trajectories - is essential for composing complex, cinematic scenes, yet existing interfaces remain limited. We introduce LAMP that leverages large language models (LLMs) as motion planners to translate natural language descriptions into explicit 3D trajectories for dynamic objects and (relatively defined) cameras. LAMP defines a motion domain-specific language (DSL), inspired by cinematography conventions. By harnessing program synthesis capabilities of LLMs, LAMP generates structured motion programs from natural language, which are deterministically mapped to 3D trajectories. We construct a large-scale procedural dataset pairing natural text descriptions with corresponding motion programs and 3D trajectories. Experiments demonstrate LAMP's improved performance in motion controllability and alignment with user intent compared to state-of-the-art alternatives establishing the first framework for generating both object and camera motions directly from natural language specifications. Code, models and data are available on our project page.",
    "arxiv_url": "https://arxiv.org/abs/2512.03619v2",
    "pdf_url": "https://arxiv.org/pdf/2512.03619v2",
    "published_date": "2025-12-03",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "layout",
      "controllable",
      "dit",
      "video generation",
      "dynamics",
      "motion control"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.03619v2",
      "pdf": "https://arxiv.org/pdf/2512.03619v2"
    },
    "bibtex": ""
  },
  {
    "title": "Beyond Boundary Frames: Audio-Visual Semantic Guidance for Context-Aware Video Interpolation",
    "authors": [
      "Yuchen Deng",
      "Xiuyang Wu",
      "Hai-Tao Zheng",
      "Jie Wang",
      "Feidiao Yang",
      "Yuxing Han"
    ],
    "abstract": "Handling fast, complex, and highly non-linear motion patterns has long posed challenges for video frame interpolation. Although recent diffusion-based approaches improve upon traditional optical-flow-based methods, they still struggle to cover diverse application scenarios and often fail to produce sharp, temporally consistent frames in fine-grained motion tasks such as audio-visual synchronized interpolation. To address these limitations, we introduce BBF (Beyond Boundary Frames), a context-aware video frame interpolation framework, which could be guided by audio/visual semantics. First, we enhance the input design of the interpolation model so that it can flexibly handle multiple conditional modalities, including text, audio, images, and video. Second, we propose a decoupled multimodal fusion mechanism that sequentially injects different conditional signals into a DiT backbone. Finally, to maintain the generation abilities of the foundation model, we adopt a progressive multi-stage training paradigm, where the start-end frame difference embedding is used to dynamically adjust both the data sampling and the loss weighting. Extensive experimental results demonstrate that BBF outperforms specialized state-of-the-art methods on both generic interpolation and audio-visual synchronized interpolation tasks, establishing a unified framework for video frame interpolation under coordinated multi-channel conditioning.",
    "arxiv_url": "https://arxiv.org/abs/2512.03590v1",
    "pdf_url": "https://arxiv.org/pdf/2512.03590v1",
    "published_date": "2025-12-03",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "frame interpolation",
      "video interpolation",
      "dit"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.03590v1",
      "pdf": "https://arxiv.org/pdf/2512.03590v1"
    },
    "bibtex": ""
  },
  {
    "title": "Rethinking Prompt Design for Inference-time Scaling in Text-to-Visual Generation",
    "authors": [
      "Subin Kim",
      "Sangwoo Mo",
      "Mamshad Nayeem Rizve",
      "Yiran Xu",
      "Difan Liu",
      "Jinwoo Shin",
      "Tobias Hinz"
    ],
    "abstract": "Achieving precise alignment between user intent and generated visuals remains a central challenge in text-to-visual generation, as a single attempt often fails to produce the desired output. To handle this, prior approaches mainly scale the visual generation process (e.g., increasing sampling steps or seeds), but this quickly leads to a quality plateau. This limitation arises because the prompt, crucial for guiding generation, is kept fixed. To address this, we propose Prompt Redesign for Inference-time Scaling, coined PRIS, a framework that adaptively revises the prompt during inference in response to the scaled visual generations. The core idea of PRIS is to review the generated visuals, identify recurring failure patterns across visuals, and redesign the prompt accordingly before regenerating the visuals with the revised prompt. To provide precise alignment feedback for prompt revision, we introduce a new verifier, element-level factual correction, which evaluates the alignment between prompt attributes and generated visuals at a fine-grained level, achieving more accurate and interpretable assessments than holistic measures. Extensive experiments on both text-to-image and text-to-video benchmarks demonstrate the effectiveness of our approach, including a 15% gain on VBench 2.0. These results highlight that jointly scaling prompts and visuals is key to fully leveraging scaling laws at inference-time. Visualizations are available at the website: https://subin-kim-cv.github.io/PRIS.",
    "arxiv_url": "https://arxiv.org/abs/2512.03534v1",
    "pdf_url": "https://arxiv.org/pdf/2512.03534v1",
    "published_date": "2025-12-03",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "benchmark",
      "text-to-video"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.03534v1",
      "pdf": "https://arxiv.org/pdf/2512.03534v1",
      "project": "https://subin-kim-cv.github.io/PRIS"
    },
    "bibtex": ""
  },
  {
    "title": "TwinFlow: Realizing One-step Generation on Large Models with Self-adversarial Flows",
    "authors": [
      "Zhenglin Cheng",
      "Peng Sun",
      "Jianguo Li",
      "Tao Lin"
    ],
    "abstract": "Recent advances in large multi-modal generative models have demonstrated impressive capabilities in multi-modal generation, including image and video generation. These models are typically built upon multi-step frameworks like diffusion and flow matching, which inherently limits their inference efficiency (requiring 40-100 Number of Function Evaluations (NFEs)). While various few-step methods aim to accelerate the inference, existing solutions have clear limitations. Prominent distillation-based methods, such as progressive and consistency distillation, either require an iterative distillation procedure or show significant degradation at very few steps (< 4-NFE). Meanwhile, integrating adversarial training into distillation (e.g., DMD/DMD2 and SANA-Sprint) to enhance performance introduces training instability, added complexity, and high GPU memory overhead due to the auxiliary trained models. To this end, we propose TwinFlow, a simple yet effective framework for training 1-step generative models that bypasses the need of fixed pretrained teacher models and avoids standard adversarial networks during training, making it ideal for building large-scale, efficient models. On text-to-image tasks, our method achieves a GenEval score of 0.83 in 1-NFE, outperforming strong baselines like SANA-Sprint (a GAN loss-based framework) and RCGM (a consistency-based framework). Notably, we demonstrate the scalability of TwinFlow by full-parameter training on Qwen-Image-20B and transform it into an efficient few-step generator. With just 1-NFE, our approach matches the performance of the original 100-NFE model on both the GenEval and DPG-Bench benchmarks, reducing computational cost by $100\\times$ with minor quality degradation. Project page is available at https://zhenglin-cheng.com/twinflow.",
    "arxiv_url": "https://arxiv.org/abs/2512.05150v2",
    "pdf_url": "https://arxiv.org/pdf/2512.05150v2",
    "published_date": "2025-12-03",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "multi-modal",
      "distillation",
      "flow matching",
      "video generation",
      "efficient",
      "evaluation",
      "benchmark"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.05150v2",
      "pdf": "https://arxiv.org/pdf/2512.05150v2",
      "project": "https://zhenglin-cheng.com/twinflow"
    },
    "bibtex": ""
  },
  {
    "title": "GeoVideo: Introducing Geometric Regularization into Video Generation Model",
    "authors": [
      "Yunpeng Bai",
      "Shaoheng Fang",
      "Chaohui Yu",
      "Fan Wang",
      "Qixing Huang"
    ],
    "abstract": "Recent advances in video generation have enabled the synthesis of high-quality and visually realistic clips using diffusion transformer models. However, most existing approaches operate purely in the 2D pixel space and lack explicit mechanisms for modeling 3D structures, often resulting in temporally inconsistent geometries, implausible motions, and structural artifacts. In this work, we introduce geometric regularization losses into video generation by augmenting latent diffusion models with per-frame depth prediction. We adopted depth as the geometric representation because of the great progress in depth prediction and its compatibility with image-based latent encoders. Specifically, to enforce structural consistency over time, we propose a multi-view geometric loss that aligns the predicted depth maps across frames within a shared 3D coordinate system. Our method bridges the gap between appearance generation and 3D structure modeling, leading to improved spatio-temporal coherence, shape consistency, and physical plausibility. Experiments across multiple datasets show that our approach produces significantly more stable and geometrically consistent results than existing baselines.",
    "arxiv_url": "https://arxiv.org/abs/2512.03453v1",
    "pdf_url": "https://arxiv.org/pdf/2512.03453v1",
    "published_date": "2025-12-03",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "diffusion model",
      "diffusion transformer",
      "physical",
      "video generation"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.03453v1",
      "pdf": "https://arxiv.org/pdf/2512.03453v1"
    },
    "bibtex": ""
  },
  {
    "title": "GalaxyDiT: Efficient Video Generation with Guidance Alignment and Adaptive Proxy in Diffusion Transformers",
    "authors": [
      "Zhiye Song",
      "Steve Dai",
      "Ben Keller",
      "Brucek Khailany"
    ],
    "abstract": "Diffusion models have revolutionized video generation, becoming essential tools in creative content generation and physical simulation. Transformer-based architectures (DiTs) and classifier-free guidance (CFG) are two cornerstones of this success, enabling strong prompt adherence and realistic video quality. Despite their versatility and superior performance, these models require intensive computation. Each video generation requires dozens of iterative steps, and CFG doubles the required compute. This inefficiency hinders broader adoption in downstream applications.   We introduce GalaxyDiT, a training-free method to accelerate video generation with guidance alignment and systematic proxy selection for reuse metrics. Through rank-order correlation analysis, our technique identifies the optimal proxy for each video model, across model families and parameter scales, thereby ensuring optimal computational reuse. We achieve $1.87\\times$ and $2.37\\times$ speedup on Wan2.1-1.3B and Wan2.1-14B with only 0.97% and 0.72% drops on the VBench-2.0 benchmark. At high speedup rates, our approach maintains superior fidelity to the base model, exceeding prior state-of-the-art approaches by 5 to 10 dB in peak signal-to-noise ratio (PSNR).",
    "arxiv_url": "https://arxiv.org/abs/2512.03451v1",
    "pdf_url": "https://arxiv.org/pdf/2512.03451v1",
    "published_date": "2025-12-03",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "creative",
      "physical simulation",
      "dit",
      "diffusion transformer",
      "physical",
      "video generation",
      "efficient",
      "diffusion model",
      "simulation",
      "architecture",
      "benchmark"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.03451v1",
      "pdf": "https://arxiv.org/pdf/2512.03451v1"
    },
    "bibtex": ""
  },
  {
    "title": "SeeU: Seeing the Unseen World via 4D Dynamics-aware Generation",
    "authors": [
      "Yu Yuan",
      "Tharindu Wickremasinghe",
      "Zeeshan Nadir",
      "Xijun Wang",
      "Yiheng Chi",
      "Stanley H. Chan"
    ],
    "abstract": "Images and videos are discrete 2D projections of the 4D world (3D space + time). Most visual understanding, prediction, and generation operate directly on 2D observations, leading to suboptimal performance. We propose SeeU, a novel approach that learns the continuous 4D dynamics and generate the unseen visual contents. The principle behind SeeU is a new 2D$\\to$4D$\\to$2D learning framework. SeeU first reconstructs the 4D world from sparse and monocular 2D frames (2D$\\to$4D). It then learns the continuous 4D dynamics on a low-rank representation and physical constraints (discrete 4D$\\to$continuous 4D). Finally, SeeU rolls the world forward in time, re-projects it back to 2D at sampled times and viewpoints, and generates unseen regions based on spatial-temporal context awareness (4D$\\to$2D). By modeling dynamics in 4D, SeeU achieves continuous and physically-consistent novel visual generation, demonstrating strong potentials in multiple tasks including unseen temporal generation, unseen spatial generation, and video editing.",
    "arxiv_url": "https://arxiv.org/abs/2512.03350v1",
    "pdf_url": "https://arxiv.org/pdf/2512.03350v1",
    "published_date": "2025-12-03",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "dynamics",
      "video editing",
      "physical",
      "dit"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.03350v1",
      "pdf": "https://arxiv.org/pdf/2512.03350v1"
    },
    "bibtex": ""
  },
  {
    "title": "OneThinker: All-in-one Reasoning Model for Image and Video",
    "authors": [
      "Kaituo Feng",
      "Manyuan Zhang",
      "Hongyu Li",
      "Kaixuan Fan",
      "Shuang Chen",
      "Yilei Jiang",
      "Dian Zheng",
      "Peiwen Sun",
      "Yiyuan Zhang",
      "Haoze Sun",
      "Yan Feng",
      "Peng Pei",
      "Xunliang Cai",
      "Xiangyu Yue"
    ],
    "abstract": "Reinforcement learning (RL) has recently achieved remarkable success in eliciting visual reasoning within Multimodal Large Language Models (MLLMs). However, existing approaches typically train separate models for different tasks and treat image and video reasoning as disjoint domains. This results in limited scalability toward a multimodal reasoning generalist, which restricts practical versatility and hinders potential knowledge sharing across tasks and modalities. To this end, we propose OneThinker, an all-in-one reasoning model that unifies image and video understanding across diverse fundamental visual tasks, including question answering, captioning, spatial and temporal grounding, tracking, and segmentation. To achieve this, we construct the OneThinker-600k training corpus covering all these tasks and employ commercial models for CoT annotation, resulting in OneThinker-SFT-340k for SFT cold start. Furthermore, we propose EMA-GRPO to handle reward heterogeneity in multi-task RL by tracking task-wise moving averages of reward standard deviations for balanced optimization. Extensive experiments on diverse visual benchmarks show that OneThinker delivers strong performance on 31 benchmarks, across 10 fundamental visual understanding tasks. Moreover, it exhibits effective knowledge transfer between certain tasks and preliminary zero-shot generalization ability, marking a step toward a unified multimodal reasoning generalist. All code, model, and data are released.",
    "arxiv_url": "https://arxiv.org/abs/2512.03043v2",
    "pdf_url": "https://arxiv.org/pdf/2512.03043v2",
    "published_date": "2025-12-02",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "benchmark"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.03043v2",
      "pdf": "https://arxiv.org/pdf/2512.03043v2"
    },
    "bibtex": ""
  },
  {
    "title": "MultiShotMaster: A Controllable Multi-Shot Video Generation Framework",
    "authors": [
      "Qinghe Wang",
      "Xiaoyu Shi",
      "Baolu Li",
      "Weikang Bian",
      "Quande Liu",
      "Huchuan Lu",
      "Xintao Wang",
      "Pengfei Wan",
      "Kun Gai",
      "Xu Jia"
    ],
    "abstract": "Current video generation techniques excel at single-shot clips but struggle to produce narrative multi-shot videos, which require flexible shot arrangement, coherent narrative, and controllability beyond text prompts. To tackle these challenges, we propose MultiShotMaster, a framework for highly controllable multi-shot video generation. We extend a pretrained single-shot model by integrating two novel variants of RoPE. First, we introduce Multi-Shot Narrative RoPE, which applies explicit phase shift at shot transitions, enabling flexible shot arrangement while preserving the temporal narrative order. Second, we design Spatiotemporal Position-Aware RoPE to incorporate reference tokens and grounding signals, enabling spatiotemporal-grounded reference injection. In addition, to overcome data scarcity, we establish an automated data annotation pipeline to extract multi-shot videos, captions, cross-shot grounding signals and reference images. Our framework leverages the intrinsic architectural properties to support multi-shot video generation, featuring text-driven inter-shot consistency, customized subject with motion control, and background-driven customized scene. Both shot count and duration are flexibly configurable. Extensive experiments demonstrate the superior performance and outstanding controllability of our framework.",
    "arxiv_url": "https://arxiv.org/abs/2512.03041v1",
    "pdf_url": "https://arxiv.org/pdf/2512.03041v1",
    "published_date": "2025-12-02",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "motion control",
      "video generation",
      "controllable",
      "dit"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.03041v1",
      "pdf": "https://arxiv.org/pdf/2512.03041v1"
    },
    "bibtex": ""
  },
  {
    "title": "Video4Spatial: Towards Visuospatial Intelligence with Context-Guided Video Generation",
    "authors": [
      "Zeqi Xiao",
      "Yiwei Zhao",
      "Lingxiao Li",
      "Yushi Lan",
      "Ning Yu",
      "Rahul Garg",
      "Roshni Cooper",
      "Mohammad H. Taghavi",
      "Xingang Pan"
    ],
    "abstract": "We investigate whether video generative models can exhibit visuospatial intelligence, a capability central to human cognition, using only visual data. To this end, we present Video4Spatial, a framework showing that video diffusion models conditioned solely on video-based scene context can perform complex spatial tasks. We validate on two tasks: scene navigation - following camera-pose instructions while remaining consistent with 3D geometry of the scene, and object grounding - which requires semantic localization, instruction following, and planning. Both tasks use video-only inputs, without auxiliary modalities such as depth or poses. With simple yet effective design choices in the framework and data curation, Video4Spatial demonstrates strong spatial understanding from video context: it plans navigation and grounds target objects end-to-end, follows camera-pose instructions while maintaining spatial consistency, and generalizes to long contexts and out-of-domain environments. Taken together, these results advance video generative models toward general visuospatial reasoning.",
    "arxiv_url": "https://arxiv.org/abs/2512.03040v2",
    "pdf_url": "https://arxiv.org/pdf/2512.03040v2",
    "published_date": "2025-12-02",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "diffusion model",
      "video generation",
      "video diffusion",
      "dit"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.03040v2",
      "pdf": "https://arxiv.org/pdf/2512.03040v2"
    },
    "bibtex": ""
  },
  {
    "title": "In-Context Sync-LoRA for Portrait Video Editing",
    "authors": [
      "Sagi Polaczek",
      "Or Patashnik",
      "Ali Mahdavi-Amiri",
      "Daniel Cohen-Or"
    ],
    "abstract": "Editing portrait videos is a challenging task that requires flexible yet precise control over a wide range of modifications, such as appearance changes, expression edits, or the addition of objects. The key difficulty lies in preserving the subject's original temporal behavior, demanding that every edited frame remains precisely synchronized with the corresponding source frame. We present Sync-LoRA, a method for editing portrait videos that achieves high-quality visual modifications while maintaining frame-accurate synchronization and identity consistency. Our approach uses an image-to-video diffusion model, where the edit is defined by modifying the first frame and then propagated to the entire sequence. To enable accurate synchronization, we train an in-context LoRA using paired videos that depict identical motion trajectories but differ in appearance. These pairs are automatically generated and curated through a synchronization-based filtering process that selects only the most temporally aligned examples for training. This training setup teaches the model to combine motion cues from the source video with the visual changes introduced in the edited first frame. Trained on a compact, highly curated set of synchronized human portraits, Sync-LoRA generalizes to unseen identities and diverse edits (e.g., modifying appearance, adding objects, or changing backgrounds), robustly handling variations in pose and expression. Our results demonstrate high visual fidelity and strong temporal coherence, achieving a robust balance between edit fidelity and precise motion preservation.",
    "arxiv_url": "https://arxiv.org/abs/2512.03013v1",
    "pdf_url": "https://arxiv.org/pdf/2512.03013v1",
    "published_date": "2025-12-02",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "video diffusion",
      "dit",
      "video editing",
      "identity",
      "image-to-video",
      "diffusion model"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.03013v1",
      "pdf": "https://arxiv.org/pdf/2512.03013v1"
    },
    "bibtex": ""
  },
  {
    "title": "Benchmarking Scientific Understanding and Reasoning for Video Generation using VideoScience-Bench",
    "authors": [
      "Lanxiang Hu",
      "Abhilash Shankarampeta",
      "Yixin Huang",
      "Zilin Dai",
      "Haoyang Yu",
      "Yujie Zhao",
      "Haoqiang Kang",
      "Daniel Zhao",
      "Tajana Rosing",
      "Hao Zhang"
    ],
    "abstract": "The next frontier for video generation lies in developing models capable of zero-shot reasoning, where understanding real-world scientific laws is crucial for accurate physical outcome modeling under diverse conditions. However, existing video benchmarks are physical commonsense-based, offering limited insight into video models' scientific reasoning capability. We introduce VideoScience-Bench, a benchmark designed to evaluate undergraduate-level scientific understanding in video models. Each prompt encodes a composite scientific scenario that requires understanding and reasoning across multiple scientific concepts to generate the correct phenomenon. The benchmark comprises 200 carefully curated prompts spanning 14 topics and 103 concepts in physics and chemistry. We conduct expert-annotated evaluations across seven state-of-the-art video models in T2V and I2V settings along five dimensions: Prompt Consistency, Phenomenon Congruency, Correct Dynamism, Immutability, and Spatio-Temporal Continuity. Using a VLM-as-a-Judge to assess video generations, we observe strong correlation with human assessments. To the best of our knowledge, VideoScience-Bench is the first benchmark to evaluate video models not only as generators but also as reasoners, requiring their generations to demonstrate scientific understanding consistent with expected physical and chemical phenomena. Our data and evaluation code are available at: \\href{https://github.com/hao-ai-lab/VideoScience}{github.com/hao-ai-lab/VideoScience}.",
    "arxiv_url": "https://arxiv.org/abs/2512.02942v1",
    "pdf_url": "https://arxiv.org/pdf/2512.02942v1",
    "published_date": "2025-12-02",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "https://github.com/hao-ai-lab/VideoScience",
    "keywords": [
      "dit",
      "physical",
      "video generation",
      "i2v",
      "evaluation",
      "concept",
      "t2v",
      "benchmark",
      "physics"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.02942v1",
      "pdf": "https://arxiv.org/pdf/2512.02942v1",
      "github": "https://github.com/hao-ai-lab/VideoScience"
    },
    "bibtex": ""
  },
  {
    "title": "LoVoRA: Text-guided and Mask-free Video Object Removal and Addition with Learnable Object-aware Localization",
    "authors": [
      "Zhihan Xiao",
      "Lin Liu",
      "Yixin Gao",
      "Xiaopeng Zhang",
      "Haoxuan Che",
      "Songping Mai",
      "Qi Tian"
    ],
    "abstract": "Text-guided video editing, particularly for object removal and addition, remains a challenging task due to the need for precise spatial and temporal consistency. Existing methods often rely on auxiliary masks or reference images for editing guidance, which limits their scalability and generalization. To address these issues, we propose LoVoRA, a novel framework for mask-free video object removal and addition using object-aware localization mechanism. Our approach utilizes a unique dataset construction pipeline that integrates image-to-video translation, optical flow-based mask propagation, and video inpainting, enabling temporally consistent edits. The core innovation of LoVoRA is its learnable object-aware localization mechanism, which provides dense spatio-temporal supervision for both object insertion and removal tasks. By leveraging a Diffusion Mask Predictor, LoVoRA achieves end-to-end video editing without requiring external control signals during inference. Extensive experiments and human evaluation demonstrate the effectiveness and high-quality performance of LoVoRA. https://cz-5f.github.io/LoVoRA.github.io",
    "arxiv_url": "https://arxiv.org/abs/2512.02933v2",
    "pdf_url": "https://arxiv.org/pdf/2512.02933v2",
    "published_date": "2025-12-02",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "temporal consistency",
      "dit",
      "video editing",
      "image-to-video",
      "video inpainting",
      "evaluation",
      "video translation"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.02933v2",
      "pdf": "https://arxiv.org/pdf/2512.02933v2",
      "project": "https://cz-5f.github.io/LoVoRA.github.io"
    },
    "bibtex": ""
  },
  {
    "title": "Taming Camera-Controlled Video Generation with Verifiable Geometry Reward",
    "authors": [
      "Zhaoqing Wang",
      "Xiaobo Xia",
      "Zhuolin Bie",
      "Jinlin Liu",
      "Dongdong Yu",
      "Jia-Wang Bian",
      "Changhu Wang"
    ],
    "abstract": "Recent advances in video diffusion models have remarkably improved camera-controlled video generation, but most methods rely solely on supervised fine-tuning (SFT), leaving online reinforcement learning (RL) post-training largely underexplored. In this work, we introduce an online RL post-training framework that optimizes a pretrained video generator for precise camera control. To make RL effective in this setting, we design a verifiable geometry reward that delivers dense segment-level feedback to guide model optimization. Specifically, we estimate the 3D camera trajectories for both generated and reference videos, divide each trajectory into short segments, and compute segment-wise relative poses. The reward function then compares each generated-reference segment pair and assigns an alignment score as the reward signal, which helps alleviate reward sparsity and improve optimization efficiency. Moreover, we construct a comprehensive dataset featuring diverse large-amplitude camera motions and scenes with varied subject dynamics. Extensive experiments show that our online RL post-training clearly outperforms SFT baselines across multiple aspects, including camera-control accuracy, geometric consistency, and visual quality, demonstrating its superiority in advancing camera-controlled video generation.",
    "arxiv_url": "https://arxiv.org/abs/2512.02870v1",
    "pdf_url": "https://arxiv.org/pdf/2512.02870v1",
    "published_date": "2025-12-02",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "video diffusion",
      "trajectory",
      "video generation",
      "diffusion model",
      "dynamics",
      "camera control"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.02870v1",
      "pdf": "https://arxiv.org/pdf/2512.02870v1"
    },
    "bibtex": ""
  },
  {
    "title": "From Navigation to Refinement: Revealing the Two-Stage Nature of Flow-based Diffusion Models through Oracle Velocity",
    "authors": [
      "Haoming Liu",
      "Jinnuo Liu",
      "Yanhao Li",
      "Liuyang Bai",
      "Yunkai Ji",
      "Yuanhe Guo",
      "Shenji Wan",
      "Hongyi Wen"
    ],
    "abstract": "Flow-based diffusion models have emerged as a leading paradigm for training generative models across images and videos. However, their memorization-generalization behavior remains poorly understood. In this work, we revisit the flow matching (FM) objective and study its marginal velocity field, which admits a closed-form expression, allowing exact computation of the oracle FM target. Analyzing this oracle velocity field reveals that flow-based diffusion models inherently formulate a two-stage training target: an early stage guided by a mixture of data modes, and a later stage dominated by the nearest data sample. The two-stage objective leads to distinct learning behaviors: the early navigation stage generalizes across data modes to form global layouts, whereas the later refinement stage increasingly memorizes fine-grained details. Leveraging these insights, we explain the effectiveness of practical techniques such as timestep-shifted schedules, classifier-free guidance intervals, and latent space design choices. Our study deepens the understanding of diffusion model training dynamics and offers principles for guiding future architectural and algorithmic improvements.",
    "arxiv_url": "https://arxiv.org/abs/2512.02826v2",
    "pdf_url": "https://arxiv.org/pdf/2512.02826v2",
    "published_date": "2025-12-02",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "diffusion model",
      "layout",
      "flow matching",
      "dynamics"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.02826v2",
      "pdf": "https://arxiv.org/pdf/2512.02826v2"
    },
    "bibtex": ""
  },
  {
    "title": "RULER-Bench: Probing Rule-based Reasoning Abilities of Next-level Video Generation Models for Vision Foundation Intelligence",
    "authors": [
      "Xuming He",
      "Zehao Fan",
      "Hengjia Li",
      "Fan Zhuo",
      "Hankun Xu",
      "Senlin Cheng",
      "Di Weng",
      "Haifeng Liu",
      "Can Ye",
      "Boxi Wu"
    ],
    "abstract": "Recent advances in video generation have enabled the synthesis of videos with strong temporal consistency and impressive visual quality, marking a crucial step toward vision foundation models. To evaluate these video generation models, existing benchmarks primarily focus on factors related to visual perception and understanding, like visual aesthetics, instruction adherence, and temporal coherence. However, the rule-based reasoning capabilities of video generation models remain largely unexplored. Although recent studies have carried out preliminary explorations into whether video models can serve as zero-shot learners, they still lack a fine-grained decomposition of reasoning capabilities and a comprehensive evaluation protocol. To address this gap, we introduce RULER-Bench, a benchmark designed to evaluate the reasoning ability of video generation models from the perspective of cognitive rules. Built upon two fundamental paradigms: text-to-video and image-to-video, RULER-Bench covers 40 representative tasks spanning six rule categories with 622 high-quality annotated instances. For the evaluation of each generated video, we construct a checklist covering four metrics and leverage GPT-o3 to assign scores to each question, achieving 85% alignment with human judgements. Extensive experiments show that the state-of-the-art model achieves only 48.87% on the rule coherence metric, highlighting significant room for improvement in the reasoning capability of next-level video models. We expect that the insight obtained from RULER-Bench will facilitate further development of reasoning-aware video generation, advancing video generation models toward vision foundation intelligence.",
    "arxiv_url": "https://arxiv.org/abs/2512.02622v1",
    "pdf_url": "https://arxiv.org/pdf/2512.02622v1",
    "published_date": "2025-12-02",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "temporal consistency",
      "text-to-video",
      "video generation",
      "image-to-video",
      "evaluation",
      "benchmark"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.02622v1",
      "pdf": "https://arxiv.org/pdf/2512.02622v1"
    },
    "bibtex": ""
  },
  {
    "title": "Co-speech Gesture Video Generation via Motion-Based Graph Retrieval",
    "authors": [
      "Yafei Song",
      "Peng Zhang",
      "Bang Zhang"
    ],
    "abstract": "Synthesizing synchronized and natural co-speech gesture videos remains a formidable challenge. Recent approaches have leveraged motion graphs to harness the potential of existing video data. To retrieve an appropriate trajectory from the graph, previous methods either utilize the distance between features extracted from the input audio and those associated with the motions in the graph or embed both the input audio and motion into a shared feature space. However, these techniques may not be optimal due to the many-to-many mapping nature between audio and gestures, which cannot be adequately addressed by one-to-one mapping. To alleviate this limitation, we propose a novel framework that initially employs a diffusion model to generate gesture motions. The diffusion model implicitly learns the joint distribution of audio and motion, enabling the generation of contextually appropriate gestures from input audio sequences. Furthermore, our method extracts both low-level and high-level features from the input audio to enrich the training process of the diffusion model. Subsequently, a meticulously designed motion-based retrieval algorithm is applied to identify the most suitable path within the graph by assessing both global and local similarities in motion. Given that not all nodes in the retrieved path are sequentially continuous, the final step involves seamlessly stitching together these segments to produce a coherent video output. Experimental results substantiate the efficacy of our proposed method, demonstrating a significant improvement over prior approaches in terms of synchronization accuracy and naturalness of generated gestures.",
    "arxiv_url": "https://arxiv.org/abs/2512.02576v1",
    "pdf_url": "https://arxiv.org/pdf/2512.02576v1",
    "published_date": "2025-12-02",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "diffusion model",
      "gesture",
      "trajectory",
      "video generation"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.02576v1",
      "pdf": "https://arxiv.org/pdf/2512.02576v1"
    },
    "bibtex": ""
  },
  {
    "title": "OmniPerson: Unified Identity-Preserving Pedestrian Generation",
    "authors": [
      "Changxiao Ma",
      "Chao Yuan",
      "Xincheng Shi",
      "Yuzhuo Ma",
      "Yongfei Zhang",
      "Longkun Zhou",
      "Yujia Zhang",
      "Shangze Li",
      "Yifan Xu"
    ],
    "abstract": "Person re-identification (ReID) suffers from a lack of large-scale high-quality training data due to challenges in data privacy and annotation costs. While previous approaches have explored pedestrian generation for data augmentation, they often fail to ensure identity consistency and suffer from insufficient controllability, thereby limiting their effectiveness in dataset augmentation. To address this, We introduce OmniPerson, the first unified identity-preserving pedestrian generation pipeline for visible/infrared image/video ReID tasks. Our contributions are threefold: 1) We proposed OmniPerson, a unified generation model, offering holistic and fine-grained control over all key pedestrian attributes. Supporting RGB/IR modality image/video generation with any number of reference images, two kinds of person poses, and text. Also including RGB-to-IR transfer and image super-resolution abilities.2) We designed Multi-Refer Fuser for robust identity preservation with any number of reference images as input, making OmniPerson could distill a unified identity from a set of multi-view reference images, ensuring our generated pedestrians achieve high-fidelity pedestrian generation.3) We introduce PersonSyn, the first large-scale dataset for multi-reference, controllable pedestrian generation, and present its automated curation pipeline which transforms public, ID-only ReID benchmarks into a richly annotated resource with the dense, multi-modal supervision required for this task. Experimental results demonstrate that OmniPerson achieves SoTA in pedestrian generation, excelling in both visual fidelity and identity consistency. Furthermore, augmenting existing datasets with our generated data consistently improves the performance of ReID models. We will open-source the full codebase, pretrained model, and the PersonSyn dataset.",
    "arxiv_url": "https://arxiv.org/abs/2512.02554v1",
    "pdf_url": "https://arxiv.org/pdf/2512.02554v1",
    "published_date": "2025-12-02",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "multi-modal",
      "controllable",
      "super-resolution",
      "identity",
      "video generation",
      "benchmark"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.02554v1",
      "pdf": "https://arxiv.org/pdf/2512.02554v1"
    },
    "bibtex": ""
  },
  {
    "title": "YingVideo-MV: Music-Driven Multi-Stage Video Generation",
    "authors": [
      "Jiahui Chen",
      "Weida Wang",
      "Runhua Shi",
      "Huan Yang",
      "Chaofan Ding",
      "Zihao Chen"
    ],
    "abstract": "While diffusion model for audio-driven avatar video generation have achieved notable process in synthesizing long sequences with natural audio-visual synchronization and identity consistency, the generation of music-performance videos with camera motions remains largely unexplored. We present YingVideo-MV, the first cascaded framework for music-driven long-video generation. Our approach integrates audio semantic analysis, an interpretable shot planning module (MV-Director), temporal-aware diffusion Transformer architectures, and long-sequence consistency modeling to enable automatic synthesis of high-quality music performance videos from audio signals. We construct a large-scale Music-in-the-Wild Dataset by collecting web data to support the achievement of diverse, high-quality results. Observing that existing long-video generation methods lack explicit camera motion control, we introduce a camera adapter module that embeds camera poses into latent noise. To enhance continulity between clips during long-sequence inference, we further propose a time-aware dynamic window range strategy that adaptively adjust denoising ranges based on audio embedding. Comprehensive benchmark tests demonstrate that YingVideo-MV achieves outstanding performance in generating coherent and expressive music videos, and enables precise music-motion-camera synchronization. More videos are available in our project page: https://giantailab.github.io/YingVideo-MV/ .",
    "arxiv_url": "https://arxiv.org/abs/2512.02492v1",
    "pdf_url": "https://arxiv.org/pdf/2512.02492v1",
    "published_date": "2025-12-02",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "motion control",
      "identity",
      "diffusion transformer",
      "video generation",
      "diffusion model",
      "denoising",
      "architecture",
      "music video",
      "benchmark",
      "avatar",
      "audio-driven"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.02492v1",
      "pdf": "https://arxiv.org/pdf/2512.02492v1",
      "project": "https://giantailab.github.io/YingVideo-MV"
    },
    "bibtex": ""
  },
  {
    "title": "Does Hearing Help Seeing? Investigating Audio-Video Joint Denoising for Video Generation",
    "authors": [
      "Jianzong Wu",
      "Hao Lian",
      "Dachao Hao",
      "Ye Tian",
      "Qingyu Shi",
      "Biaolong Chen",
      "Hao Jiang",
      "Yunhai Tong"
    ],
    "abstract": "Recent audio-video generative systems suggest that coupling modalities benefits not only audio-video synchrony but also the video modality itself. We pose a fundamental question: Does audio-video joint denoising training improve video generation, even when we only care about video quality? To study this, we introduce a parameter-efficient Audio-Video Full DiT (AVFullDiT) architecture that leverages pre-trained text-to-video (T2V) and text-to-audio (T2A) modules for joint denoising. We train (i) a T2AV model with AVFullDiT and (ii) a T2V-only counterpart under identical settings. Our results provide the first systematic evidence that audio-video joint denoising can deliver more than synchrony. We observe consistent improvements on challenging subsets featuring large and object contact motions. We hypothesize that predicting audio acts as a privileged signal, encouraging the model to internalize causal relationships between visual events and their acoustic consequences (e.g., collision $\\times$ impact sound), which in turn regularizes video dynamics. Our findings suggest that cross-modal co-training is a promising approach to developing stronger, more physically grounded world models. Code and dataset will be made publicly available.",
    "arxiv_url": "https://arxiv.org/abs/2512.02457v2",
    "pdf_url": "https://arxiv.org/pdf/2512.02457v2",
    "published_date": "2025-12-02",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "dit",
      "world model",
      "physical",
      "text-to-video",
      "video generation",
      "architecture",
      "denoising",
      "sound",
      "dynamics",
      "t2v",
      "efficient"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.02457v2",
      "pdf": "https://arxiv.org/pdf/2512.02457v2"
    },
    "bibtex": ""
  },
  {
    "title": "Video Diffusion Models Excel at Tracking Similar-Looking Objects Without Supervision",
    "authors": [
      "Chenshuang Zhang",
      "Kang Zhang",
      "Joon Son Chung",
      "In So Kweon",
      "Junmo Kim",
      "Chengzhi Mao"
    ],
    "abstract": "Distinguishing visually similar objects by their motion remains a critical challenge in computer vision. Although supervised trackers show promise, contemporary self-supervised trackers struggle when visual cues become ambiguous, limiting their scalability and generalization without extensive labeled data. We find that pre-trained video diffusion models inherently learn motion representations suitable for tracking without task-specific training. This ability arises because their denoising process isolates motion in early, high-noise stages, distinct from later appearance refinement. Capitalizing on this discovery, our self-supervised tracker significantly improves performance in distinguishing visually similar objects, an underexplored failure point for existing methods. Our method achieves up to a 6-point improvement over recent self-supervised approaches on established benchmarks and our newly introduced tests focused on tracking visually similar items. Visualizations confirm that these diffusion-derived motion representations enable robust tracking of even identical objects across challenging viewpoint changes and deformations.",
    "arxiv_url": "https://arxiv.org/abs/2512.02339v1",
    "pdf_url": "https://arxiv.org/pdf/2512.02339v1",
    "published_date": "2025-12-02",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "benchmark",
      "diffusion model",
      "denoising",
      "video diffusion"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.02339v1",
      "pdf": "https://arxiv.org/pdf/2512.02339v1"
    },
    "bibtex": ""
  },
  {
    "title": "OmniGuard: Unified Omni-Modal Guardrails with Deliberate Reasoning",
    "authors": [
      "Boyu Zhu",
      "Xiaofei Wen",
      "Wenjie Jacky Mo",
      "Tinghui Zhu",
      "Yanan Xie",
      "Peng Qi",
      "Muhao Chen"
    ],
    "abstract": "Omni-modal Large Language Models (OLLMs) that process text, images, videos, and audio introduce new challenges for safety and value guardrails in human-AI interaction. Prior guardrail research largely targets unimodal settings and typically frames safeguarding as binary classification, which limits robustness across diverse modalities and tasks. To address this gap, we propose OmniGuard, the first family of omni-modal guardrails that performs safeguarding across all modalities with deliberate reasoning ability. To support the training of OMNIGUARD, we curate a large, comprehensive omni-modal safety dataset comprising over 210K diverse samples, with inputs that cover all modalities through both unimodal and cross-modal samples. Each sample is annotated with structured safety labels and carefully curated safety critiques from expert models through targeted distillation. Extensive experiments on 15 benchmarks show that OmniGuard achieves strong effectiveness and generalization across a wide range of multimodal safety scenarios. Importantly, OmniGuard provides a unified framework that enforces policies and mitigates risks in omni-modalities, paving the way toward building more robust and capable omnimodal safeguarding systems.",
    "arxiv_url": "https://arxiv.org/abs/2512.02306v1",
    "pdf_url": "https://arxiv.org/pdf/2512.02306v1",
    "published_date": "2025-12-02",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CR",
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "benchmark",
      "distillation"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.02306v1",
      "pdf": "https://arxiv.org/pdf/2512.02306v1"
    },
    "bibtex": ""
  },
  {
    "title": "Progressive Image Restoration via Text-Conditioned Video Generation",
    "authors": [
      "Peng Kang",
      "Xijun Wang",
      "Yu Yuan"
    ],
    "abstract": "Recent text-to-video models have demonstrated strong temporal generation capabilities, yet their potential for image restoration remains underexplored. In this work, we repurpose CogVideo for progressive visual restoration tasks by fine-tuning it to generate restoration trajectories rather than natural video motion. Specifically, we construct synthetic datasets for super-resolution, deblurring, and low-light enhancement, where each sample depicts a gradual transition from degraded to clean frames. Two prompting strategies are compared: a uniform text prompt shared across all samples, and a scene-specific prompting scheme generated via LLaVA multi-modal LLM and refined with ChatGPT. Our fine-tuned model learns to associate temporal progression with restoration quality, producing sequences that improve perceptual metrics such as PSNR, SSIM, and LPIPS across frames. Extensive experiments show that CogVideo effectively restores spatial detail and illumination consistency while maintaining temporal coherence. Moreover, the model generalizes to real-world scenarios on the ReLoBlur dataset without additional training, demonstrating strong zero-shot robustness and interpretability through temporal restoration.",
    "arxiv_url": "https://arxiv.org/abs/2512.02273v1",
    "pdf_url": "https://arxiv.org/pdf/2512.02273v1",
    "published_date": "2025-12-01",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "multi-modal",
      "dit",
      "super-resolution",
      "text-to-video",
      "video generation"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.02273v1",
      "pdf": "https://arxiv.org/pdf/2512.02273v1"
    },
    "bibtex": ""
  },
  {
    "title": "Objects in Generated Videos Are Slower Than They Appear: Models Suffer Sub-Earth Gravity and Don't Know Galileo's Principle...for now",
    "authors": [
      "Varun Varma Thozhiyoor",
      "Shivam Tripathi",
      "Venkatesh Babu Radhakrishnan",
      "Anand Bhattad"
    ],
    "abstract": "Video generators are increasingly evaluated as potential world models, which requires them to encode and understand physical laws. We investigate their representation of a fundamental law: gravity. Out-of-the-box video generators consistently generate objects falling at an effectively slower acceleration. However, these physical tests are often confounded by ambiguous metric scale. We first investigate if observed physical errors are artifacts of these ambiguities (e.g., incorrect frame rate assumptions). We find that even temporal rescaling cannot correct the high-variance gravity artifacts. To rigorously isolate the underlying physical representation from these confounds, we introduce a unit-free, two-object protocol that tests the timing ratio $t_1^2/t_2^2 = h_1/h_2$, a relationship independent of $g$, focal length, and scale. This relative test reveals violations of Galileo's equivalence principle. We then demonstrate that this physical gap can be partially mitigated with targeted specialization. A lightweight low-rank adaptor fine-tuned on only 100 single-ball clips raises $g_{\\mathrm{eff}}$ from $1.81\\,\\mathrm{m/s^2}$ to $6.43\\,\\mathrm{m/s^2}$ (reaching $65\\%$ of terrestrial gravity). This specialist adaptor also generalizes zero-shot to two-ball drops and inclined planes, offering initial evidence that specific physical laws can be corrected with minimal data.",
    "arxiv_url": "https://arxiv.org/abs/2512.02016v1",
    "pdf_url": "https://arxiv.org/pdf/2512.02016v1",
    "published_date": "2025-12-01",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "world model",
      "acceleration",
      "physical"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.02016v1",
      "pdf": "https://arxiv.org/pdf/2512.02016v1"
    },
    "bibtex": ""
  },
  {
    "title": "Generative Video Motion Editing with 3D Point Tracks",
    "authors": [
      "Yao-Chih Lee",
      "Zhoutong Zhang",
      "Jiahui Huang",
      "Jui-Hsien Wang",
      "Joon-Young Lee",
      "Jia-Bin Huang",
      "Eli Shechtman",
      "Zhengqi Li"
    ],
    "abstract": "Camera and object motions are central to a video's narrative. However, precisely editing these captured motions remains a significant challenge, especially under complex object movements. Current motion-controlled image-to-video (I2V) approaches often lack full-scene context for consistent video editing, while video-to-video (V2V) methods provide viewpoint changes or basic object translation, but offer limited control over fine-grained object motion. We present a track-conditioned V2V framework that enables joint editing of camera and object motion. We achieve this by conditioning a video generation model on a source video and paired 3D point tracks representing source and target motions. These 3D tracks establish sparse correspondences that transfer rich context from the source video to new motions while preserving spatiotemporal coherence. Crucially, compared to 2D tracks, 3D tracks provide explicit depth cues, allowing the model to resolve depth order and handle occlusions for precise motion editing. Trained in two stages on synthetic and real data, our model supports diverse motion edits, including joint camera/object manipulation, motion transfer, and non-rigid deformation, unlocking new creative potential in video editing.",
    "arxiv_url": "https://arxiv.org/abs/2512.02015v1",
    "pdf_url": "https://arxiv.org/pdf/2512.02015v1",
    "published_date": "2025-12-01",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "creative",
      "dit",
      "video editing",
      "video generation",
      "image-to-video",
      "video-to-video",
      "i2v"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.02015v1",
      "pdf": "https://arxiv.org/pdf/2512.02015v1"
    },
    "bibtex": ""
  },
  {
    "title": "TUNA: Taming Unified Visual Representations for Native Unified Multimodal Models",
    "authors": [
      "Zhiheng Liu",
      "Weiming Ren",
      "Haozhe Liu",
      "Zijian Zhou",
      "Shoufa Chen",
      "Haonan Qiu",
      "Xiaoke Huang",
      "Zhaochong An",
      "Fanny Yang",
      "Aditya Patel",
      "Viktar Atliha",
      "Tony Ng",
      "Xiao Han",
      "Chuyan Zhu",
      "Chenyang Zhang",
      "Ding Liu",
      "Juan-Manuel Perez-Rua",
      "Sen He",
      "Jrgen Schmidhuber",
      "Wenhu Chen",
      "Ping Luo",
      "Wei Liu",
      "Tao Xiang",
      "Jonas Schult",
      "Yuren Cong"
    ],
    "abstract": "Unified multimodal models (UMMs) aim to jointly perform multimodal understanding and generation within a single framework. We present TUNA, a native UMM that builds a unified continuous visual representation by cascading a VAE encoder with a representation encoder. This unified representation space allows end-to-end processing of images and videos for both understanding and generation tasks. Compared to prior UMMs with decoupled representations, TUNA's unified visual space avoids representation format mismatches introduced by separate encoders, outperforming decoupled alternatives in both understanding and generation. Moreover, we observe that stronger pretrained representation encoders consistently yield better performance across all multimodal tasks, highlighting the importance of the representation encoder. Finally, in this unified setting, jointly training on both understanding and generation data allows the two tasks to benefit from each other rather than interfere. Our extensive experiments on multimodal understanding and generation benchmarks show that TUNA achieves state-of-the-art results in image and video understanding, image and video generation, and image editing, demonstrating the effectiveness and scalability of its unified representation design.",
    "arxiv_url": "https://arxiv.org/abs/2512.02014v1",
    "pdf_url": "https://arxiv.org/pdf/2512.02014v1",
    "published_date": "2025-12-01",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "benchmark",
      "video generation",
      "dit"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.02014v1",
      "pdf": "https://arxiv.org/pdf/2512.02014v1"
    },
    "bibtex": ""
  },
  {
    "title": "PAI-Bench: A Comprehensive Benchmark For Physical AI",
    "authors": [
      "Fengzhe Zhou",
      "Jiannan Huang",
      "Jialuo Li",
      "Deva Ramanan",
      "Humphrey Shi"
    ],
    "abstract": "Physical AI aims to develop models that can perceive and predict real-world dynamics; yet, the extent to which current multi-modal large language models and video generative models support these abilities is insufficiently understood. We introduce Physical AI Bench (PAI-Bench), a unified and comprehensive benchmark that evaluates perception and prediction capabilities across video generation, conditional video generation, and video understanding, comprising 2,808 real-world cases with task-aligned metrics designed to capture physical plausibility and domain-specific reasoning. Our study provides a systematic assessment of recent models and shows that video generative models, despite strong visual fidelity, often struggle to maintain physically coherent dynamics, while multi-modal large language models exhibit limited performance in forecasting and causal interpretation. These observations suggest that current systems are still at an early stage in handling the perceptual and predictive demands of Physical AI. In summary, PAI-Bench establishes a realistic foundation for evaluating Physical AI and highlights key gaps that future systems must address.",
    "arxiv_url": "https://arxiv.org/abs/2512.01989v1",
    "pdf_url": "https://arxiv.org/pdf/2512.01989v1",
    "published_date": "2025-12-01",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "multi-modal",
      "dit",
      "physical",
      "video generation",
      "dynamics",
      "benchmark"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.01989v1",
      "pdf": "https://arxiv.org/pdf/2512.01989v1"
    },
    "bibtex": ""
  },
  {
    "title": "SpriteHand: Real-Time Versatile Hand-Object Interaction with Autoregressive Video Generation",
    "authors": [
      "Zisu Li",
      "Hengye Lyu",
      "Jiaxin Shi",
      "Yufeng Zeng",
      "Mingming Fan",
      "Hanwang Zhang",
      "Chen Liang"
    ],
    "abstract": "Modeling and synthesizing complex hand-object interactions remains a significant challenge, even for state-of-the-art physics engines. Conventional simulation-based approaches rely on explicitly defined rigid object models and pre-scripted hand gestures, making them inadequate for capturing dynamic interactions with non-rigid or articulated entities such as deformable fabrics, elastic materials, hinge-based structures, furry surfaces, or even living creatures. In this paper, we present SpriteHand, an autoregressive video generation framework for real-time synthesis of versatile hand-object interaction videos across a wide range of object types and motion patterns. SpriteHand takes as input a static object image and a video stream in which the hands are imagined to interact with the virtual object embedded in a real-world scene, and generates corresponding hand-object interaction effects in real time. Our model employs a causal inference architecture for autoregressive generation and leverages a hybrid post-training approach to enhance visual realism and temporal coherence. Our 1.3B model supports real-time streaming generation at around 18 FPS and 640x368 resolution, with an approximate 150 ms latency on a single NVIDIA RTX 5090 GPU, and more than a minute of continuous output. Experiments demonstrate superior visual quality, physical plausibility, and interaction fidelity compared to both generative and engine-based baselines.",
    "arxiv_url": "https://arxiv.org/abs/2512.01960v1",
    "pdf_url": "https://arxiv.org/pdf/2512.01960v1",
    "published_date": "2025-12-01",
    "categories": [
      "cs.CV",
      "cs.HC"
    ],
    "github_url": "",
    "keywords": [
      "streaming",
      "physical",
      "video generation",
      "architecture",
      "simulation",
      "autoregressive",
      "physics",
      "gesture"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.01960v1",
      "pdf": "https://arxiv.org/pdf/2512.01960v1"
    },
    "bibtex": ""
  },
  {
    "title": "Script: Graph-Structured and Query-Conditioned Semantic Token Pruning for Multimodal Large Language Models",
    "authors": [
      "Zhongyu Yang",
      "Dannong Xu",
      "Wei Pang",
      "Yingfang Yuan"
    ],
    "abstract": "The rapid growth of visual tokens in multimodal large language models (MLLMs) leads to excessive memory consumption and inference latency, especially when handling high-resolution images and videos. Token pruning is a technique used to mitigate this issue by removing redundancy, but existing methods often ignore relevance to the user query or suffer from the limitations of attention mechanisms, reducing their adaptability and effectiveness. To address these challenges, we propose Script, a plug-and-play pruning method that requires no retraining and generalizes across diverse MLLMs. Script comprises two modules: a graph-structured pruning module that removes visually redundant tokens, and a query-conditioned semantic pruning module that preserves query-relevant visual information. Together, they enhance performance on multimodal tasks. Experiments on fourteen benchmarks across image and video understanding tasks show that Script consistently achieves higher model efficiency and predictive accuracy compared to existing pruning methods. On LLaVA-NeXT-7B, it achieves up to 6.8x prefill speedup and 10x FLOP reduction, while retaining 96.88% of the original performance.",
    "arxiv_url": "https://arxiv.org/abs/2512.01949v1",
    "pdf_url": "https://arxiv.org/pdf/2512.01949v1",
    "published_date": "2025-12-01",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "benchmark",
      "dit"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.01949v1",
      "pdf": "https://arxiv.org/pdf/2512.01949v1"
    },
    "bibtex": ""
  },
  {
    "title": "PhyDetEx: Detecting and Explaining the Physical Plausibility of T2V Models",
    "authors": [
      "Zeqing Wang",
      "Keze Wang",
      "Lei Zhang"
    ],
    "abstract": "Driven by the growing capacity and training scale, Text-to-Video (T2V) generation models have recently achieved substantial progress in video quality, length, and instruction-following capability. However, whether these models can understand physics and generate physically plausible videos remains a question. While Vision-Language Models (VLMs) have been widely used as general-purpose evaluators in various applications, they struggle to identify the physically impossible content from generated videos. To investigate this issue, we construct a \\textbf{PID} (\\textbf{P}hysical \\textbf{I}mplausibility \\textbf{D}etection) dataset, which consists of a \\textit{test split} of 500 manually annotated videos and a \\textit{train split} of 2,588 paired videos, where each implausible video is generated by carefully rewriting the caption of its corresponding real-world video to induce T2V models producing physically implausible content. With the constructed dataset, we introduce a lightweight fine-tuning approach, enabling VLMs to not only detect physically implausible events but also generate textual explanations on the violated physical principles. Taking the fine-tuned VLM as a physical plausibility detector and explainer, namely \\textbf{PhyDetEx}, we benchmark a series of state-of-the-art T2V models to assess their adherence to physical laws. Our findings show that although recent T2V models have made notable progress toward generating physically plausible content, understanding and adhering to physical laws remains a challenging issue, especially for open-source models. Our dataset, training code, and checkpoints are available at \\href{https://github.com/Zeqing-Wang/PhyDetEx}{https://github.com/Zeqing-Wang/PhyDetEx}.",
    "arxiv_url": "https://arxiv.org/abs/2512.01843v1",
    "pdf_url": "https://arxiv.org/pdf/2512.01843v1",
    "published_date": "2025-12-01",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/Zeqing-Wang/PhyDetEx",
    "keywords": [
      "physical",
      "text-to-video",
      "t2v",
      "benchmark",
      "physics"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.01843v1",
      "pdf": "https://arxiv.org/pdf/2512.01843v1",
      "github": "https://github.com/Zeqing-Wang/PhyDetEx"
    },
    "bibtex": ""
  },
  {
    "title": "Generative Action Tell-Tales: Assessing Human Motion in Synthesized Videos",
    "authors": [
      "Xavier Thomas",
      "Youngsun Lim",
      "Ananya Srinivasan",
      "Audrey Zheng",
      "Deepti Ghadiyaram"
    ],
    "abstract": "Despite rapid advances in video generative models, robust metrics for evaluating visual and temporal correctness of complex human actions remain elusive. Critically, existing pure-vision encoders and Multimodal Large Language Models (MLLMs) are strongly appearance-biased, lack temporal understanding, and thus struggle to discern intricate motion dynamics and anatomical implausibilities in generated videos. We tackle this gap by introducing a novel evaluation metric derived from a learned latent space of real-world human actions. Our method first captures the nuances, constraints, and temporal smoothness of real-world motion by fusing appearance-agnostic human skeletal geometry features with appearance-based features. We posit that this combined feature space provides a robust representation of action plausibility. Given a generated video, our metric quantifies its action quality by measuring the distance between its underlying representations and this learned real-world action distribution. For rigorous validation, we develop a new multi-faceted benchmark specifically designed to probe temporally challenging aspects of human action fidelity. Through extensive experiments, we show that our metric achieves substantial improvement of more than 68% compared to existing state-of-the-art methods on our benchmark, performs competitively on established external benchmarks, and has a stronger correlation with human perception. Our in-depth analysis reveals critical limitations in current video generative models and establishes a new standard for advanced research in video generation.",
    "arxiv_url": "https://arxiv.org/abs/2512.01803v2",
    "pdf_url": "https://arxiv.org/pdf/2512.01803v2",
    "published_date": "2025-12-01",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "video generation",
      "evaluation",
      "dynamics",
      "human motion",
      "benchmark"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.01803v2",
      "pdf": "https://arxiv.org/pdf/2512.01803v2"
    },
    "bibtex": ""
  },
  {
    "title": "Evaluating SAM2 for Video Semantic Segmentation",
    "authors": [
      "Syed Hesham Syed Ariff",
      "Yun Liu",
      "Guolei Sun",
      "Jing Yang",
      "Henghui Ding",
      "Xue Geng",
      "Xudong Jiang"
    ],
    "abstract": "The Segmentation Anything Model 2 (SAM2) has proven to be a powerful foundation model for promptable visual object segmentation in both images and videos, capable of storing object-aware memories and transferring them temporally through memory blocks. While SAM2 excels in video object segmentation by providing dense segmentation masks based on prompts, extending it to dense Video Semantic Segmentation (VSS) poses challenges due to the need for spatial accuracy, temporal consistency, and the ability to track multiple objects with complex boundaries and varying scales. This paper explores the extension of SAM2 for VSS, focusing on two primary approaches and highlighting firsthand observations and common challenges faced during this process. The first approach involves using SAM2 to extract unique objects as masks from a given image, with a segmentation network employed in parallel to generate and refine initial predictions. The second approach utilizes the predicted masks to extract unique feature vectors, which are then fed into a simple network for classification. The resulting classifications and masks are subsequently combined to produce the final segmentation. Our experiments suggest that leveraging SAM2 enhances overall performance in VSS, primarily due to its precise predictions of object boundaries.",
    "arxiv_url": "https://arxiv.org/abs/2512.01774v1",
    "pdf_url": "https://arxiv.org/pdf/2512.01774v1",
    "published_date": "2025-12-01",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "temporal consistency"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.01774v1",
      "pdf": "https://arxiv.org/pdf/2512.01774v1"
    },
    "bibtex": ""
  },
  {
    "title": "VideoScoop: A Non-Traditional Domain-Independent Framework For Video Analysis",
    "authors": [
      "Hafsa Billah"
    ],
    "abstract": "Automatically understanding video contents is important for several applications in Civic Monitoring (CM), general Surveillance (SL), Assisted Living (AL), etc. Decades of Image and Video Analysis (IVA) research have advanced tasks such as content extraction (e.g., object recognition and tracking). Identifying meaningful activities or situations (e.g., two objects coming closer) remains difficult and cannot be achieved by content extraction alone. Currently, Video Situation Analysis (VSA) is done manually with a human in the loop, which is error-prone and labor-intensive, or through custom algorithms designed for specific video types or situations. These algorithms are not general-purpose and require a new algorithm/software for each new situation or video from a new domain.   This report proposes a general-purpose VSA framework that overcomes the above limitations. Video contents are extracted once using state-of-the-art Video Content Extraction technologies. They are represented using two alternative models -- the extended relational model (R++) and graph models. When represented using R++, the extracted contents can be used as data streams, enabling Continuous Query Processing via the proposed Continuous Query Language for Video Analysis. The graph models complement this by enabling the detection of situations that are difficult or impossible to detect using the relational model alone. Existing graph algorithms and newly developed algorithms support a wide variety of situation detection. To support domain independence, primitive situation variants across domains are identified and expressed as parameterized templates. Extensive experiments were conducted across several interesting situations from three domains -- AL, CM, and SL-- to evaluate the accuracy, efficiency, and robustness of the proposed approach using a dataset of videos of varying lengths from these domains.",
    "arxiv_url": "https://arxiv.org/abs/2512.01769v1",
    "pdf_url": "https://arxiv.org/pdf/2512.01769v1",
    "published_date": "2025-12-01",
    "categories": [
      "cs.CV",
      "cs.DB"
    ],
    "github_url": "",
    "keywords": [
      "dit"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.01769v1",
      "pdf": "https://arxiv.org/pdf/2512.01769v1"
    },
    "bibtex": ""
  },
  {
    "title": "DreamingComics: A Story Visualization Pipeline via Subject and Layout Customized Generation using Video Models",
    "authors": [
      "Patrick Kwon",
      "Chen Chen"
    ],
    "abstract": "Current story visualization methods tend to position subjects solely by text and face challenges in maintaining artistic consistency. To address these limitations, we introduce DreamingComics, a layout-aware story visualization framework. We build upon a pretrained video diffusion-transformer (DiT) model, leveraging its spatiotemporal priors to enhance identity and style consistency. For layout-based position control, we propose RegionalRoPE, a region-aware positional encoding scheme that re-indexes embeddings based on the target layout. Additionally, we introduce a masked condition loss to further constrain each subject's visual features to their designated region. To infer layouts from natural language scripts, we integrate an LLM-based layout generator trained to produce comic-style layouts, enabling flexible and controllable layout conditioning. We present a comprehensive evaluation of our approach, showing a 29.2% increase in character consistency and a 36.2% increase in style similarity compared to previous methods, while displaying high spatial accuracy. Our project page is available at https://yj7082126.github.io/dreamingcomics/",
    "arxiv_url": "https://arxiv.org/abs/2512.01686v1",
    "pdf_url": "https://arxiv.org/pdf/2512.01686v1",
    "published_date": "2025-12-01",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "layout",
      "controllable",
      "video diffusion",
      "dit",
      "identity",
      "evaluation",
      "style"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.01686v1",
      "pdf": "https://arxiv.org/pdf/2512.01686v1",
      "project": "https://yj7082126.github.io/dreamingcomics"
    },
    "bibtex": ""
  },
  {
    "title": "Open-world Hand-Object Interaction Video Generation Based on Structure and Contact-aware Representation",
    "authors": [
      "Haodong Yan",
      "Hang Yu",
      "Zhide Zhong",
      "Weilin Yuan",
      "Xin Gong",
      "Zehang Luo",
      "Chengxi Heyu",
      "Junfeng Li",
      "Wenxuan Song",
      "Shunbo Zhou",
      "Haoang Li"
    ],
    "abstract": "Generating realistic hand-object interactions (HOI) videos is a significant challenge due to the difficulty of modeling physical constraints (e.g., contact and occlusion between hands and manipulated objects). Current methods utilize HOI representation as an auxiliary generative objective to guide video synthesis. However, there is a dilemma between 2D and 3D representations that cannot simultaneously guarantee scalability and interaction fidelity. To address this limitation, we propose a structure and contact-aware representation that captures hand-object contact, hand-object occlusion, and holistic structure context without 3D annotations. This interaction-oriented and scalable supervision signal enables the model to learn fine-grained interaction physics and generalize to open-world scenarios. To fully exploit the proposed representation, we introduce a joint-generation paradigm with a share-and-specialization strategy that generates interaction-oriented representations and videos. Extensive experiments demonstrate that our method outperforms state-of-the-art methods on two real-world datasets in generating physics-realistic and temporally coherent HOI videos. Furthermore, our approach exhibits strong generalization to challenging open-world scenarios, highlighting the benefit of our scalable design. Our project page is https://hgzn258.github.io/SCAR/.",
    "arxiv_url": "https://arxiv.org/abs/2512.01677v1",
    "pdf_url": "https://arxiv.org/pdf/2512.01677v1",
    "published_date": "2025-12-01",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "video synthesis",
      "physics",
      "physical",
      "video generation"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.01677v1",
      "pdf": "https://arxiv.org/pdf/2512.01677v1",
      "project": "https://hgzn258.github.io/SCAR"
    },
    "bibtex": ""
  },
  {
    "title": "ChronosObserver: Taming 4D World with Hyperspace Diffusion Sampling",
    "authors": [
      "Qisen Wang",
      "Yifan Zhao",
      "Peisen Shen",
      "Jialu Li",
      "Jia Li"
    ],
    "abstract": "Although prevailing camera-controlled video generation models can produce cinematic results, lifting them directly to the generation of 3D-consistent and high-fidelity time-synchronized multi-view videos remains challenging, which is a pivotal capability for taming 4D worlds. Some works resort to data augmentation or test-time optimization, but these strategies are constrained by limited model generalization and scalability issues. To this end, we propose ChronosObserver, a training-free method including World State Hyperspace to represent the spatiotemporal constraints of a 4D world scene, and Hyperspace Guided Sampling to synchronize the diffusion sampling trajectories of multiple views using the hyperspace. Experimental results demonstrate that our method achieves high-fidelity and 3D-consistent time-synchronized multi-view videos generation without training or fine-tuning for diffusion models.",
    "arxiv_url": "https://arxiv.org/abs/2512.01481v1",
    "pdf_url": "https://arxiv.org/pdf/2512.01481v1",
    "published_date": "2025-12-01",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "diffusion model",
      "video generation",
      "multi-view video"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.01481v1",
      "pdf": "https://arxiv.org/pdf/2512.01481v1"
    },
    "bibtex": ""
  },
  {
    "title": "AlignVid: Training-Free Attention Scaling for Semantic Fidelity in Text-Guided Image-to-Video Generation",
    "authors": [
      "Yexin Liu",
      "Wen-Jie Shu",
      "Zile Huang",
      "Haoze Zheng",
      "Yueze Wang",
      "Manyuan Zhang",
      "Ser-Nam Lim",
      "Harry Yang"
    ],
    "abstract": "Text-guided image-to-video (TI2V) generation has recently achieved remarkable progress, particularly in maintaining subject consistency and temporal coherence. However, existing methods still struggle to adhere to fine-grained prompt semantics, especially when prompts entail substantial transformations of the input image (e.g., object addition, deletion, or modification), a shortcoming we term semantic negligence. In a pilot study, we find that applying a Gaussian blur to the input image improves semantic adherence. Analyzing attention maps, we observe clearer foreground-background separation. From an energy perspective, this corresponds to a lower-entropy cross-attention distribution. Motivated by this, we introduce AlignVid, a training-free framework with two components: (i) Attention Scaling Modulation (ASM), which directly reweights attention via lightweight Q or K scaling, and (ii) Guidance Scheduling (GS), which applies ASM selectively across transformer blocks and denoising steps to reduce visual quality degradation. This minimal intervention improves prompt adherence while limiting aesthetic degradation. In addition, we introduce OmitI2V to evaluate semantic negligence in TI2V generation, comprising 367 human-annotated samples that span addition, deletion, and modification scenarios. Extensive experiments demonstrate that AlignVid can enhance semantic fidelity.",
    "arxiv_url": "https://arxiv.org/abs/2512.01334v1",
    "pdf_url": "https://arxiv.org/pdf/2512.01334v1",
    "published_date": "2025-12-01",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "dit",
      "video generation",
      "image-to-video",
      "i2v",
      "denoising"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.01334v1",
      "pdf": "https://arxiv.org/pdf/2512.01334v1"
    },
    "bibtex": ""
  },
  {
    "title": "OmniFD: A Unified Model for Versatile Face Forgery Detection",
    "authors": [
      "Haotian Liu",
      "Haoyu Chen",
      "Chenhui Pan",
      "You Hu",
      "Guoying Zhao",
      "Xiaobai Li"
    ],
    "abstract": "Face forgery detection encompasses multiple critical tasks, including identifying forged images and videos and localizing manipulated regions and temporal segments. Current approaches typically employ task-specific models with independent architectures, leading to computational redundancy and ignoring potential correlations across related tasks. We introduce OmniFD, a unified framework that jointly addresses four core face forgery detection tasks within a single model, i.e., image and video classification, spatial localization, and temporal localization. Our architecture consists of three principal components: (1) a shared Swin Transformer encoder that extracts unified 4D spatiotemporal representations from both images and video inputs, (2) a cross-task interaction module with learnable queries that dynamically captures inter-task dependencies through attention-based reasoning, and (3) lightweight decoding heads that transform refined representations into corresponding predictions for all FFD tasks. Extensive experiments demonstrate OmniFD's advantage over task-specific models. Its unified design leverages multi-task learning to capture generalized representations across tasks, especially enabling fine-grained knowledge transfer that facilitates other tasks. For example, video classification accuracy improves by 4.63% when image data are incorporated. Furthermore, by unifying images, videos and the four tasks within one framework, OmniFD achieves superior performance across diverse benchmarks with high efficiency and scalability, e.g., reducing 63% model parameters and 50% training time. It establishes a practical and generalizable solution for comprehensive face forgery detection in real-world applications. The source code is made available at https://github.com/haotianll/OmniFD.",
    "arxiv_url": "https://arxiv.org/abs/2512.01128v1",
    "pdf_url": "https://arxiv.org/pdf/2512.01128v1",
    "published_date": "2025-11-30",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/haotianll/OmniFD",
    "keywords": [
      "benchmark",
      "architecture"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.01128v1",
      "pdf": "https://arxiv.org/pdf/2512.01128v1",
      "github": "https://github.com/haotianll/OmniFD"
    },
    "bibtex": ""
  },
  {
    "title": "Goal-Driven Reward by Video Diffusion Models for Reinforcement Learning",
    "authors": [
      "Qi Wang",
      "Mian Wu",
      "Yuyang Zhang",
      "Mingqi Yuan",
      "Wenyao Zhang",
      "Haoxiang You",
      "Yunbo Wang",
      "Xin Jin",
      "Xiaokang Yang",
      "Wenjun Zeng"
    ],
    "abstract": "Reinforcement Learning (RL) has achieved remarkable success in various domains, yet it often relies on carefully designed programmatic reward functions to guide agent behavior. Designing such reward functions can be challenging and may not generalize well across different tasks. To address this limitation, we leverage the rich world knowledge contained in pretrained video diffusion models to provide goal-driven reward signals for RL agents without ad-hoc design of reward. Our key idea is to exploit off-the-shelf video diffusion models pretrained on large-scale video datasets as informative reward functions in terms of video-level and frame-level goals. For video-level rewards, we first finetune a pretrained video diffusion model on domain-specific datasets and then employ its video encoder to evaluate the alignment between the latent representations of agent's trajectories and the generated goal videos. To enable more fine-grained goal-achievement, we derive a frame-level goal by identifying the most relevant frame from the generated video using CLIP, which serves as the goal state. We then employ a learned forward-backward representation that represents the probability of visiting the goal state from a given state-action pair as frame-level reward, promoting more coherent and goal-driven trajectories. Experiments on various Meta-World tasks demonstrate the effectiveness of our approach.",
    "arxiv_url": "https://arxiv.org/abs/2512.00961v1",
    "pdf_url": "https://arxiv.org/pdf/2512.00961v1",
    "published_date": "2025-11-30",
    "categories": [
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "diffusion model",
      "video diffusion"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.00961v1",
      "pdf": "https://arxiv.org/pdf/2512.00961v1"
    },
    "bibtex": ""
  },
  {
    "title": "PanFlow: Decoupled Motion Control for Panoramic Video Generation",
    "authors": [
      "Cheng Zhang",
      "Hanwen Liang",
      "Donny Y. Chen",
      "Qianyi Wu",
      "Konstantinos N. Plataniotis",
      "Camilo Cruz Gambardella",
      "Jianfei Cai"
    ],
    "abstract": "Panoramic video generation has attracted growing attention due to its applications in virtual reality and immersive media. However, existing methods lack explicit motion control and struggle to generate scenes with large and complex motions. We propose PanFlow, a novel approach that exploits the spherical nature of panoramas to decouple the highly dynamic camera rotation from the input optical flow condition, enabling more precise control over large and dynamic motions. We further introduce a spherical noise warping strategy to promote loop consistency in motion across panorama boundaries. To support effective training, we curate a large-scale, motion-rich panoramic video dataset with frame-level pose and flow annotations. We also showcase the effectiveness of our method in various applications, including motion transfer and video editing. Extensive experiments demonstrate that PanFlow significantly outperforms prior methods in motion fidelity, visual quality, and temporal coherence. Our code, dataset, and models are available at https://github.com/chengzhag/PanFlow.",
    "arxiv_url": "https://arxiv.org/abs/2512.00832v1",
    "pdf_url": "https://arxiv.org/pdf/2512.00832v1",
    "published_date": "2025-11-30",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/chengzhag/PanFlow",
    "keywords": [
      "motion control",
      "video editing",
      "video generation",
      "dit"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.00832v1",
      "pdf": "https://arxiv.org/pdf/2512.00832v1",
      "github": "https://github.com/chengzhag/PanFlow"
    },
    "bibtex": ""
  },
  {
    "title": "Seeing the Wind from a Falling Leaf",
    "authors": [
      "Zhiyuan Gao",
      "Jiageng Mao",
      "Hong-Xing Yu",
      "Haozhe Lou",
      "Emily Yue-Ting Jia",
      "Jernej Barbic",
      "Jiajun Wu",
      "Yue Wang"
    ],
    "abstract": "A longstanding goal in computer vision is to model motions from videos, while the representations behind motions, i.e. the invisible physical interactions that cause objects to deform and move, remain largely unexplored. In this paper, we study how to recover the invisible forces from visual observations, e.g., estimating the wind field by observing a leaf falling to the ground. Our key innovation is an end-to-end differentiable inverse graphics framework, which jointly models object geometry, physical properties, and interactions directly from videos. Through backpropagation, our approach enables the recovery of force representations from object motions. We validate our method on both synthetic and real-world scenarios, and the results demonstrate its ability to infer plausible force fields from videos. Furthermore, we show the potential applications of our approach, including physics-based video generation and editing. We hope our approach sheds light on understanding and modeling the physical process behind pixels, bridging the gap between vision and physics. Please check more video results in our \\href{https://chaoren2357.github.io/seeingthewind/}{project page}.",
    "arxiv_url": "https://arxiv.org/abs/2512.00762v1",
    "pdf_url": "https://arxiv.org/pdf/2512.00762v1",
    "published_date": "2025-11-30",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "physics",
      "physical",
      "video generation",
      "dit"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.00762v1",
      "pdf": "https://arxiv.org/pdf/2512.00762v1",
      "project": "https://chaoren2357.github.io/seeingthewind"
    },
    "bibtex": ""
  },
  {
    "title": "Image Generation as a Visual Planner for Robotic Manipulation",
    "authors": [
      "Ye Pang"
    ],
    "abstract": "Generating realistic robotic manipulation videos is an important step toward unifying perception, planning, and action in embodied agents. While existing video diffusion models require large domain-specific datasets and struggle to generalize, recent image generation models trained on language-image corpora exhibit strong compositionality, including the ability to synthesize temporally coherent grid images. This suggests a latent capacity for video-like generation even without explicit temporal modeling.   We explore whether such models can serve as visual planners for robots when lightly adapted using LoRA finetuning. We propose a two-part framework that includes: (1) text-conditioned generation, which uses a language instruction and the first frame, and (2) trajectory-conditioned generation, which uses a 2D trajectory overlay and the same initial frame. Experiments on the Jaco Play dataset, Bridge V2, and the RT1 dataset show that both modes produce smooth, coherent robot videos aligned with their respective conditions.   Our findings indicate that pretrained image generators encode transferable temporal priors and can function as video-like robotic planners under minimal supervision. Code is released at \\href{https://github.com/pangye202264690373/Image-Generation-as-a-Visual-Planner-for-Robotic-Manipulation}{https://github.com/pangye202264690373/Image-Generation-as-a-Visual-Planner-for-Robotic-Manipulation}.",
    "arxiv_url": "https://arxiv.org/abs/2512.00532v1",
    "pdf_url": "https://arxiv.org/pdf/2512.00532v1",
    "published_date": "2025-11-29",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "https://github.com/pangye202264690373/Image-Generation-as-a-Visua",
    "keywords": [
      "diffusion model",
      "trajectory",
      "video diffusion",
      "dit"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.00532v1",
      "pdf": "https://arxiv.org/pdf/2512.00532v1",
      "github": "https://github.com/pangye202264690373/Image-Generation-as-a-Visual-Planner-for-Robotic-Manipulation"
    },
    "bibtex": ""
  },
  {
    "title": "What about gravity in video generation? Post-Training Newton's Laws with Verifiable Rewards",
    "authors": [
      "Minh-Quan Le",
      "Yuanzhi Zhu",
      "Vicky Kalogeiton",
      "Dimitris Samaras"
    ],
    "abstract": "Recent video diffusion models can synthesize visually compelling clips, yet often violate basic physical laws-objects float, accelerations drift, and collisions behave inconsistently-revealing a persistent gap between visual realism and physical realism. We propose $\\texttt{NewtonRewards}$, the first physics-grounded post-training framework for video generation based on $\\textit{verifiable rewards}$. Instead of relying on human or VLM feedback, $\\texttt{NewtonRewards}$ extracts $\\textit{measurable proxies}$ from generated videos using frozen utility models: optical flow serves as a proxy for velocity, while high-level appearance features serve as a proxy for mass. These proxies enable explicit enforcement of Newtonian structure through two complementary rewards: a Newtonian kinematic constraint enforcing constant-acceleration dynamics, and a mass conservation reward preventing trivial, degenerate solutions. We evaluate $\\texttt{NewtonRewards}$ on five Newtonian Motion Primitives (free fall, horizontal/parabolic throw, and ramp sliding down/up) using our newly constructed large-scale benchmark, $\\texttt{NewtonBench-60K}$. Across all primitives in visual and physics metrics, $\\texttt{NewtonRewards}$ consistently improves physical plausibility, motion smoothness, and temporal coherence over prior post-training methods. It further maintains strong performance under out-of-distribution shifts in height, speed, and friction. Our results show that physics-grounded verifiable rewards offer a scalable path toward physics-aware video generation.",
    "arxiv_url": "https://arxiv.org/abs/2512.00425v1",
    "pdf_url": "https://arxiv.org/pdf/2512.00425v1",
    "published_date": "2025-11-29",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "acceleration",
      "video diffusion",
      "physical",
      "video generation",
      "diffusion model",
      "dynamics",
      "physics-aware",
      "benchmark",
      "physics"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.00425v1",
      "pdf": "https://arxiv.org/pdf/2512.00425v1"
    },
    "bibtex": ""
  },
  {
    "title": "Low-Bitrate Video Compression through Semantic-Conditioned Diffusion",
    "authors": [
      "Lingdong Wang",
      "Guan-Ming Su",
      "Divya Kothandaraman",
      "Tsung-Wei Huang",
      "Mohammad Hajiesmaili",
      "Ramesh K. Sitaraman"
    ],
    "abstract": "Traditional video codecs optimized for pixel fidelity collapse at ultra-low bitrates and produce severe artifacts. This failure arises from a fundamental misalignment between pixel accuracy and human perception. We propose a semantic video compression framework named DiSCo that transmits only the most meaningful information while relying on generative priors for detail synthesis. The source video is decomposed into three compact modalities: a textual description, a spatiotemporally degraded video, and optional sketches or poses that respectively capture semantic, appearance, and motion cues. A conditional video diffusion model then reconstructs high-quality, temporally coherent videos from these compact representations. Temporal forward filling, token interleaving, and modality-specific codecs are proposed to improve multimodal generation and modality compactness. Experiments show that our method outperforms baseline semantic and traditional codecs by 2-10X on perceptual metrics at low bitrates.",
    "arxiv_url": "https://arxiv.org/abs/2512.00408v1",
    "pdf_url": "https://arxiv.org/pdf/2512.00408v1",
    "published_date": "2025-11-29",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "diffusion model",
      "video diffusion",
      "dit"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.00408v1",
      "pdf": "https://arxiv.org/pdf/2512.00408v1"
    },
    "bibtex": ""
  },
  {
    "title": "Chunking Strategies for Multimodal AI Systems",
    "authors": [
      "Shashanka B R",
      "Mohith Charan R",
      "Seema Banu F"
    ],
    "abstract": "Our goal is to consolidate the landscape of multimodal chunking strategies, providing researchers and practitioners with a technical foundation and design space for developing more effective and efficient multimodal AI systems. This survey paves the way for innovations in robust chunking pipelines that scale with modality complexity, enhance processing accuracy, and improve generative coherence in real-world applications. This survey provides a comprehensive taxonomy and technical analysis of chunking strategies tailored for each modality: text, images, audio, video, and cross-modal data. We examine classical and modern approaches such as fixed-size token windowing, recursive text splitting, object-centric visual chunking, silence-based audio segmentation, and scene detection in videos. Each approach is analyzed in terms of its underlying methodology, supporting tools (e.g., LangChain, Detectron2, PySceneDetect), benefits, and challenges, particularly those related to granularity-context trade-offs and multimodal alignment. Furthermore, we explore emerging cross-modal chunking strategies that aim to preserve alignment and semantic consistency across disparate data types [4]. We also include comparative insights, highlight open problems such as asynchronous information density and noisy alignment signals, and identify opportunities for future research in adaptive, learning-based, and task-specific chunking.",
    "arxiv_url": "https://arxiv.org/abs/2512.00185v1",
    "pdf_url": "https://arxiv.org/pdf/2512.00185v1",
    "published_date": "2025-11-28",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "survey"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.00185v1",
      "pdf": "https://arxiv.org/pdf/2512.00185v1"
    },
    "bibtex": ""
  },
  {
    "title": "AnyTalker: Scaling Multi-Person Talking Video Generation with Interactivity Refinement",
    "authors": [
      "Zhizhou Zhong",
      "Yicheng Ji",
      "Zhe Kong",
      "Yiying Liu",
      "Jiarui Wang",
      "Jiasun Feng",
      "Lupeng Liu",
      "Xiangyi Wang",
      "Yanjia Li",
      "Yuqing She",
      "Ying Qin",
      "Huan Li",
      "Shuiyang Mao",
      "Wei Liu",
      "Wenhan Luo"
    ],
    "abstract": "Recently, multi-person video generation has started to gain prominence. While a few preliminary works have explored audio-driven multi-person talking video generation, they often face challenges due to the high costs of diverse multi-person data collection and the difficulty of driving multiple identities with coherent interactivity. To address these challenges, we propose AnyTalker, a multi-person generation framework that features an extensible multi-stream processing architecture. Specifically, we extend Diffusion Transformer's attention block with a novel identity-aware attention mechanism that iteratively processes identity-audio pairs, allowing arbitrary scaling of drivable identities. Besides, training multi-person generative models demands massive multi-person data. Our proposed training pipeline depends solely on single-person videos to learn multi-person speaking patterns and refines interactivity with only a few real multi-person clips. Furthermore, we contribute a targeted metric and dataset designed to evaluate the naturalness and interactivity of the generated multi-person videos. Extensive experiments demonstrate that AnyTalker achieves remarkable lip synchronization, visual quality, and natural interactivity, striking a favorable balance between data costs and identity scalability.",
    "arxiv_url": "https://arxiv.org/abs/2511.23475v1",
    "pdf_url": "https://arxiv.org/pdf/2511.23475v1",
    "published_date": "2025-11-28",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "identity",
      "diffusion transformer",
      "video generation",
      "architecture",
      "audio-driven"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.23475v1",
      "pdf": "https://arxiv.org/pdf/2511.23475v1"
    },
    "bibtex": ""
  },
  {
    "title": "Hunyuan-GameCraft-2: Instruction-following Interactive Game World Model",
    "authors": [
      "Junshu Tang",
      "Jiacheng Liu",
      "Jiaqi Li",
      "Longhuang Wu",
      "Haoyu Yang",
      "Penghao Zhao",
      "Siruis Gong",
      "Xiang Yuan",
      "Shuai Shao",
      "Qinglin Lu"
    ],
    "abstract": "Recent advances in generative world models have enabled remarkable progress in creating open-ended game environments, evolving from static scene synthesis toward dynamic, interactive simulation. However, current approaches remain limited by rigid action schemas and high annotation costs, restricting their ability to model diverse in-game interactions and player-driven dynamics. To address these challenges, we introduce Hunyuan-GameCraft-2, a new paradigm of instruction-driven interaction for generative game world modeling. Instead of relying on fixed keyboard inputs, our model allows users to control game video contents through natural language prompts, keyboard, or mouse signals, enabling flexible and semantically rich interaction within generated worlds. We formally defined the concept of interactive video data and developed an automated process to transform large-scale, unstructured text-video pairs into causally aligned interactive datasets. Built upon a 14B image-to-video Mixture-of-Experts(MoE) foundation model, our model incorporates a text-driven interaction injection mechanism for fine-grained control over camera motion, character behavior, and environment dynamics. We introduce an interaction-focused benchmark, InterBench, to evaluate interaction performance comprehensively. Extensive experiments demonstrate that our model generates temporally coherent and causally grounded interactive game videos that faithfully respond to diverse and free-form user instructions such as \"open the door\", \"draw a torch\", or \"trigger an explosion\".",
    "arxiv_url": "https://arxiv.org/abs/2511.23429v1",
    "pdf_url": "https://arxiv.org/pdf/2511.23429v1",
    "published_date": "2025-11-28",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "interactive",
      "world model",
      "image-to-video",
      "concept",
      "dynamics",
      "simulation",
      "benchmark"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.23429v1",
      "pdf": "https://arxiv.org/pdf/2511.23429v1"
    },
    "bibtex": ""
  },
  {
    "title": "DisMo: Disentangled Motion Representations for Open-World Motion Transfer",
    "authors": [
      "Thomas Ressler-Antal",
      "Frank Fundel",
      "Malek Ben Alaya",
      "Stefan Andreas Baumann",
      "Felix Krause",
      "Ming Gui",
      "Bjrn Ommer"
    ],
    "abstract": "Recent advances in text-to-video (T2V) and image-to-video (I2V) models, have enabled the creation of visually compelling and dynamic videos from simple textual descriptions or initial frames. However, these models often fail to provide an explicit representation of motion separate from content, limiting their applicability for content creators. To address this gap, we propose DisMo, a novel paradigm for learning abstract motion representations directly from raw video data via an image-space reconstruction objective. Our representation is generic and independent of static information such as appearance, object identity, or pose. This enables open-world motion transfer, allowing motion to be transferred across semantically unrelated entities without requiring object correspondences, even between vastly different categories. Unlike prior methods, which trade off motion fidelity and prompt adherence, are overfitting to source structure or drifting from the described action, our approach disentangles motion semantics from appearance, enabling accurate transfer and faithful conditioning. Furthermore, our motion representation can be combined with any existing video generator via lightweight adapters, allowing us to effortlessly benefit from future advancements in video models. We demonstrate the effectiveness of our method through a diverse set of motion transfer tasks. Finally, we show that the learned representations are well-suited for downstream motion understanding tasks, consistently outperforming state-of-the-art video representation models such as V-JEPA in zero-shot action classification on benchmarks including Something-Something v2 and Jester. Project page: https://compvis.github.io/DisMo",
    "arxiv_url": "https://arxiv.org/abs/2511.23428v1",
    "pdf_url": "https://arxiv.org/pdf/2511.23428v1",
    "published_date": "2025-11-28",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "dit",
      "identity",
      "text-to-video",
      "image-to-video",
      "i2v",
      "t2v",
      "benchmark"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.23428v1",
      "pdf": "https://arxiv.org/pdf/2511.23428v1",
      "project": "https://compvis.github.io/DisMo"
    },
    "bibtex": ""
  },
  {
    "title": "Vision Bridge Transformer at Scale",
    "authors": [
      "Zhenxiong Tan",
      "Zeqing Wang",
      "Xingyi Yang",
      "Songhua Liu",
      "Xinchao Wang"
    ],
    "abstract": "We introduce Vision Bridge Transformer (ViBT), a large-scale instantiation of Brownian Bridge Models designed for conditional generation. Unlike traditional diffusion models that transform noise into data, Bridge Models directly model the trajectory between inputs and outputs, creating an efficient data-to-data translation paradigm. By scaling these models to 20B and 1.3B parameters, we demonstrate their effectiveness for image and video translation tasks. To support this scale, we adopt a Transformer architecture and propose a variance-stabilized velocity-matching objective for robust training. Together, these advances highlight the power of scaling Bridge Models for instruction-based image editing and complex video translation.",
    "arxiv_url": "https://arxiv.org/abs/2511.23199v1",
    "pdf_url": "https://arxiv.org/pdf/2511.23199v1",
    "published_date": "2025-11-28",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "dit",
      "trajectory",
      "diffusion model",
      "architecture",
      "efficient",
      "video translation"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.23199v1",
      "pdf": "https://arxiv.org/pdf/2511.23199v1"
    },
    "bibtex": ""
  },
  {
    "title": "GeoWorld: Unlocking the Potential of Geometry Models to Facilitate High-Fidelity 3D Scene Generation",
    "authors": [
      "Yuhao Wan",
      "Lijuan Liu",
      "Jingzhi Zhou",
      "Zihan Zhou",
      "Xuying Zhang",
      "Dongbo Zhang",
      "Shaohui Jiao",
      "Qibin Hou",
      "Ming-Ming Cheng"
    ],
    "abstract": "Previous works leveraging video models for image-to-3D scene generation tend to suffer from geometric distortions and blurry content. In this paper, we renovate the pipeline of image-to-3D scene generation by unlocking the potential of geometry models and present our GeoWorld. Instead of exploiting geometric information obtained from a single-frame input, we propose to first generate consecutive video frames and then take advantage of the geometry model to provide full-frame geometry features, which contain richer information than single-frame depth maps or camera embeddings used in previous methods, and use these geometry features as geometrical conditions to aid the video generation model. To enhance the consistency of geometric structures, we further propose a geometry alignment loss to provide the model with real-world geometric constraints and a geometry adaptation module to ensure the effective utilization of geometry features. Extensive experiments show that our GeoWorld can generate high-fidelity 3D scenes from a single image and a given camera trajectory, outperforming prior methods both qualitatively and quantitatively. Project Page: https://peaes.github.io/GeoWorld/.",
    "arxiv_url": "https://arxiv.org/abs/2511.23191v1",
    "pdf_url": "https://arxiv.org/pdf/2511.23191v1",
    "published_date": "2025-11-28",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "trajectory",
      "video generation",
      "dit"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.23191v1",
      "pdf": "https://arxiv.org/pdf/2511.23191v1",
      "project": "https://peaes.github.io/GeoWorld"
    },
    "bibtex": ""
  },
  {
    "title": "Fast Multi-view Consistent 3D Editing with Video Priors",
    "authors": [
      "Liyi Chen",
      "Ruihuang Li",
      "Guowen Zhang",
      "Pengfei Wang",
      "Lei Zhang"
    ],
    "abstract": "Text-driven 3D editing enables user-friendly 3D object or scene editing with text instructions. Due to the lack of multi-view consistency priors, existing methods typically resort to employing 2D generation or editing models to process each view individually, followed by iterative 2D-3D-2D updating. However, these methods are not only time-consuming but also prone to over-smoothed results because the different editing signals gathered from different views are averaged during the iterative process. In this paper, we propose generative Video Prior based 3D Editing (ViP3DE) to employ the temporal consistency priors from pre-trained video generation models for multi-view consistent 3D editing in a single forward pass. Our key insight is to condition the video generation model on a single edited view to generate other consistent edited views for 3D updating directly, thereby bypassing the iterative editing paradigm. Since 3D updating requires edited views to be paired with specific camera poses, we propose motion-preserved noise blending for the video model to generate edited views at predefined camera poses. In addition, we introduce geometry-aware denoising to further enhance multi-view consistency by integrating 3D geometric priors into video models. Extensive experiments demonstrate that our proposed ViP3DE can achieve high-quality 3D editing results even within a single forward pass, significantly outperforming existing methods in both editing quality and speed.",
    "arxiv_url": "https://arxiv.org/abs/2511.23172v2",
    "pdf_url": "https://arxiv.org/pdf/2511.23172v2",
    "published_date": "2025-11-28",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "dit",
      "denoising",
      "video generation",
      "temporal consistency"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.23172v2",
      "pdf": "https://arxiv.org/pdf/2511.23172v2"
    },
    "bibtex": ""
  },
  {
    "title": "InstanceV: Instance-Level Video Generation",
    "authors": [
      "Yuheng Chen",
      "Teng Hu",
      "Jiangning Zhang",
      "Zhucun Xue",
      "Ran Yi",
      "Lizhuang Ma"
    ],
    "abstract": "Recent advances in text-to-video diffusion models have enabled the generation of high-quality videos conditioned on textual descriptions. However, most existing text-to-video models rely solely on textual conditions, lacking general fine-grained controllability over video generation. To address this challenge, we propose InstanceV, a video generation framework that enables i) instance-level control and ii) global semantic consistency. Specifically, with the aid of proposed Instance-aware Masked Cross-Attention mechanism, InstanceV maximizes the utilization of additional instance-level grounding information to generate correctly attributed instances at designated spatial locations. To improve overall consistency, We introduce the Shared Timestep-Adaptive Prompt Enhancement module, which connects local instances with global semantics in a parameter-efficient manner. Furthermore, we incorporate Spatially-Aware Unconditional Guidance during both training and inference to alleviate the disappearance of small instances. Finally, we propose a new benchmark, named InstanceBench, which combines general video quality metrics with instance-aware metrics for more comprehensive evaluation on instance-level video generation. Extensive experiments demonstrate that InstanceV not only achieves remarkable instance-level controllability in video generation, but also outperforms existing state-of-the-art models in both general quality and instance-aware metrics across qualitative and quantitative evaluations.",
    "arxiv_url": "https://arxiv.org/abs/2511.23146v1",
    "pdf_url": "https://arxiv.org/pdf/2511.23146v1",
    "published_date": "2025-11-28",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "video diffusion",
      "dit",
      "text-to-video",
      "video generation",
      "efficient",
      "evaluation",
      "diffusion model",
      "benchmark"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.23146v1",
      "pdf": "https://arxiv.org/pdf/2511.23146v1"
    },
    "bibtex": ""
  },
  {
    "title": "DualCamCtrl: Dual-Branch Diffusion Model for Geometry-Aware Camera-Controlled Video Generation",
    "authors": [
      "Hongfei Zhang",
      "Kanghao Chen",
      "Zixin Zhang",
      "Harold Haodong Chen",
      "Yuanhuiyi Lyu",
      "Yuqi Zhang",
      "Shuai Yang",
      "Kun Zhou",
      "Yingcong Chen"
    ],
    "abstract": "This paper presents DualCamCtrl, a novel end-to-end diffusion model for camera-controlled video generation. Recent works have advanced this field by representing camera poses as ray-based conditions, yet they often lack sufficient scene understanding and geometric awareness. DualCamCtrl specifically targets this limitation by introducing a dual-branch framework that mutually generates camera-consistent RGB and depth sequences. To harmonize these two modalities, we further propose the Semantic Guided Mutual Alignment (SIGMA) mechanism, which performs RGB-depth fusion in a semantics-guided and mutually reinforced manner. These designs collectively enable DualCamCtrl to better disentangle appearance and geometry modeling, generating videos that more faithfully adhere to the specified camera trajectories. Additionally, we analyze and reveal the distinct influence of depth and camera poses across denoising stages and further demonstrate that early and late stages play complementary roles in forming global structure and refining local details. Extensive experiments demonstrate that DualCamCtrl achieves more consistent camera-controlled video generation, with over 40\\% reduction in camera motion errors compared with prior methods. Our project page: https://soyouthinkyoucantell.github.io/dualcamctrl-page/",
    "arxiv_url": "https://arxiv.org/abs/2511.23127v2",
    "pdf_url": "https://arxiv.org/pdf/2511.23127v2",
    "published_date": "2025-11-28",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "diffusion model",
      "denoising",
      "video generation",
      "dit"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.23127v2",
      "pdf": "https://arxiv.org/pdf/2511.23127v2",
      "project": "https://soyouthinkyoucantell.github.io/dualcamctrl-page"
    },
    "bibtex": ""
  },
  {
    "title": "McSc: Motion-Corrective Preference Alignment for Video Generation with Self-Critic Hierarchical Reasoning",
    "authors": [
      "Qiushi Yang",
      "Yingjie Chen",
      "Yuan Yao",
      "Yifang Men",
      "Huaizhuo Liu",
      "Miaomiao Cui"
    ],
    "abstract": "Text-to-video (T2V) generation has achieved remarkable progress in producing high-quality videos aligned with textual prompts. However, aligning synthesized videos with nuanced human preference remains challenging due to the subjective and multifaceted nature of human judgment. Existing video preference alignment methods rely on costly human annotations or utilize proxy metrics to predict preference, which lacks the understanding of human preference logic. Moreover, they usually directly align T2V models with the overall preference distribution, ignoring potential conflict dimensions like motion dynamics and visual quality, which may bias models towards low-motion content. To address these issues, we present Motion-corrective alignment with Self-critic hierarchical Reasoning (McSc), a three-stage reinforcement learning framework for robust preference modeling and alignment. Firstly, Self-critic Dimensional Reasoning (ScDR) trains a generative reward model (RM) to decompose preferences into per-dimension assessments, using self-critic reasoning chains for reliable learning. Secondly, to achieve holistic video comparison, we introduce Hierarchical Comparative Reasoning (HCR) for structural multi-dimensional reasoning with hierarchical reward supervision. Finally, using RM-preferred videos, we propose Motion-corrective Direct Preference Optimization (McDPO) to optimize T2V models, while dynamically re-weighting alignment objective to mitigate bias towards low-motion content. Experiments show that McSc achieves superior performance in human preference alignment and generates videos with high-motion dynamic.",
    "arxiv_url": "https://arxiv.org/abs/2511.22974v1",
    "pdf_url": "https://arxiv.org/pdf/2511.22974v1",
    "published_date": "2025-11-28",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "dynamics",
      "text-to-video",
      "video generation",
      "t2v"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.22974v1",
      "pdf": "https://arxiv.org/pdf/2511.22974v1"
    },
    "bibtex": ""
  },
  {
    "title": "BlockVid: Block Diffusion for High-Quality and Consistent Minute-Long Video Generation",
    "authors": [
      "Zeyu Zhang",
      "Shuning Chang",
      "Yuanyu He",
      "Yizeng Han",
      "Jiasheng Tang",
      "Fan Wang",
      "Bohan Zhuang"
    ],
    "abstract": "Generating minute-long videos is a critical step toward developing world models, providing a foundation for realistic extended scenes and advanced AI simulators. The emerging semi-autoregressive (block diffusion) paradigm integrates the strengths of diffusion and autoregressive models, enabling arbitrary-length video generation and improving inference efficiency through KV caching and parallel sampling. However, it yet faces two enduring challenges: (i) KV-cache-induced long-horizon error accumulation, and (ii) the lack of fine-grained long-video benchmarks and coherence-aware metrics. To overcome these limitations, we propose BlockVid, a novel block diffusion framework equipped with semantic-aware sparse KV cache, an effective training strategy called Block Forcing, and dedicated chunk-wise noise scheduling and shuffling to reduce error propagation and enhance temporal consistency. We further introduce LV-Bench, a fine-grained benchmark for minute-long videos, complete with new metrics evaluating long-range coherence. Extensive experiments on VBench and LV-Bench demonstrate that BlockVid consistently outperforms existing methods in generating high-quality, coherent minute-long videos. In particular, it achieves a 22.2% improvement on VDE Subject and a 19.4% improvement on VDE Clarity in LV-Bench over the state of the art approaches. Project website: https://ziplab.co/BlockVid. Inferix (Code): https://github.com/alibaba-damo-academy/Inferix.",
    "arxiv_url": "https://arxiv.org/abs/2511.22973v1",
    "pdf_url": "https://arxiv.org/pdf/2511.22973v1",
    "published_date": "2025-11-28",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/alibaba-damo-academy/Inferix",
    "keywords": [
      "temporal consistency",
      "world model",
      "video generation",
      "long video",
      "autoregressive",
      "benchmark"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.22973v1",
      "pdf": "https://arxiv.org/pdf/2511.22973v1",
      "project": "https://ziplab.co/BlockVid",
      "github": "https://github.com/alibaba-damo-academy/Inferix"
    },
    "bibtex": ""
  },
  {
    "title": "RobotSeg: A Model and Dataset for Segmenting Robots in Image and Video",
    "authors": [
      "Haiyang Mei",
      "Qiming Huang",
      "Hai Ci",
      "Mike Zheng Shou"
    ],
    "abstract": "Accurate robot segmentation is a fundamental capability for robotic perception. It enables precise visual servoing for VLA systems, scalable robot-centric data augmentation, accurate real-to-sim transfer, and reliable safety monitoring in dynamic human-robot environments. Despite the strong capabilities of modern segmentation models, surprisingly it remains challenging to segment robots. This is due to robot embodiment diversity, appearance ambiguity, structural complexity, and rapid shape changes. Embracing these challenges, we introduce RobotSeg, a foundation model for robot segmentation in image and video. RobotSeg is built upon the versatile SAM 2 foundation model but addresses its three limitations for robot segmentation, namely the lack of adaptation to articulated robots, reliance on manual prompts, and the need for per-frame training mask annotations, by introducing a structure-enhanced memory associator, a robot prompt generator, and a label-efficient training strategy. These innovations collectively enable a structure-aware, automatic, and label-efficient solution. We further construct the video robot segmentation (VRS) dataset comprising over 2.8k videos (138k frames) with diverse robot embodiments and environments. Extensive experiments demonstrate that RobotSeg achieves state-of-the-art performance on both images and videos, establishing a strong foundation for future advances in robot perception.",
    "arxiv_url": "https://arxiv.org/abs/2511.22950v1",
    "pdf_url": "https://arxiv.org/pdf/2511.22950v1",
    "published_date": "2025-11-28",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "efficient"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.22950v1",
      "pdf": "https://arxiv.org/pdf/2511.22950v1"
    },
    "bibtex": ""
  },
  {
    "title": "One-to-All Animation: Alignment-Free Character Animation and Image Pose Transfer",
    "authors": [
      "Shijun Shi",
      "Jing Xu",
      "Zhihang Li",
      "Chunli Peng",
      "Xiaoda Yang",
      "Lijing Lu",
      "Kai Hu",
      "Jiangning Zhang"
    ],
    "abstract": "Recent advances in diffusion models have greatly improved pose-driven character animation. However, existing methods are limited to spatially aligned reference-pose pairs with matched skeletal structures. Handling reference-pose misalignment remains unsolved. To address this, we present One-to-All Animation, a unified framework for high-fidelity character animation and image pose transfer for references with arbitrary layouts. First, to handle spatially misaligned reference, we reformulate training as a self-supervised outpainting task that transforms diverse-layout reference into a unified occluded-input format. Second, to process partially visible reference, we design a reference extractor for comprehensive identity feature extraction. Further, we integrate hybrid reference fusion attention to handle varying resolutions and dynamic sequence lengths. Finally, from the perspective of generation quality, we introduce identity-robust pose control that decouples appearance from skeletal structure to mitigate pose overfitting, and a token replace strategy for coherent long-video generation. Extensive experiments show that our method outperforms existing approaches. The code and model are available at https://github.com/ssj9596/One-to-All-Animation.",
    "arxiv_url": "https://arxiv.org/abs/2511.22940v2",
    "pdf_url": "https://arxiv.org/pdf/2511.22940v2",
    "published_date": "2025-11-28",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/ssj9596/One-to-All-Animation",
    "keywords": [
      "layout",
      "identity",
      "video generation",
      "diffusion model",
      "outpainting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.22940v2",
      "pdf": "https://arxiv.org/pdf/2511.22940v2",
      "github": "https://github.com/ssj9596/One-to-All-Animation"
    },
    "bibtex": ""
  },
  {
    "title": "TARFVAE: Efficient One-Step Generative Time Series Forecasting via TARFLOW based VAE",
    "authors": [
      "Jiawen Wei",
      "Lan Jiang",
      "Pengbo Wei",
      "Ziwen Ye",
      "Teng Song",
      "Chen Chen",
      "Guangrui Ma"
    ],
    "abstract": "Time series data is ubiquitous, with forecasting applications spanning from finance to healthcare. Beyond popular deterministic methods, generative models are gaining attention due to advancements in areas like image synthesis and video generation, as well as their inherent ability to provide probabilistic predictions. However, existing generative approaches mostly involve recurrent generative operations or repeated denoising steps, making the prediction laborious, particularly for long-term forecasting. Most of them only conduct experiments for relatively short-term forecasting, with limited comparison to deterministic methods in long-term forecasting, leaving their practical advantages unclear. This paper presents TARFVAE, a novel generative framework that combines the Transformer-based autoregressive flow (TARFLOW) and variational autoencoder (VAE) for efficient one-step generative time series forecasting. Inspired by the rethinking that complex architectures for extracting time series representations might not be necessary, we add a flow module, TARFLOW, to VAE to promote spontaneous learning of latent variables that benefit predictions. TARFLOW enhances VAE's posterior estimation by breaking the Gaussian assumption, thereby enabling a more informative latent space. TARFVAE uses only the forward process of TARFLOW, avoiding autoregressive inverse operations and thus ensuring fast generation. During generation, it samples from the prior latent space and directly generates full-horizon forecasts via the VAE decoder. With simple MLP modules, TARFVAE achieves superior performance over state-of-the-art deterministic and generative models across different forecast horizons on benchmark datasets while maintaining efficient prediction speed, demonstrating its effectiveness as an efficient and powerful solution for generative time series forecasting.",
    "arxiv_url": "https://arxiv.org/abs/2511.22853v1",
    "pdf_url": "https://arxiv.org/pdf/2511.22853v1",
    "published_date": "2025-11-28",
    "categories": [
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "video generation",
      "efficient",
      "architecture",
      "denoising",
      "autoregressive",
      "benchmark"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.22853v1",
      "pdf": "https://arxiv.org/pdf/2511.22853v1"
    },
    "bibtex": ""
  },
  {
    "title": "Captain Safari: A World Engine",
    "authors": [
      "Yu-Cheng Chou",
      "Xingrui Wang",
      "Yitong Li",
      "Jiahao Wang",
      "Hanting Liu",
      "Cihang Xie",
      "Alan Yuille",
      "Junfei Xiao"
    ],
    "abstract": "World engines aim to synthesize long, 3D-consistent videos that support interactive exploration of a scene under user-controlled camera motion. However, existing systems struggle under aggressive 6-DoF trajectories and complex outdoor layouts: they lose long-range geometric coherence, deviate from the target path, or collapse into overly conservative motion. To this end, we introduce Captain Safari, a pose-conditioned world engine that generates videos by retrieving from a persistent world memory. Given a camera path, our method maintains a dynamic local memory and uses a retriever to fetch pose-aligned world tokens, which then condition video generation along the trajectory. This design enables the model to maintain stable 3D structure while accurately executing challenging camera maneuvers. To evaluate this setting, we curate OpenSafari, a new in-the-wild FPV dataset containing high-dynamic drone videos with verified camera trajectories, constructed through a multi-stage geometric and kinematic validation pipeline. Across video quality, 3D consistency, and trajectory following, Captain Safari substantially outperforms state-of-the-art camera-controlled generators. It reduces MEt3R from 0.3703 to 0.3690, improves AUC@30 from 0.181 to 0.200, and yields substantially lower FVD than all camera-controlled baselines. More importantly, in a 50-participant, 5-way human study where annotators select the best result among five anonymized models, 67.6% of preferences favor our method across all axes. Our results demonstrate that pose-conditioned world memory is a powerful mechanism for long-horizon, controllable video generation and provide OpenSafari as a challenging new benchmark for future world-engine research.",
    "arxiv_url": "https://arxiv.org/abs/2511.22815v1",
    "pdf_url": "https://arxiv.org/pdf/2511.22815v1",
    "published_date": "2025-11-28",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "interactive",
      "layout",
      "controllable",
      "dit",
      "trajectory",
      "video generation",
      "benchmark"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.22815v1",
      "pdf": "https://arxiv.org/pdf/2511.22815v1"
    },
    "bibtex": ""
  },
  {
    "title": "ReAG: Reasoning-Augmented Generation for Knowledge-based Visual Question Answering",
    "authors": [
      "Alberto Compagnoni",
      "Marco Morini",
      "Sara Sarto",
      "Federico Cocchi",
      "Davide Caffagni",
      "Marcella Cornia",
      "Lorenzo Baraldi",
      "Rita Cucchiara"
    ],
    "abstract": "Multimodal Large Language Models (MLLMs) have shown impressive capabilities in jointly understanding text, images, and videos, often evaluated via Visual Question Answering (VQA). However, even state-of-the-art MLLMs struggle with domain-specific or knowledge-intensive queries, where relevant information is underrepresented in pre-training data. Knowledge-based VQA (KB-VQA) addresses this by retrieving external documents to condition answer generation, but current retrieval-augmented approaches suffer from low precision, noisy passages, and limited reasoning. To address this, we propose ReAG, a novel Reasoning-Augmented Multimodal RAG approach that combines coarse- and fine-grained retrieval with a critic model that filters irrelevant passages, ensuring high-quality additional context. The model follows a multi-stage training strategy leveraging reinforcement learning to enhance reasoning over retrieved content, while supervised fine-tuning serves only as a cold start. Extensive experiments on Encyclopedic-VQA and InfoSeek demonstrate that ReAG significantly outperforms prior methods, improving answer accuracy and providing interpretable reasoning grounded in retrieved evidence. Our source code is publicly available at: https://github.com/aimagelab/ReAG.",
    "arxiv_url": "https://arxiv.org/abs/2511.22715v1",
    "pdf_url": "https://arxiv.org/pdf/2511.22715v1",
    "published_date": "2025-11-27",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.MM"
    ],
    "github_url": "https://github.com/aimagelab/ReAG",
    "keywords": [
      "dit"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.22715v1",
      "pdf": "https://arxiv.org/pdf/2511.22715v1",
      "github": "https://github.com/aimagelab/ReAG"
    },
    "bibtex": ""
  },
  {
    "title": "Fast3Dcache: Training-free 3D Geometry Synthesis Acceleration",
    "authors": [
      "Mengyu Yang",
      "Yanming Yang",
      "Chenyi Xu",
      "Chenxi Song",
      "Yufan Zuo",
      "Tong Zhao",
      "Ruibo Li",
      "Chi Zhang"
    ],
    "abstract": "Diffusion models have achieved impressive generative quality across modalities like 2D images, videos, and 3D shapes, but their inference remains computationally expensive due to the iterative denoising process. While recent caching-based methods effectively reuse redundant computations to speed up 2D and video generation, directly applying these techniques to 3D diffusion models can severely disrupt geometric consistency. In 3D synthesis, even minor numerical errors in cached latent features accumulate, causing structural artifacts and topological inconsistencies. To overcome this limitation, we propose Fast3Dcache, a training-free geometry-aware caching framework that accelerates 3D diffusion inference while preserving geometric fidelity. Our method introduces a Predictive Caching Scheduler Constraint (PCSC) to dynamically determine cache quotas according to voxel stabilization patterns and a Spatiotemporal Stability Criterion (SSC) to select stable features for reuse based on velocity magnitude and acceleration criterion. Comprehensive experiments show that Fast3Dcache accelerates inference significantly, achieving up to a 27.12% speed-up and a 54.8% reduction in FLOPs, with minimal degradation in geometric quality as measured by Chamfer Distance (2.48%) and F-Score (1.95%).",
    "arxiv_url": "https://arxiv.org/abs/2511.22533v1",
    "pdf_url": "https://arxiv.org/pdf/2511.22533v1",
    "published_date": "2025-11-27",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "acceleration",
      "diffusion model",
      "denoising",
      "video generation"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.22533v1",
      "pdf": "https://arxiv.org/pdf/2511.22533v1"
    },
    "bibtex": ""
  },
  {
    "title": "VSpeechLM: A Visual Speech Language Model for Visual Text-to-Speech Task",
    "authors": [
      "Yuyue Wang",
      "Xin Cheng",
      "Yihan Wu",
      "Xihua Wang",
      "Jinchuan Tian",
      "Ruihua Song"
    ],
    "abstract": "The task of Visual Text-to-Speech (VisualTTS), also known as video dubbing, aims to generate speech synchronized with the lip movements in an input video, in additional to being consistent with the content of input text and cloning the timbre of a reference speech. Existing VisualTTS models typically adopt lightweight architectures and design specialized modules to achieve the above goals respectively, yet the speech quality is not satisfied due to the model capacity and the limited data in VisualTTS. Recently, speech large language models (SpeechLLM) show the robust ability to generate high-quality speech. But few work has been done to well leverage temporal cues from video input in generating lip-synchronized speech. To generate both high-quality and lip-synchronized speech in VisualTTS tasks, we propose a novel Visual Speech Language Model called VSpeechLM based upon a SpeechLLM. To capture the synchronization relationship between text and video, we propose a text-video aligner. It first learns fine-grained alignment between phonemes and lip movements, and then outputs an expanded phoneme sequence containing lip-synchronization cues. Next, our proposed SpeechLLM based decoders take the expanded phoneme sequence as input and learns to generate lip-synchronized speech. Extensive experiments demonstrate that our VSpeechLM significantly outperforms previous VisualTTS methods in terms of overall quality, speaker similarity, and synchronization metrics.",
    "arxiv_url": "https://arxiv.org/abs/2511.22229v1",
    "pdf_url": "https://arxiv.org/pdf/2511.22229v1",
    "published_date": "2025-11-27",
    "categories": [
      "cs.MM"
    ],
    "github_url": "",
    "keywords": [
      "architecture",
      "dit"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.22229v1",
      "pdf": "https://arxiv.org/pdf/2511.22229v1"
    },
    "bibtex": ""
  },
  {
    "title": "WorldWander: Bridging Egocentric and Exocentric Worlds in Video Generation",
    "authors": [
      "Quanjian Song",
      "Yiren Song",
      "Kelly Peng",
      "Yuan Gao",
      "Mike Zheng Shou"
    ],
    "abstract": "Video diffusion models have recently achieved remarkable progress in realism and controllability. However, achieving seamless video translation across different perspectives, such as first-person (egocentric) and third-person (exocentric), remains underexplored. Bridging these perspectives is crucial for filmmaking, embodied AI, and world models. Motivated by this, we present WorldWander, an in-context learning framework tailored for translating between egocentric and exocentric worlds in video generation. Building upon advanced video diffusion transformers, WorldWander integrates (i) In-Context Perspective Alignment and (ii) Collaborative Position Encoding to efficiently model cross-view synchronization. To further support our task, we curate EgoExo-8K, a large-scale dataset containing synchronized egocentric-exocentric triplets from both synthetic and real-world scenarios. Experiments demonstrate that WorldWander achieves superior perspective synchronization, character consistency, and generalization, setting a new benchmark for egocentric-exocentric video translation.",
    "arxiv_url": "https://arxiv.org/abs/2511.22098v1",
    "pdf_url": "https://arxiv.org/pdf/2511.22098v1",
    "published_date": "2025-11-27",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "video diffusion",
      "world model",
      "diffusion transformer",
      "video generation",
      "efficient",
      "film",
      "diffusion model",
      "benchmark",
      "video translation"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.22098v1",
      "pdf": "https://arxiv.org/pdf/2511.22098v1"
    },
    "bibtex": ""
  },
  {
    "title": "G$^2$VLM: Geometry Grounded Vision Language Model with Unified 3D Reconstruction and Spatial Reasoning",
    "authors": [
      "Wenbo Hu",
      "Jingli Lin",
      "Yilin Long",
      "Yunlong Ran",
      "Lihan Jiang",
      "Yifan Wang",
      "Chenming Zhu",
      "Runsen Xu",
      "Tai Wang",
      "Jiangmiao Pang"
    ],
    "abstract": "Vision-Language Models (VLMs) still lack robustness in spatial intelligence, demonstrating poor performance on spatial understanding and reasoning tasks. We attribute this gap to the absence of a visual geometry learning process capable of reconstructing 3D space from 2D images. We present G$^2$VLM, a geometry grounded vision-language model that bridges two fundamental aspects of spatial intelligence: spatial 3D reconstruction and spatial understanding. G$^2$VLM natively leverages learned 3D visual geometry features to directly predict 3D attributes and enhance spatial reasoning tasks via in-context learning and interleaved reasoning. Our unified design is highly scalable for spatial understanding: it trains on abundant multi-view image and video data, while simultaneously leveraging the benefits of 3D visual priors that are typically only derived from hard-to-collect annotations. Experimental results demonstrate G$^2$VLM is proficient in both tasks, achieving comparable results to state-of-the-art feed-forward 3D reconstruction models and achieving better or competitive results across spatial understanding and reasoning tasks. By unifying a semantically strong VLM with low-level 3D vision tasks, we hope G$^2$VLM can serve as a strong baseline for the community and unlock more future applications, such as 3D scene editing.",
    "arxiv_url": "https://arxiv.org/abs/2511.21688v2",
    "pdf_url": "https://arxiv.org/pdf/2511.21688v2",
    "published_date": "2025-11-26",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "github_url": "",
    "keywords": [
      "dit"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.21688v2",
      "pdf": "https://arxiv.org/pdf/2511.21688v2"
    },
    "bibtex": ""
  },
  {
    "title": "Qwen3-VL Technical Report",
    "authors": [
      "Shuai Bai",
      "Yuxuan Cai",
      "Ruizhe Chen",
      "Keqin Chen",
      "Xionghui Chen",
      "Zesen Cheng",
      "Lianghao Deng",
      "Wei Ding",
      "Chang Gao",
      "Chunjiang Ge",
      "Wenbin Ge",
      "Zhifang Guo",
      "Qidong Huang",
      "Jie Huang",
      "Fei Huang",
      "Binyuan Hui",
      "Shutong Jiang",
      "Zhaohai Li",
      "Mingsheng Li",
      "Mei Li",
      "Kaixin Li",
      "Zicheng Lin",
      "Junyang Lin",
      "Xuejing Liu",
      "Jiawei Liu",
      "Chenglong Liu",
      "Yang Liu",
      "Dayiheng Liu",
      "Shixuan Liu",
      "Dunjie Lu",
      "Ruilin Luo",
      "Chenxu Lv",
      "Rui Men",
      "Lingchen Meng",
      "Xuancheng Ren",
      "Xingzhang Ren",
      "Sibo Song",
      "Yuchong Sun",
      "Jun Tang",
      "Jianhong Tu",
      "Jianqiang Wan",
      "Peng Wang",
      "Pengfei Wang",
      "Qiuyue Wang",
      "Yuxuan Wang",
      "Tianbao Xie",
      "Yiheng Xu",
      "Haiyang Xu",
      "Jin Xu",
      "Zhibo Yang",
      "Mingkun Yang",
      "Jianxin Yang",
      "An Yang",
      "Bowen Yu",
      "Fei Zhang",
      "Hang Zhang",
      "Xi Zhang",
      "Bo Zheng",
      "Humen Zhong",
      "Jingren Zhou",
      "Fan Zhou",
      "Jing Zhou",
      "Yuanzhi Zhu",
      "Ke Zhu"
    ],
    "abstract": "We introduce Qwen3-VL, the most capable vision-language model in the Qwen series to date, achieving superior performance across a broad range of multimodal benchmarks. It natively supports interleaved contexts of up to 256K tokens, seamlessly integrating text, images, and video. The model family includes both dense (2B/4B/8B/32B) and mixture-of-experts (30B-A3B/235B-A22B) variants to accommodate diverse latency-quality trade-offs. Qwen3-VL delivers three core pillars: (i) markedly stronger pure-text understanding, surpassing comparable text-only backbones in several cases; (ii) robust long-context comprehension with a native 256K-token window for both text and interleaved multimodal inputs, enabling faithful retention, retrieval, and cross-referencing across long documents and videos; and (iii) advanced multimodal reasoning across single-image, multi-image, and video tasks, demonstrating leading performance on comprehensive evaluations such as MMMU and visual-math benchmarks (e.g., MathVista and MathVision). Architecturally, we introduce three key upgrades: (i) an enhanced interleaved-MRoPE for stronger spatial-temporal modeling across images and video; (ii) DeepStack integration, which effectively leverages multi-level ViT features to tighten vision-language alignment; and (iii) text-based time alignment for video, evolving from T-RoPE to explicit textual timestamp alignment for more precise temporal grounding. Under comparable token budgets and latency constraints, Qwen3-VL achieves superior performance in both dense and Mixture-of-Experts (MoE) architectures. We envision Qwen3-VL serving as a foundational engine for image-grounded reasoning, agentic decision-making, and multimodal code intelligence in real-world workflows.",
    "arxiv_url": "https://arxiv.org/abs/2511.21631v2",
    "pdf_url": "https://arxiv.org/pdf/2511.21631v2",
    "published_date": "2025-11-26",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "benchmark",
      "evaluation",
      "architecture"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.21631v2",
      "pdf": "https://arxiv.org/pdf/2511.21631v2"
    },
    "bibtex": ""
  },
  {
    "title": "MoGAN: Improving Motion Quality in Video Diffusion via Few-Step Motion Adversarial Post-Training",
    "authors": [
      "Haotian Xue",
      "Qi Chen",
      "Zhonghao Wang",
      "Xun Huang",
      "Eli Shechtman",
      "Jinrong Xie",
      "Yongxin Chen"
    ],
    "abstract": "Video diffusion models achieve strong frame-level fidelity but still struggle with motion coherence, dynamics and realism, often producing jitter, ghosting, or implausible dynamics. A key limitation is that the standard denoising MSE objective provides no direct supervision on temporal consistency, allowing models to achieve low loss while still generating poor motion. We propose MoGAN, a motion-centric post-training framework that improves motion realism without reward models or human preference data. Built atop a 3-step distilled video diffusion model, we train a DiT-based optical-flow discriminator to differentiate real from generated motion, combined with a distribution-matching regularizer to preserve visual fidelity. With experiments on Wan2.1-T2V-1.3B, MoGAN substantially improves motion quality across benchmarks. On VBench, MoGAN boosts motion score by +7.3% over the 50-step teacher and +13.3% over the 3-step DMD model. On VideoJAM-Bench, MoGAN improves motion score by +7.4% over the teacher and +8.8% over DMD, while maintaining comparable or even better aesthetic and image-quality scores. A human study further confirms that MoGAN is preferred for motion quality (52% vs. 38% for the teacher; 56% vs. 29% for DMD). Overall, MoGAN delivers significantly more realistic motion without sacrificing visual fidelity or efficiency, offering a practical path toward fast, high-quality video generation. Project webpage is: https://xavihart.github.io/mogan.",
    "arxiv_url": "https://arxiv.org/abs/2511.21592v1",
    "pdf_url": "https://arxiv.org/pdf/2511.21592v1",
    "published_date": "2025-11-26",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "temporal consistency",
      "video diffusion",
      "dit",
      "video generation",
      "diffusion model",
      "denoising",
      "dynamics",
      "t2v",
      "benchmark"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.21592v1",
      "pdf": "https://arxiv.org/pdf/2511.21592v1",
      "project": "https://xavihart.github.io/mogan"
    },
    "bibtex": ""
  },
  {
    "title": "Harmony: Harmonizing Audio and Video Generation through Cross-Task Synergy",
    "authors": [
      "Teng Hu",
      "Zhentao Yu",
      "Guozhen Zhang",
      "Zihan Su",
      "Zhengguang Zhou",
      "Youliang Zhang",
      "Yuan Zhou",
      "Qinglin Lu",
      "Ran Yi"
    ],
    "abstract": "The synthesis of synchronized audio-visual content is a key challenge in generative AI, with open-source models facing challenges in robust audio-video alignment. Our analysis reveals that this issue is rooted in three fundamental challenges of the joint diffusion process: (1) Correspondence Drift, where concurrently evolving noisy latents impede stable learning of alignment; (2) inefficient global attention mechanisms that fail to capture fine-grained temporal cues; and (3) the intra-modal bias of conventional Classifier-Free Guidance (CFG), which enhances conditionality but not cross-modal synchronization. To overcome these challenges, we introduce Harmony, a novel framework that mechanistically enforces audio-visual synchronization. We first propose a Cross-Task Synergy training paradigm to mitigate drift by leveraging strong supervisory signals from audio-driven video and video-driven audio generation tasks. Then, we design a Global-Local Decoupled Interaction Module for efficient and precise temporal-style alignment. Finally, we present a novel Synchronization-Enhanced CFG (SyncCFG) that explicitly isolates and amplifies the alignment signal during inference. Extensive experiments demonstrate that Harmony establishes a new state-of-the-art, significantly outperforming existing methods in both generation fidelity and, critically, in achieving fine-grained audio-visual synchronization.",
    "arxiv_url": "https://arxiv.org/abs/2511.21579v2",
    "pdf_url": "https://arxiv.org/pdf/2511.21579v2",
    "published_date": "2025-11-26",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "dit",
      "video generation",
      "style",
      "efficient",
      "audio-driven"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.21579v2",
      "pdf": "https://arxiv.org/pdf/2511.21579v2"
    },
    "bibtex": ""
  },
  {
    "title": "Video Generation Models Are Good Latent Reward Models",
    "authors": [
      "Xiaoyue Mi",
      "Wenqing Yu",
      "Jiesong Lian",
      "Shibo Jie",
      "Ruizhe Zhong",
      "Zijun Liu",
      "Guozhen Zhang",
      "Zixiang Zhou",
      "Zhiyong Xu",
      "Yuan Zhou",
      "Qinglin Lu",
      "Fan Tang"
    ],
    "abstract": "Reward feedback learning (ReFL) has proven effective for aligning image generation with human preferences. However, its extension to video generation faces significant challenges. Existing video reward models rely on vision-language models designed for pixel-space inputs, confining ReFL optimization to near-complete denoising steps after computationally expensive VAE decoding. This pixel-space approach incurs substantial memory overhead and increased training time, and its late-stage optimization lacks early-stage supervision, refining only visual quality rather than fundamental motion dynamics and structural coherence. In this work, we show that pre-trained video generation models are naturally suited for reward modeling in the noisy latent space, as they are explicitly designed to process noisy latent representations at arbitrary timesteps and inherently preserve temporal information through their sequential modeling capabilities. Accordingly, we propose Process Reward Feedback Learning~(PRFL), a framework that conducts preference optimization entirely in latent space, enabling efficient gradient backpropagation throughout the full denoising chain without VAE decoding. Extensive experiments demonstrate that PRFL significantly improves alignment with human preferences, while achieving substantial reductions in memory consumption and training time compared to RGB ReFL.",
    "arxiv_url": "https://arxiv.org/abs/2511.21541v3",
    "pdf_url": "https://arxiv.org/pdf/2511.21541v3",
    "published_date": "2025-11-26",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "dynamics",
      "denoising",
      "video generation"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.21541v3",
      "pdf": "https://arxiv.org/pdf/2511.21541v3"
    },
    "bibtex": ""
  },
  {
    "title": "MobileI2V: Fast and High-Resolution Image-to-Video on Mobile Devices",
    "authors": [
      "Shuai Zhang",
      "Bao Tang",
      "Siyuan Yu",
      "Yueting Zhu",
      "Jingfeng Yao",
      "Ya Zou",
      "Shanglin Yuan",
      "Li Yu",
      "Wenyu Liu",
      "Xinggang Wang"
    ],
    "abstract": "Recently, video generation has witnessed rapid advancements, drawing increasing attention to image-to-video (I2V) synthesis on mobile devices. However, the substantial computational complexity and slow generation speed of diffusion models pose significant challenges for real-time, high-resolution video generation on resource-constrained mobile devices. In this work, we propose MobileI2V, a 270M lightweight diffusion model for real-time image-to-video generation on mobile devices. The core lies in: (1) We analyzed the performance of linear attention modules and softmax attention modules on mobile devices, and proposed a linear hybrid architecture denoiser that balances generation efficiency and quality. (2) We design a time-step distillation strategy that compresses the I2V sampling steps from more than 20 to only two without significant quality loss, resulting in a 10-fold increase in generation speed. (3) We apply mobile-specific attention optimizations that yield a 2-fold speed-up for attention operations during on-device inference. MobileI2V enables, for the first time, fast 720p image-to-video generation on mobile devices, with quality comparable to existing models. Under one-step conditions, the generation speed of each frame of 720p video is less than 100 ms. Our code is available at: https://github.com/hustvl/MobileI2V.",
    "arxiv_url": "https://arxiv.org/abs/2511.21475v1",
    "pdf_url": "https://arxiv.org/pdf/2511.21475v1",
    "published_date": "2025-11-26",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/hustvl/MobileI2V",
    "keywords": [
      "distillation",
      "dit",
      "video generation",
      "image-to-video",
      "i2v",
      "diffusion model",
      "architecture"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.21475v1",
      "pdf": "https://arxiv.org/pdf/2511.21475v1",
      "github": "https://github.com/hustvl/MobileI2V"
    },
    "bibtex": ""
  },
  {
    "title": "3MDiT: Unified Tri-Modal Diffusion Transformer for Text-Driven Synchronized Audio-Video Generation",
    "authors": [
      "Yaoru Li",
      "Heyu Si",
      "Federico Landi",
      "Pilar Oplustil Gallegos",
      "Ioannis Koutsoumpas",
      "O. Ricardo Cortez Vazquez",
      "Ruiju Fu",
      "Qi Guo",
      "Xin Jin",
      "Shunyu Liu",
      "Mingli Song"
    ],
    "abstract": "Text-to-video (T2V) diffusion models have recently achieved impressive visual quality, yet most systems still generate silent clips and treat audio as a secondary concern. Existing audio-video generation pipelines typically decompose the task into cascaded stages, which accumulate errors across modalities and are trained under separate objectives. Recent joint audio-video generators alleviate this issue but often rely on dual-tower architectures with ad-hoc cross-modal bridges and static, single-shot text conditioning, making it difficult to both reuse T2V backbones and to reason about how audio, video and language interact over time. To address these challenges, we propose 3MDiT, a unified tri-modal diffusion transformer for text-driven synchronized audio-video generation. Our framework models video, audio and text as jointly evolving streams: an isomorphic audio branch mirrors a T2V backbone, tri-modal omni-blocks perform feature-level fusion across the three modalities, and an optional dynamic text conditioning mechanism updates the text representation as audio and video evidence co-evolve. The design supports two regimes: training from scratch on audio-video data, and orthogonally adapting a pretrained T2V model without modifying its backbone. Experiments show that our approach generates high-quality videos and realistic audio while consistently improving audio-video synchronization and tri-modal alignment across a range of quantitative metrics.",
    "arxiv_url": "https://arxiv.org/abs/2511.21780v1",
    "pdf_url": "https://arxiv.org/pdf/2511.21780v1",
    "published_date": "2025-11-26",
    "categories": [
      "cs.MM",
      "cs.SD"
    ],
    "github_url": "",
    "keywords": [
      "dit",
      "diffusion transformer",
      "text-to-video",
      "video generation",
      "diffusion model",
      "architecture",
      "t2v"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.21780v1",
      "pdf": "https://arxiv.org/pdf/2511.21780v1"
    },
    "bibtex": ""
  },
  {
    "title": "TEAR: Temporal-aware Automated Red-teaming for Text-to-Video Models",
    "authors": [
      "Jiaming He",
      "Guanyu Hou",
      "Hongwei Li",
      "Zhicong Huang",
      "Kangjie Chen",
      "Yi Yu",
      "Wenbo Jiang",
      "Guowen Xu",
      "Tianwei Zhang"
    ],
    "abstract": "Text-to-Video (T2V) models are capable of synthesizing high-quality, temporally coherent dynamic video content, but the diverse generation also inherently introduces critical safety challenges. Existing safety evaluation methods,which focus on static image and text generation, are insufficient to capture the complex temporal dynamics in video generation. To address this, we propose a TEmporal-aware Automated Red-teaming framework, named TEAR, an automated framework designed to uncover safety risks specifically linked to the dynamic temporal sequencing of T2V models. TEAR employs a temporal-aware test generator optimized via a two-stage approach: initial generator training and temporal-aware online preference learning, to craft textually innocuous prompts that exploit temporal dynamics to elicit policy-violating video output. And a refine model is adopted to improve the prompt stealthiness and adversarial effectiveness cyclically. Extensive experimental evaluation demonstrates the effectiveness of TEAR across open-source and commercial T2V systems with over 80% attack success rate, a significant boost from prior best result of 57%.",
    "arxiv_url": "https://arxiv.org/abs/2511.21145v1",
    "pdf_url": "https://arxiv.org/pdf/2511.21145v1",
    "published_date": "2025-11-26",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "text-to-video",
      "video generation",
      "evaluation",
      "dynamics",
      "t2v"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.21145v1",
      "pdf": "https://arxiv.org/pdf/2511.21145v1"
    },
    "bibtex": ""
  },
  {
    "title": "Efficient Training for Human Video Generation with Entropy-Guided Prioritized Progressive Learning",
    "authors": [
      "Changlin Li",
      "Jiawei Zhang",
      "Shuhao Liu",
      "Sihao Lin",
      "Zeyi Shi",
      "Zhihui Li",
      "Xiaojun Chang"
    ],
    "abstract": "Human video generation has advanced rapidly with the development of diffusion models, but the high computational cost and substantial memory consumption associated with training these models on high-resolution, multi-frame data pose significant challenges. In this paper, we propose Entropy-Guided Prioritized Progressive Learning (Ent-Prog), an efficient training framework tailored for diffusion models on human video generation. First, we introduce Conditional Entropy Inflation (CEI) to assess the importance of different model components on the target conditional generation task, enabling prioritized training of the most critical components. Second, we introduce an adaptive progressive schedule that adaptively increases computational complexity during training by measuring the convergence efficiency. Ent-Prog reduces both training time and GPU memory consumption while maintaining model performance. Extensive experiments across three datasets, demonstrate the effectiveness of Ent-Prog, achieving up to 2.2$\\times$ training speedup and 2.4$\\times$ GPU memory reduction without compromising generative performance.",
    "arxiv_url": "https://arxiv.org/abs/2511.21136v1",
    "pdf_url": "https://arxiv.org/pdf/2511.21136v1",
    "published_date": "2025-11-26",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "diffusion model",
      "video generation",
      "dit"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.21136v1",
      "pdf": "https://arxiv.org/pdf/2511.21136v1"
    },
    "bibtex": ""
  },
  {
    "title": "CtrlVDiff: Controllable Video Generation via Unified Multimodal Video Diffusion",
    "authors": [
      "Dianbing Xi",
      "Jiepeng Wang",
      "Yuanzhi Liang",
      "Xi Qiu",
      "Jialun Liu",
      "Hao Pan",
      "Yuchi Huo",
      "Rui Wang",
      "Haibin Huang",
      "Chi Zhang",
      "Xuelong Li"
    ],
    "abstract": "We tackle the dual challenges of video understanding and controllable video generation within a unified diffusion framework. Our key insights are two-fold: geometry-only cues (e.g., depth, edges) are insufficient: they specify layout but under-constrain appearance, materials, and illumination, limiting physically meaningful edits such as relighting or material swaps and often causing temporal drift. Enriching the model with additional graphics-based modalities (intrinsics and semantics) provides complementary constraints that both disambiguate understanding and enable precise, predictable control during generation.   However, building a single model that uses many heterogeneous cues introduces two core difficulties. Architecturally, the model must accept any subset of modalities, remain robust to missing inputs, and inject control signals without sacrificing temporal consistency. Data-wise, training demands large-scale, temporally aligned supervision that ties real videos to per-pixel multimodal annotations.   We then propose CtrlVDiff, a unified diffusion model trained with a Hybrid Modality Control Strategy (HMCS) that routes and fuses features from depth, normals, segmentation, edges, and graphics-based intrinsics (albedo, roughness, metallic), and re-renders videos from any chosen subset with strong temporal coherence. To enable this, we build MMVideo, a hybrid real-and-synthetic dataset aligned across modalities and captions. Across understanding and generation benchmarks, CtrlVDiff delivers superior controllability and fidelity, enabling layer-wise edits (relighting, material adjustment, object insertion) and surpassing state-of-the-art baselines while remaining robust when some modalities are unavailable.",
    "arxiv_url": "https://arxiv.org/abs/2511.21129v1",
    "pdf_url": "https://arxiv.org/pdf/2511.21129v1",
    "published_date": "2025-11-26",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "layout",
      "controllable",
      "video diffusion",
      "temporal consistency",
      "dit",
      "physical",
      "video generation",
      "diffusion model",
      "benchmark"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.21129v1",
      "pdf": "https://arxiv.org/pdf/2511.21129v1"
    },
    "bibtex": ""
  },
  {
    "title": "Layer-Aware Video Composition via Split-then-Merge",
    "authors": [
      "Ozgur Kara",
      "Yujia Chen",
      "Ming-Hsuan Yang",
      "James M. Rehg",
      "Wen-Sheng Chu",
      "Du Tran"
    ],
    "abstract": "We present Split-then-Merge (StM), a novel framework designed to enhance control in generative video composition and address its data scarcity problem. Unlike conventional methods relying on annotated datasets or handcrafted rules, StM splits a large corpus of unlabeled videos into dynamic foreground and background layers, then self-composes them to learn how dynamic subjects interact with diverse scenes. This process enables the model to learn the complex compositional dynamics required for realistic video generation. StM introduces a novel transformation-aware training pipeline that utilizes a multi-layer fusion and augmentation to achieve affordance-aware composition, alongside an identity-preservation loss that maintains foreground fidelity during blending. Experiments show StM outperforms SoTA methods in both quantitative benchmarks and in humans/VLLM-based qualitative evaluations. More details are available at our project page: https://split-then-merge.github.io",
    "arxiv_url": "https://arxiv.org/abs/2511.20809v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20809v1",
    "published_date": "2025-11-25",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "identity",
      "video generation",
      "evaluation",
      "dynamics",
      "benchmark"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.20809v1",
      "pdf": "https://arxiv.org/pdf/2511.20809v1",
      "project": "https://split-then-merge.github.io"
    },
    "bibtex": ""
  },
  {
    "title": "Infinity-RoPE: Action-Controllable Infinite Video Generation Emerges From Autoregressive Self-Rollout",
    "authors": [
      "Hidir Yesiltepe",
      "Tuna Han Salih Meral",
      "Adil Kaan Akan",
      "Kaan Oktay",
      "Pinar Yanardag"
    ],
    "abstract": "Current autoregressive video diffusion models are constrained by three core bottlenecks: (i) the finite temporal horizon imposed by the base model's 3D Rotary Positional Embedding (3D-RoPE), (ii) slow prompt responsiveness in maintaining fine-grained action control during long-form rollouts, and (iii) the inability to realize discontinuous cinematic transitions within a single generation stream. We introduce $\\infty$-RoPE, a unified inference-time framework that addresses all three limitations through three interconnected components: Block-Relativistic RoPE, KV Flush, and RoPE Cut. Block-Relativistic RoPE reformulates temporal encoding as a moving local reference frame, where each newly generated latent block is rotated relative to the base model's maximum frame horizon while earlier blocks are rotated backward to preserve relative temporal geometry. This relativistic formulation eliminates fixed temporal positions, enabling continuous video generation far beyond the base positional limits. To obtain fine-grained action control without re-encoding, KV Flush renews the KV cache by retaining only two latent frames, the global sink and the last generated latent frame, thereby ensuring immediate prompt responsiveness. Finally, RoPE Cut introduces controlled discontinuities in temporal RoPE coordinates, enabling multi-cut scene transitions within a single continuous rollout. Together, these components establish $\\infty$-RoPE as a training-free foundation for infinite-horizon, controllable, and cinematic video diffusion. Comprehensive experiments show that $\\infty$-RoPE consistently surpasses previous autoregressive models in overall VBench scores.",
    "arxiv_url": "https://arxiv.org/abs/2511.20649v2",
    "pdf_url": "https://arxiv.org/pdf/2511.20649v2",
    "published_date": "2025-11-25",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "controllable",
      "video diffusion",
      "long-form",
      "video generation",
      "diffusion model",
      "autoregressive"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.20649v2",
      "pdf": "https://arxiv.org/pdf/2511.20649v2"
    },
    "bibtex": ""
  },
  {
    "title": "Diverse Video Generation with Determinantal Point Process-Guided Policy Optimization",
    "authors": [
      "Tahira Kazimi",
      "Connor Dunlop",
      "Pinar Yanardag"
    ],
    "abstract": "While recent text-to-video (T2V) diffusion models have achieved impressive quality and prompt alignment, they often produce low-diversity outputs when sampling multiple videos from a single text prompt. We tackle this challenge by formulating it as a set-level policy optimization problem, with the goal of training a policy that can cover the diverse range of plausible outcomes for a given prompt. To address this, we introduce DPP-GRPO, a novel framework for diverse video generation that combines Determinantal Point Processes (DPPs) and Group Relative Policy Optimization (GRPO) theories to enforce explicit reward on diverse generations. Our objective turns diversity into an explicit signal by imposing diminishing returns on redundant samples (via DPP) while supplies groupwise feedback over candidate sets (via GRPO). Our framework is plug-and-play and model-agnostic, and encourages diverse generations across visual appearance, camera motions, and scene structure without sacrificing prompt fidelity or perceptual quality. We implement our method on WAN and CogVideoX, and show that our method consistently improves video diversity on state-of-the-art benchmarks such as VBench, VideoScore, and human preference studies. Moreover, we release our code and a new benchmark dataset of 30,000 diverse prompts to support future research.",
    "arxiv_url": "https://arxiv.org/abs/2511.20647v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20647v1",
    "published_date": "2025-11-25",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "text-to-video",
      "video generation",
      "diffusion model",
      "t2v",
      "benchmark"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.20647v1",
      "pdf": "https://arxiv.org/pdf/2511.20647v1"
    },
    "bibtex": ""
  },
  {
    "title": "MotionV2V: Editing Motion in a Video",
    "authors": [
      "Ryan Burgert",
      "Charles Herrmann",
      "Forrester Cole",
      "Michael S Ryoo",
      "Neal Wadhwa",
      "Andrey Voynov",
      "Nataniel Ruiz"
    ],
    "abstract": "While generative video models have achieved remarkable fidelity and consistency, applying these capabilities to video editing remains a complex challenge. Recent research has explored motion controllability as a means to enhance text-to-video generation or image animation; however, we identify precise motion control as a promising yet under-explored paradigm for editing existing videos. In this work, we propose modifying video motion by directly editing sparse trajectories extracted from the input. We term the deviation between input and output trajectories a \"motion edit\" and demonstrate that this representation, when coupled with a generative backbone, enables powerful video editing capabilities. To achieve this, we introduce a pipeline for generating \"motion counterfactuals\", video pairs that share identical content but distinct motion, and we fine-tune a motion-conditioned video diffusion architecture on this dataset. Our approach allows for edits that start at any timestamp and propagate naturally. In a four-way head-to-head user study, our model achieves over 65 percent preference against prior work. Please see our project page: https://ryanndagreat.github.io/MotionV2V",
    "arxiv_url": "https://arxiv.org/abs/2511.20640v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20640v1",
    "published_date": "2025-11-25",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "video diffusion",
      "dit",
      "video editing",
      "text-to-video",
      "video generation",
      "architecture",
      "motion control",
      "image animation"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.20640v1",
      "pdf": "https://arxiv.org/pdf/2511.20640v1",
      "project": "https://ryanndagreat.github.io/MotionV2V"
    },
    "bibtex": ""
  },
  {
    "title": "MapReduce LoRA: Advancing the Pareto Front in Multi-Preference Optimization for Generative Models",
    "authors": [
      "Chieh-Yun Chen",
      "Zhonghao Wang",
      "Qi Chen",
      "Zhifan Ye",
      "Min Shi",
      "Yue Zhao",
      "Yinan Zhao",
      "Hui Qu",
      "Wei-An Lin",
      "Yiru Shen",
      "Ajinkya Kale",
      "Irfan Essa",
      "Humphrey Shi"
    ],
    "abstract": "Reinforcement learning from human feedback (RLHF) with reward models has advanced alignment of generative models to human aesthetic and perceptual preferences. However, jointly optimizing multiple rewards often incurs an alignment tax, improving one dimension while degrading others. To address this, we introduce two complementary methods: MapReduce LoRA and Reward-aware Token Embedding (RaTE). MapReduce LoRA trains preference-specific LoRA experts in parallel and iteratively merges them to refine a shared base model; RaTE learns reward-specific token embeddings that compose at inference for flexible preference control. Experiments on Text-to-Image generation (Stable Diffusion 3.5 Medium and FLUX.1-dev) show improvements of 36.1%, 4.6%, and 55.7%, and 32.7%, 4.3%, and 67.1% on GenEval, PickScore, and OCR, respectively. On Text-to-Video generation (HunyuanVideo), visual and motion quality improve by 48.1% and 90.0%, respectively. On the language task, Helpful Assistant, with Llama-2 7B, helpful and harmless improve by 43.4% and 136.7%, respectively. Our framework sets a new state-of-the-art multi-preference alignment recipe across modalities.",
    "arxiv_url": "https://arxiv.org/abs/2511.20629v2",
    "pdf_url": "https://arxiv.org/pdf/2511.20629v2",
    "published_date": "2025-11-25",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "text-to-video",
      "video generation"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.20629v2",
      "pdf": "https://arxiv.org/pdf/2511.20629v2"
    },
    "bibtex": ""
  },
  {
    "title": "ShapeGen: Towards High-Quality 3D Shape Synthesis",
    "authors": [
      "Yangguang Li",
      "Xianglong He",
      "Zi-Xin Zou",
      "Zexiang Liu",
      "Wanli Ouyang",
      "Ding Liang",
      "Yan-Pei Cao"
    ],
    "abstract": "Inspired by generative paradigms in image and video, 3D shape generation has made notable progress, enabling the rapid synthesis of high-fidelity 3D assets from a single image. However, current methods still face challenges, including the lack of intricate details, overly smoothed surfaces, and fragmented thin-shell structures. These limitations leave the generated 3D assets still one step short of meeting the standards favored by artists. In this paper, we present ShapeGen, which achieves high-quality image-to-3D shape generation through 3D representation and supervision improvements, resolution scaling up, and the advantages of linear transformers. These advancements allow the generated assets to be seamlessly integrated into 3D pipelines, facilitating their widespread adoption across various applications. Through extensive experiments, we validate the impact of these improvements on overall performance. Ultimately, thanks to the synergistic effects of these enhancements, ShapeGen achieves a significant leap in image-to-3D generation, establishing a new state-of-the-art performance.",
    "arxiv_url": "https://arxiv.org/abs/2511.20624v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20624v1",
    "published_date": "2025-11-25",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.20624v1",
      "pdf": "https://arxiv.org/pdf/2511.20624v1"
    },
    "bibtex": ""
  },
  {
    "title": "A Reason-then-Describe Instruction Interpreter for Controllable Video Generation",
    "authors": [
      "Shengqiong Wu",
      "Weicai Ye",
      "Yuanxing Zhang",
      "Jiahao Wang",
      "Quande Liu",
      "Xintao Wang",
      "Pengfei Wan",
      "Kun Gai",
      "Hao Fei",
      "Tat-Seng Chua"
    ],
    "abstract": "Diffusion Transformers have significantly improved video fidelity and temporal coherence, however, practical controllability remains limited. Concise, ambiguous, and compositionally complex user inputs contrast with the detailed prompts used in training, yielding an intent-output mismatch. We propose ReaDe, a universal, model-agnostic interpreter that converts raw instructions into precise, actionable specifications for downstream video generators. ReaDe follows a reason-then-describe paradigm: it first analyzes the user request to identify core requirements and resolve ambiguities, then produces detailed guidance that enables faithful, controllable generation. We train ReaDe via a two-stage optimization: (i) reasoning-augmented supervision imparts analytic parsing with stepwise traces and dense captions, and (ii) a multi-dimensional reward assigner enables stable, feedback-driven refinement for natural-style captions. Experiments across single- and multi-condition scenarios show consistent gains in instruction fidelity, caption accuracy, and downstream video quality, with strong generalization to reasoning-intensive and unseen inputs. ReaDe offers a practical route to aligning controllable video generation with accurately interpreted user intent. Project Page: https://sqwu.top/ReaDe/.",
    "arxiv_url": "https://arxiv.org/abs/2511.20563v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20563v1",
    "published_date": "2025-11-25",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "controllable",
      "dit",
      "diffusion transformer",
      "video generation",
      "style"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.20563v1",
      "pdf": "https://arxiv.org/pdf/2511.20563v1",
      "project": "https://sqwu.top/ReaDe"
    },
    "bibtex": ""
  },
  {
    "title": "PhysChoreo: Physics-Controllable Video Generation with Part-Aware Semantic Grounding",
    "authors": [
      "Haoze Zhang",
      "Tianyu Huang",
      "Zichen Wan",
      "Xiaowei Jin",
      "Hongzhi Zhang",
      "Hui Li",
      "Wangmeng Zuo"
    ],
    "abstract": "While recent video generation models have achieved significant visual fidelity, they often suffer from the lack of explicit physical controllability and plausibility. To address this, some recent studies attempted to guide the video generation with physics-based rendering. However, these methods face inherent challenges in accurately modeling complex physical properties and effectively control ling the resulting physical behavior over extended temporal sequences. In this work, we introduce PhysChoreo, a novel framework that can generate videos with diverse controllability and physical realism from a single image. Our method consists of two stages: first, it estimates the static initial physical properties of all objects in the image through part-aware physical property reconstruction. Then, through temporally instructed and physically editable simulation, it synthesizes high-quality videos with rich dynamic behaviors and physical realism. Experimental results show that PhysChoreo can generate videos with rich behaviors and physical realism, outperforming state-of-the-art methods on multiple evaluation metrics.",
    "arxiv_url": "https://arxiv.org/abs/2511.20562v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20562v1",
    "published_date": "2025-11-25",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "controllable",
      "dit",
      "physical",
      "video generation",
      "evaluation",
      "simulation",
      "physics"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.20562v1",
      "pdf": "https://arxiv.org/pdf/2511.20562v1"
    },
    "bibtex": ""
  },
  {
    "title": "STARFlow-V: End-to-End Video Generative Modeling with Normalizing Flows",
    "authors": [
      "Jiatao Gu",
      "Ying Shen",
      "Tianrong Chen",
      "Laurent Dinh",
      "Yuyang Wang",
      "Miguel Angel Bautista",
      "David Berthelot",
      "Josh Susskind",
      "Shuangfei Zhai"
    ],
    "abstract": "Normalizing flows (NFs) are end-to-end likelihood-based generative models for continuous data, and have recently regained attention with encouraging progress on image generation. Yet in the video generation domain, where spatiotemporal complexity and computational cost are substantially higher, state-of-the-art systems almost exclusively rely on diffusion-based models. In this work, we revisit this design space by presenting STARFlow-V, a normalizing flow-based video generator with substantial benefits such as end-to-end learning, robust causal prediction, and native likelihood estimation. Building upon the recently proposed STARFlow, STARFlow-V operates in the spatiotemporal latent space with a global-local architecture which restricts causal dependencies to a global latent space while preserving rich local within-frame interactions. This eases error accumulation over time, a common pitfall of standard autoregressive diffusion model generation. Additionally, we propose flow-score matching, which equips the model with a light-weight causal denoiser to improve the video generation consistency in an autoregressive fashion. To improve the sampling efficiency, STARFlow-V employs a video-aware Jacobi iteration scheme that recasts inner updates as parallelizable iterations without breaking causality. Thanks to the invertible structure, the same model can natively support text-to-video, image-to-video as well as video-to-video generation tasks. Empirically, STARFlow-V achieves strong visual fidelity and temporal consistency with practical sampling throughput relative to diffusion-based baselines. These results present the first evidence, to our knowledge, that NFs are capable of high-quality autoregressive video generation, establishing them as a promising research direction for building world models. Code and generated samples are available at https://github.com/apple/ml-starflow.",
    "arxiv_url": "https://arxiv.org/abs/2511.20462v2",
    "pdf_url": "https://arxiv.org/pdf/2511.20462v2",
    "published_date": "2025-11-25",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "https://github.com/apple/ml-starflow",
    "keywords": [
      "temporal consistency",
      "dit",
      "world model",
      "text-to-video",
      "video generation",
      "image-to-video",
      "video-to-video",
      "diffusion model",
      "architecture",
      "autoregressive"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.20462v2",
      "pdf": "https://arxiv.org/pdf/2511.20462v2",
      "github": "https://github.com/apple/ml-starflow"
    },
    "bibtex": ""
  },
  {
    "title": "Block Cascading: Training Free Acceleration of Block-Causal Video Models",
    "authors": [
      "Hmrishav Bandyopadhyay",
      "Nikhil Pinnaparaju",
      "Rahim Entezari",
      "Jim Scott",
      "Yi-Zhe Song",
      "Varun Jampani"
    ],
    "abstract": "Block-causal video generation faces a stark speed-quality trade-off: small 1.3B models manage only 16 FPS while large 14B models crawl at 4.5 FPS, forcing users to choose between responsiveness and quality. Block Cascading significantly mitigates this trade-off through training-free parallelization. Our key insight: future video blocks do not need fully denoised current blocks to begin generation. By starting block generation with partially denoised context from predecessors, we transform sequential pipelines into parallel cascades where multiple blocks denoise simultaneously. With 5 GPUs exploiting temporal parallelism, we achieve ~2x acceleration across all model scales: 1.3B models accelerate from 16 to 30 FPS, 14B models from 4.5 to 12.5 FPS. Beyond inference speed, Block Cascading eliminates overhead from KV-recaching (of ~200ms) during context switches for interactive generation. Extensive evaluations validated against multiple block-causal pipelines demonstrate no significant loss in generation quality when switching from block-causal to Block Cascading pipelines for inference. Project Page: https://hmrishavbandy.github.io/block_cascading_page/",
    "arxiv_url": "https://arxiv.org/abs/2511.20426v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20426v1",
    "published_date": "2025-11-25",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "evaluation",
      "interactive",
      "acceleration",
      "video generation"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.20426v1",
      "pdf": "https://arxiv.org/pdf/2511.20426v1",
      "project": "https://hmrishavbandy.github.io/block_cascading_page"
    },
    "bibtex": ""
  },
  {
    "title": "TReFT: Taming Rectified Flow Models For One-Step Image Translation",
    "authors": [
      "Shengqian Li",
      "Ming Gao",
      "Yi Liu",
      "Zuzeng Lin",
      "Feng Wang",
      "Feng Dai"
    ],
    "abstract": "Rectified Flow (RF) models have advanced high-quality image and video synthesis via optimal transport theory. However, when applied to image-to-image translation, they still depend on costly multi-step denoising, hindering real-time applications. Although the recent adversarial training paradigm, CycleGAN-Turbo, works in pretrained diffusion models for one-step image translation, we find that directly applying it to RF models leads to severe convergence issues. In this paper, we analyze these challenges and propose TReFT, a novel method to Tame Rectified Flow models for one-step image Translation. Unlike previous works, TReFT directly uses the velocity predicted by pretrained DiT or UNet as output-a simple yet effective design that tackles the convergence issues under adversarial training with one-step inference. This design is mainly motivated by a novel observation that, near the end of the denoising process, the velocity predicted by pretrained RF models converges to the vector from origin to the final clean image, a property we further justify through theoretical analysis. When applying TReFT to large pretrained RF models such as SD3.5 and FLUX, we introduce memory-efficient latent cycle-consistency and identity losses during training, as well as lightweight architectural simplifications for faster inference. Pretrained RF models finetuned with TReFT achieve performance comparable to sota methods across multiple image translation datasets while enabling real-time inference.",
    "arxiv_url": "https://arxiv.org/abs/2511.20307v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20307v1",
    "published_date": "2025-11-25",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "rectified flow",
      "dit",
      "identity",
      "video synthesis",
      "denoising",
      "diffusion model",
      "efficient"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.20307v1",
      "pdf": "https://arxiv.org/pdf/2511.20307v1"
    },
    "bibtex": ""
  }
]